{
  "collected_at": "2026-02-19T08:14:37Z",
  "issues": [
    {
      "number": 34759,
      "title": "[Bug]: nvidia/Llama-3.3-70B-Instruct-NVFP4 Degraded / Gibberish Output with TRITON_ATTN",
      "author": "frankwang28",
      "state": "open",
      "created_at": "2026-02-17T23:19:47Z",
      "updated_at": "2026-02-19T07:22:17Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34759",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 29521,
      "title": "[CI Failure]: mi325_1: Samplers Test",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2025-11-26T18:26:45Z",
      "updated_at": "2026-02-19T06:51:46Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29521",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 27300,
      "title": "[Bug]: vLLM produces invalid UTF-8 tokens and \u201c\ufffd\u201d replacement characters when returning logprobs",
      "author": "yinggeh",
      "state": "open",
      "created_at": "2025-10-21T22:17:25Z",
      "updated_at": "2026-02-19T04:42:21Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/27300",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 34619,
      "title": "[Bug]: Qwen3.5. `illegal memory access`",
      "author": "vadiklyutiy",
      "state": "open",
      "created_at": "2026-02-16T12:52:21Z",
      "updated_at": "2026-02-19T02:16:28Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34619",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 27645,
      "title": "[Bug]: Qwen3-VL-235B-A22B-Instruct stuck with assert placeholder < len(self._out_of_band_tensors)",
      "author": "yry0008",
      "state": "open",
      "created_at": "2025-10-28T09:53:39Z",
      "updated_at": "2026-02-19T02:16:14Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/27645",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 33672,
      "title": "[Bug]: HTTP API multimodal embedding causes image_pad token duplication, producing incorrect results",
      "author": "ojipadeson",
      "state": "open",
      "created_at": "2026-02-03T09:08:23Z",
      "updated_at": "2026-02-19T02:15:46Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33672",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 28234,
      "title": "Online Dynamic FP8 Quantization (--quantization=\"fp8\") is slower than BF16/FP16 on RTX 5090",
      "author": "rEwfii",
      "state": "open",
      "created_at": "2025-11-06T18:18:29Z",
      "updated_at": "2026-02-19T02:15:25Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/28234",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 28371,
      "title": "[Installation]: Recipe Installation fails due to missing linux aarch64.whl",
      "author": "SourBhow",
      "state": "open",
      "created_at": "2025-11-10T01:30:20Z",
      "updated_at": "2026-02-19T02:15:15Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/28371",
      "labels": [
        "installation",
        "stale"
      ]
    },
    {
      "number": 28456,
      "title": "[Usage]: benchmark_moe Usage",
      "author": "ekmekovski",
      "state": "open",
      "created_at": "2025-11-11T09:22:33Z",
      "updated_at": "2026-02-19T02:15:10Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/28456",
      "labels": [
        "usage",
        "stale"
      ]
    },
    {
      "number": 28669,
      "title": "vLLM 0.11.0 CUDA Library Mismatch on ARM64 with CUDA 13.x",
      "author": "swatson1000000",
      "state": "open",
      "created_at": "2025-11-13T17:06:38Z",
      "updated_at": "2026-02-19T02:14:58Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/28669",
      "labels": [
        "installation",
        "stale"
      ]
    },
    {
      "number": 28843,
      "title": "[Bug]: GPTQ quants fails to run ( RuntimeError: gpt_marlin_gemm only supports bfloat16 and float16 )",
      "author": "drrros",
      "state": "open",
      "created_at": "2025-11-17T07:58:37Z",
      "updated_at": "2026-02-19T02:14:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/28843",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 28910,
      "title": "[Bug]:",
      "author": "SongXiaoMao",
      "state": "open",
      "created_at": "2025-11-18T06:22:25Z",
      "updated_at": "2026-02-19T02:14:38Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/28910",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 29014,
      "title": "[Bug]: stride mismatch when using torch compile on graphs with splitting_ops and non-standard tensor dimensions",
      "author": "tomeras91",
      "state": "open",
      "created_at": "2025-11-19T15:48:38Z",
      "updated_at": "2026-02-19T02:14:28Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29014",
      "labels": [
        "bug",
        "torch.compile",
        "stale"
      ]
    },
    {
      "number": 29098,
      "title": "[Installation]: Failed building wheel for vllm",
      "author": "aizyler",
      "state": "open",
      "created_at": "2025-11-20T15:16:30Z",
      "updated_at": "2026-02-19T02:14:18Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29098",
      "labels": [
        "installation",
        "stale"
      ]
    },
    {
      "number": 33857,
      "title": "[Bug]: Qwen3-Coder-Next fails with Triton allocator error on DGX Spark cluster (GB10, sm121)",
      "author": "eugr",
      "state": "open",
      "created_at": "2026-02-05T02:15:14Z",
      "updated_at": "2026-02-19T02:08:04Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33857",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 34817,
      "title": "[Bug]: Trying to run gpt-oss-120b on rtx pro 6000",
      "author": "chadbek",
      "state": "open",
      "created_at": "2026-02-18T15:37:43Z",
      "updated_at": "2026-02-19T01:48:55Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34817",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 34859,
      "title": "[Bug]: missing shards from quantized checkpoint fails silently",
      "author": "andrea-fasoli",
      "state": "open",
      "created_at": "2026-02-19T00:22:43Z",
      "updated_at": "2026-02-19T00:22:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34859",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 26206,
      "title": "[Bug]: Qwen3-Next-80B-A3B-Thinking fails to load with CPU offload",
      "author": "br00t4c",
      "state": "open",
      "created_at": "2025-10-04T00:45:39Z",
      "updated_at": "2026-02-19T00:02:53Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/26206",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 34205,
      "title": "[Bug]: Set env ROCP_TOOL_ATTACH=1 caused vllm server stopped",
      "author": "BigFaceBoy",
      "state": "open",
      "created_at": "2026-02-10T03:50:51Z",
      "updated_at": "2026-02-18T22:54:58Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34205",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 34470,
      "title": "[Bug]: NVIDIA Jetson Thor: Value 'sm_110a' is not defined for option 'gpu-name'",
      "author": "Kaweees",
      "state": "open",
      "created_at": "2026-02-12T22:08:41Z",
      "updated_at": "2026-02-18T22:11:52Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34470",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 34781,
      "title": "[Feature]: parity with cuda - ROCm Kimi K2.5 disagg PD +wideEP recipe",
      "author": "functionstackx",
      "state": "open",
      "created_at": "2026-02-18T06:02:12Z",
      "updated_at": "2026-02-18T19:24:00Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34781",
      "labels": [
        "feature request",
        "rocm"
      ]
    },
    {
      "number": 29534,
      "title": "[CI Failure]: mi325_8: LoRA Test %N",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2025-11-26T19:12:59Z",
      "updated_at": "2026-02-18T19:04:46Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29534",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 29541,
      "title": "[CI Failure]: mi325_1: Entrypoints Integration Test (API Server 1)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2025-11-26T20:07:54Z",
      "updated_at": "2026-02-18T18:40:36Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29541",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 29520,
      "title": "[CI Failure]: mi325_1: Multi-Modal Models Test (Standard)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2025-11-26T18:24:37Z",
      "updated_at": "2026-02-18T18:39:09Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29520",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 34755,
      "title": "Qwen3-Coder-Next-FP8 with tool calling causes system hard-freeze on multi-GPU tensor parallel (v0.15.1)",
      "author": "zaidorx",
      "state": "open",
      "created_at": "2026-02-17T21:59:44Z",
      "updated_at": "2026-02-18T17:07:14Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34755",
      "labels": [
        "usage"
      ]
    },
    {
      "number": 33702,
      "title": "[Roadmap]: PD Disaggregation with `NixlConnector` Roadmap",
      "author": "NickLucche",
      "state": "open",
      "created_at": "2026-02-03T14:55:43Z",
      "updated_at": "2026-02-18T16:53:07Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33702",
      "labels": [
        "feature request"
      ]
    },
    {
      "number": 34583,
      "title": "[Bug] Missing Vocabulary Validation for MTP and Eagle Speculative Methods leads to potential OOB Access",
      "author": "amadhan882",
      "state": "open",
      "created_at": "2026-02-15T13:31:28Z",
      "updated_at": "2026-02-18T15:57:31Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34583",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 26362,
      "title": "[Bug]: whitespace_pattern not doing anything",
      "author": "BramVanroy",
      "state": "open",
      "created_at": "2025-10-07T16:45:39Z",
      "updated_at": "2026-02-18T15:40:50Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/26362",
      "labels": [
        "bug",
        "unstale"
      ]
    },
    {
      "number": 34812,
      "title": "[Bug]: GraniteMoeHybridModel not applying embedding_multiplier to input embeddings",
      "author": "gabe-l-hart",
      "state": "open",
      "created_at": "2026-02-18T15:09:32Z",
      "updated_at": "2026-02-18T15:13:35Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34812",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 32554,
      "title": "[Bug]: Error in inspecting model architecture 'Gemma3ForConditionalGeneration'",
      "author": "Skullians",
      "state": "open",
      "created_at": "2026-01-18T15:23:24Z",
      "updated_at": "2026-02-18T14:39:18Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/32554",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 34573,
      "title": "[Installation/Runtime]: Linux ROCM7 /  RuntimeError: No HIP GPUs are available",
      "author": "NickJLange",
      "state": "open",
      "created_at": "2026-02-15T05:46:42Z",
      "updated_at": "2026-02-18T14:26:07Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34573",
      "labels": [
        "installation",
        "rocm"
      ]
    },
    {
      "number": 34186,
      "title": "[Bug]: LoRA adapters with mismatched module name prefixes silently produce base-model output",
      "author": "Butanium",
      "state": "open",
      "created_at": "2026-02-09T23:40:03Z",
      "updated_at": "2026-02-18T13:45:29Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34186",
      "labels": []
    },
    {
      "number": 34792,
      "title": "[Bug]: setting VLLM_LOGGING_LEVEL=debug breaks tool calling",
      "author": "dtrifiro",
      "state": "open",
      "created_at": "2026-02-18T10:27:47Z",
      "updated_at": "2026-02-18T13:07:37Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34792",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 34650,
      "title": "Bug: Speculative Decoding (MTP) Causes </think> Detection Failure in Structured Output + Reasoning Mode",
      "author": "cicirori",
      "state": "open",
      "created_at": "2026-02-16T22:09:56Z",
      "updated_at": "2026-02-18T05:28:07Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34650",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 29516,
      "title": "[CI Failure]: mi325_4: Distributed Tests (A100)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2025-11-26T18:12:19Z",
      "updated_at": "2026-02-18T04:46:14Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29516",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 34752,
      "title": "[Bug]: Improve `--kv-cache-dtype` behavior when checkpoint specifies `kv_cache_quant_algo`",
      "author": "pavanimajety",
      "state": "open",
      "created_at": "2026-02-17T21:29:55Z",
      "updated_at": "2026-02-18T02:30:35Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34752",
      "labels": [
        "bug",
        "good first issue"
      ]
    },
    {
      "number": 28993,
      "title": "[Bug]: Llama 4 Scout on 2 x B200 errors during FlashInfer attention metadata build",
      "author": "jaywonchung",
      "state": "open",
      "created_at": "2025-11-19T05:35:48Z",
      "updated_at": "2026-02-18T02:14:45Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/28993",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 29006,
      "title": "[Bug]: OpenAI completion error: 500 Unable to allocate 31.6 GiB for an array with shape (65158, 65158) and data type int64",
      "author": "abhiram1809",
      "state": "open",
      "created_at": "2025-11-19T10:46:01Z",
      "updated_at": "2026-02-18T02:14:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29006",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 29007,
      "title": "[Bug]: 0.11.1 A6000x2 120B OSS wrong generations",
      "author": "Nokimann",
      "state": "open",
      "created_at": "2025-11-19T10:57:06Z",
      "updated_at": "2026-02-18T02:14:41Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29007",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 32864,
      "title": "[Bug] [ROCm]: Loading Qwen3-MoE-MXFP4 Weights in v0.14.",
      "author": "tjtanaa",
      "state": "open",
      "created_at": "2026-01-22T15:31:23Z",
      "updated_at": "2026-02-18T01:45:06Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/32864",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 34607,
      "title": "[Bug]: specualative decoding error in 0.15.1",
      "author": "hocop",
      "state": "open",
      "created_at": "2026-02-16T07:46:49Z",
      "updated_at": "2026-02-18T01:21:54Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34607",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 29462,
      "title": "[CI Failure]: mi325_8: Language Models Tests (Hybrid) %N",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2025-11-26T00:21:48Z",
      "updated_at": "2026-02-18T00:42:48Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29462",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 31028,
      "title": "[RFC]: AMD-Quark Online Quantization",
      "author": "hangy-amd",
      "state": "open",
      "created_at": "2025-12-19T12:27:00Z",
      "updated_at": "2026-02-17T23:38:12Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/31028",
      "labels": [
        "rocm",
        "RFC"
      ]
    },
    {
      "number": 34331,
      "title": "[RFC]: Ahead of time dequantization of weights for quantization emulation (OCP MX, NVFP4) on unsupported devices",
      "author": "fxmarty-amd",
      "state": "open",
      "created_at": "2026-02-11T10:26:38Z",
      "updated_at": "2026-02-17T23:37:45Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34331",
      "labels": [
        "rocm",
        "RFC"
      ]
    },
    {
      "number": 33812,
      "title": "[CI Failure]:  mi325_4: LM Eval Large Models (H100)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-04T17:50:56Z",
      "updated_at": "2026-02-17T21:49:16Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33812",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 31856,
      "title": "[Bug]: Text degeneration / repetition loops with MiniMax-M2.1-NVFP4 on v0.14.0rc1.dev308",
      "author": "ktsaou",
      "state": "open",
      "created_at": "2026-01-07T01:36:58Z",
      "updated_at": "2026-02-17T17:41:37Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/31856",
      "labels": []
    },
    {
      "number": 34437,
      "title": "[Bug]: Qwen3 Next with heterogeneous GPU (FP8 overflow?)",
      "author": "Nepherpitou",
      "state": "open",
      "created_at": "2026-02-12T14:29:01Z",
      "updated_at": "2026-02-17T16:38:03Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34437",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 18455,
      "title": "[Bug]: Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.",
      "author": "tonyz0x0",
      "state": "open",
      "created_at": "2025-05-21T05:23:40Z",
      "updated_at": "2026-02-17T15:10:16Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/18455",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 24147,
      "title": "[Bug]: model failure for OpenGVLab/InternVL3-38B-hf",
      "author": "thesillystudent",
      "state": "open",
      "created_at": "2025-09-03T04:57:42Z",
      "updated_at": "2026-02-17T02:17:09Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/24147",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 28640,
      "title": "[Bug]: LoRA/Adapter Loading Error with Qwen3-VL-8B-Instruct Multimodal Model in vLLM Deployment (AssertionError in lora_shrink_op)",
      "author": "pidanshourouzhou",
      "state": "open",
      "created_at": "2025-11-13T12:37:36Z",
      "updated_at": "2026-02-17T02:14:57Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/28640",
      "labels": [
        "bug",
        "unstale"
      ]
    },
    {
      "number": 29523,
      "title": "[CI Failure]: mi325_4: Distributed Tests (4 GPUs)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2025-11-26T18:33:22Z",
      "updated_at": "2026-02-16T23:13:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29523",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 33809,
      "title": "[CI Failure]:  Kernels MoE Test %N",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-04T17:31:26Z",
      "updated_at": "2026-02-16T23:11:18Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33809",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 31920,
      "title": "[Bug]: Prefix cache hit rate remains 0 in multi-round conversation with history of identical prompts.",
      "author": "fdarbeha-amd-com",
      "state": "open",
      "created_at": "2026-01-07T20:53:40Z",
      "updated_at": "2026-02-16T21:40:35Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/31920",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 29511,
      "title": "[CI Failure]: mi325_1: Multi-Modal Models Test (Extended) 1",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2025-11-26T17:36:17Z",
      "updated_at": "2026-02-16T20:11:55Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29511",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 34641,
      "title": "[ROCm] Default VLLM_ROCM_USE_AITER_FP4BMM=True crashes on MI300X (gfx942)",
      "author": "khairulkabir1661",
      "state": "open",
      "created_at": "2026-02-16T19:02:38Z",
      "updated_at": "2026-02-16T19:04:53Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34641",
      "labels": [
        "rocm"
      ]
    }
  ]
}