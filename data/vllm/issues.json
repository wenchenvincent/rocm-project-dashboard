{
  "collected_at": "2026-03-01T08:18:09Z",
  "issues": [
    {
      "number": 29478,
      "title": "[Bug]: Olmo-3 vLLM Online API generates gibberish",
      "author": "hanseungwook",
      "state": "open",
      "created_at": "2025-11-26T05:17:36Z",
      "updated_at": "2026-03-01T08:07:15Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29478",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35624,
      "title": "[Bug]: Qwen3-Omni Model Fails when try to l",
      "author": "Blaze-DSP",
      "state": "open",
      "created_at": "2026-02-28T19:25:01Z",
      "updated_at": "2026-03-01T08:01:21Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35624",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35638,
      "title": "[Question][XPU]: Best practices and optimized arguments for running 30B+ models on Intel Arc B580 (Dual GPU) via vLLM-XPU",
      "author": "jiekechoo",
      "state": "open",
      "created_at": "2026-03-01T05:09:09Z",
      "updated_at": "2026-03-01T07:42:47Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35638",
      "labels": [
        "usage"
      ]
    },
    {
      "number": 35391,
      "title": "[Bug]: Value error, Model architectures ['Qwen3_5ForConditionalGeneration'] are not supported for now",
      "author": "Iam-a-Savage-coder",
      "state": "open",
      "created_at": "2026-02-26T10:08:09Z",
      "updated_at": "2026-03-01T07:31:50Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35391",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35637,
      "title": "[Bug]: minimax m2.1 arch mxfp4 rocm AITER TP4 error",
      "author": "functionstackx",
      "state": "open",
      "created_at": "2026-03-01T04:32:27Z",
      "updated_at": "2026-03-01T07:31:39Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35637",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 24865,
      "title": "[Bug]: v0.10.2 Qwen 3 Next 80b FP8 Slow first request and potential format mismatch: seq_len (4) < num_heads (8)",
      "author": "Syst3m1cAn0maly",
      "state": "open",
      "created_at": "2025-09-15T08:42:46Z",
      "updated_at": "2026-03-01T07:18:53Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/24865",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35644,
      "title": "[Feature]: AMD MXFP4 MiniMax M2.5 Checkpoint",
      "author": "functionstackx",
      "state": "open",
      "created_at": "2026-03-01T07:13:17Z",
      "updated_at": "2026-03-01T07:14:39Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35644",
      "labels": [
        "feature request",
        "rocm"
      ]
    },
    {
      "number": 35642,
      "title": "[Bug]: HIP build in Docker: offload-arch stderr contaminates compiler flags via cmake/utils.cmake and CMAKE_HIP_FLAGS",
      "author": "npathak13",
      "state": "open",
      "created_at": "2026-03-01T07:01:55Z",
      "updated_at": "2026-03-01T07:02:04Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35642",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35641,
      "title": "[Bug]: ROCm MI355X Kimi K2.5 AITER TP8 MLA kernel Error (num_head=8)",
      "author": "functionstackx",
      "state": "open",
      "created_at": "2026-03-01T06:49:47Z",
      "updated_at": "2026-03-01T06:55:04Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35641",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35633,
      "title": "[Bug]: parity with cuda: rocm image missing amd quark kimi k2.5 mxfp4",
      "author": "functionstackx",
      "state": "open",
      "created_at": "2026-03-01T01:20:50Z",
      "updated_at": "2026-03-01T06:20:44Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35633",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35639,
      "title": "[RFC]: `vllm bench eval` for Unified Accuracy + Performance Evaluation",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-03-01T05:36:39Z",
      "updated_at": "2026-03-01T05:54:13Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35639",
      "labels": [
        "rocm",
        "RFC"
      ]
    },
    {
      "number": 35504,
      "title": "[Bug]: qwen3-coder-next inference randomly hangs, accuracy degradation in 0.16.0+ with TP > 1 and  fuse_allreduce_rms=False (H100s on PCIe)",
      "author": "vitush93",
      "state": "open",
      "created_at": "2026-02-27T11:05:38Z",
      "updated_at": "2026-03-01T03:02:34Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35504",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 33638,
      "title": "[Bug]: DeepSeekV3.1 with fp8 kvcache in v0.15.0 produces garbled output",
      "author": "lyg95",
      "state": "open",
      "created_at": "2026-02-03T03:26:24Z",
      "updated_at": "2026-03-01T02:41:31Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33638",
      "labels": [
        "bug",
        "deepseek"
      ]
    },
    {
      "number": 27604,
      "title": "[Bug]: Is Flashinfer Attn backend supposed to work with FP8 KV cache on Hopper?",
      "author": "jmkuebler",
      "state": "open",
      "created_at": "2025-10-27T20:22:37Z",
      "updated_at": "2026-03-01T02:16:14Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/27604",
      "labels": [
        "bug",
        "unstale",
        "nvidia"
      ]
    },
    {
      "number": 27890,
      "title": "\"fatal error: Python.h: No such file or directory\" upon vllm startup after a clean install",
      "author": "kha84",
      "state": "open",
      "created_at": "2025-10-31T16:25:06Z",
      "updated_at": "2026-03-01T02:16:03Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/27890",
      "labels": [
        "bug",
        "unstale"
      ]
    },
    {
      "number": 29743,
      "title": "[Feature]: Turing support in Qwen3-VL backends",
      "author": "TURX",
      "state": "open",
      "created_at": "2025-11-30T08:14:39Z",
      "updated_at": "2026-03-01T02:14:45Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29743",
      "labels": [
        "feature request",
        "stale"
      ]
    },
    {
      "number": 35625,
      "title": "[Bug]: TTFT latency issue with Qwen3.5-35B-A3B model using vllm",
      "author": "shahizat",
      "state": "open",
      "created_at": "2026-02-28T20:34:01Z",
      "updated_at": "2026-02-28T20:34:01Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35625",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35519,
      "title": "[Bug]: Qwen3.5 NVFP4 models crash on ARM64 GB10 DGX Spark (CUDA illegal instruction during generation)",
      "author": "EmilHaase",
      "state": "open",
      "created_at": "2026-02-27T15:11:48Z",
      "updated_at": "2026-02-28T15:57:06Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35519",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35028,
      "title": "[Bug]: RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmEx",
      "author": "shahizat",
      "state": "open",
      "created_at": "2026-02-21T17:55:04Z",
      "updated_at": "2026-02-28T15:53:47Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35028",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35608,
      "title": "[Bug]: vllm 0.16.0+image encountered CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmEx",
      "author": "adenzhou1350",
      "state": "open",
      "created_at": "2026-02-28T10:40:31Z",
      "updated_at": "2026-02-28T15:47:35Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35608",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35319,
      "title": "[Bug]: Multi-Node inference with PP > 1 crashes after processing completions request with non-None `logprobs` parameter.",
      "author": "JeanPaulShapo",
      "state": "open",
      "created_at": "2026-02-25T18:31:26Z",
      "updated_at": "2026-02-28T15:13:04Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35319",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35521,
      "title": "[Bug]: Suffix decoding crashes with assert total_num_scheduled_tokens > 0",
      "author": "rosstang",
      "state": "open",
      "created_at": "2026-02-27T15:35:13Z",
      "updated_at": "2026-02-28T14:05:29Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35521",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35599,
      "title": "[Bug]: cpu version compile failed in 0.16.0",
      "author": "wanghualex1-wq",
      "state": "open",
      "created_at": "2026-02-28T07:40:54Z",
      "updated_at": "2026-02-28T10:42:42Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35599",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35344,
      "title": "[Bug]: Value error, Model architectures ['Qwen3_5MoeForConditionalGeneration'] are not supported for now",
      "author": "Dong09",
      "state": "open",
      "created_at": "2026-02-26T01:19:33Z",
      "updated_at": "2026-02-28T03:59:57Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35344",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35191,
      "title": "[Bug]: Qwen3.5 397B FP8 fills 1TB RAM and OOM killed with high-concurrency multimodal requests",
      "author": "FWao",
      "state": "open",
      "created_at": "2026-02-24T12:45:47Z",
      "updated_at": "2026-02-28T03:05:11Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35191",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35569,
      "title": "[Bug]: [ROCm] ROCM_ATTN backend shows ~8.5% systematic score deviation on Qwen3-VL-Reranker pooling",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-28T02:31:46Z",
      "updated_at": "2026-02-28T03:04:20Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35569",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35507,
      "title": "[Bug]: Assertionerror when using OffloadingConnector: assert len(req.block_hashes) >= num_gpu_blocks",
      "author": "liunianxuxie",
      "state": "open",
      "created_at": "2026-02-27T12:20:49Z",
      "updated_at": "2026-02-28T02:49:49Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35507",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 25870,
      "title": "[Bug]: gpt-oss-120b failed with data-parallel-size >1",
      "author": "Yaomt",
      "state": "open",
      "created_at": "2025-09-29T09:36:08Z",
      "updated_at": "2026-02-28T02:16:41Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/25870",
      "labels": [
        "bug",
        "unstale"
      ]
    },
    {
      "number": 26646,
      "title": "[Bug]: KV cache can't be quantized for Qwen3-Next",
      "author": "drrros",
      "state": "open",
      "created_at": "2025-10-11T16:43:48Z",
      "updated_at": "2026-02-28T02:16:27Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/26646",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 29725,
      "title": "[Bug]: Responses API: Streaming returns ResponseTextDeltaEvent instead of ResponseFunctionCallArgumentsDeltaEvent for tool calls while using non-harmony models",
      "author": "sumitaryal",
      "state": "open",
      "created_at": "2025-11-29T12:12:14Z",
      "updated_at": "2026-02-28T02:13:55Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29725",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 35541,
      "title": "[Bug]: vLLM hangs indefinitely with low `num_gpu_blocks_override`",
      "author": "kvcache670",
      "state": "open",
      "created_at": "2026-02-27T19:45:33Z",
      "updated_at": "2026-02-27T20:09:55Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35541",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 24499,
      "title": "[Installation]: \u5728amd rocm6.4.2\u8bbe\u5907\u4e0a\u5b89\u88c5\u62a5\u9519",
      "author": "tanghui886",
      "state": "open",
      "created_at": "2025-09-09T10:32:26Z",
      "updated_at": "2026-02-27T19:17:42Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/24499",
      "labels": [
        "installation",
        "rocm",
        "unstale"
      ]
    },
    {
      "number": 35528,
      "title": "[Feature]: Support serving ModelOpt W4A8 MXFP4+FP8 checkpoints",
      "author": "zeryx",
      "state": "open",
      "created_at": "2026-02-27T16:47:50Z",
      "updated_at": "2026-02-27T17:07:33Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35528",
      "labels": [
        "feature request"
      ]
    },
    {
      "number": 35465,
      "title": "[Bug]: No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization)",
      "author": "DongZhaoXiong",
      "state": "open",
      "created_at": "2026-02-27T02:32:50Z",
      "updated_at": "2026-02-27T16:32:24Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35465",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35522,
      "title": "[Bug]: Music Flamingo `ValueError: Following weights were not initialized from checkpoint: {'audio_tower.pos_emb.freqs'}`",
      "author": "denadai2",
      "state": "open",
      "created_at": "2026-02-27T15:35:38Z",
      "updated_at": "2026-02-27T16:09:00Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35522",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 34154,
      "title": "[Bug]: Failing to run DeepSeek-OCR on Radeon GPUs (memory fault)",
      "author": "umarinkovic",
      "state": "open",
      "created_at": "2026-02-09T17:41:29Z",
      "updated_at": "2026-02-27T16:00:54Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34154",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 34449,
      "title": "[Bug]: GLM-5-FP8 malformed tool calls",
      "author": "TALLEC-Scott",
      "state": "open",
      "created_at": "2026-02-12T18:20:25Z",
      "updated_at": "2026-02-27T14:26:21Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34449",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35286,
      "title": "[Bug]: Qwen3.5-MoE failed with enable_lora",
      "author": "hjh0119",
      "state": "open",
      "created_at": "2026-02-25T11:42:35Z",
      "updated_at": "2026-02-27T12:10:22Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35286",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35464,
      "title": "[Bug]: Fix GLM-OCR text_config model_type handling",
      "author": "iron-shaper",
      "state": "open",
      "created_at": "2026-02-27T02:31:14Z",
      "updated_at": "2026-02-27T02:33:50Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35464",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 34641,
      "title": "[ROCm] Default VLLM_ROCM_USE_AITER_FP4BMM=True crashes on MI300X (gfx942)",
      "author": "khairulkabir1661",
      "state": "open",
      "created_at": "2026-02-16T19:02:38Z",
      "updated_at": "2026-02-27T01:59:36Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34641",
      "labels": [
        "rocm"
      ]
    },
    {
      "number": 35163,
      "title": "[Bug]: AMD docker image still using torch 2.9 despite 2.10.0 in `requirements/rocm-build.txt`",
      "author": "mikaylagawarecki",
      "state": "open",
      "created_at": "2026-02-24T01:47:25Z",
      "updated_at": "2026-02-27T01:56:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35163",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 34118,
      "title": "[Feature]: [ROCm]: GPTQ INT4 MoE kernel fails on non-SiLU activations - no Marlin fallback on AMD",
      "author": "ehartford",
      "state": "open",
      "created_at": "2026-02-09T05:28:24Z",
      "updated_at": "2026-02-27T01:32:00Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34118",
      "labels": [
        "feature request",
        "rocm"
      ]
    },
    {
      "number": 34399,
      "title": "[Bug]: Nemotron 3 (all quants) take a LONG time to load",
      "author": "jiangwu300",
      "state": "open",
      "created_at": "2026-02-12T03:44:51Z",
      "updated_at": "2026-02-26T22:38:52Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34399",
      "labels": [
        "bug",
        "torch.compile"
      ]
    },
    {
      "number": 25179,
      "title": "[Performance]: Custom fused kernel tracking",
      "author": "ProExpertProg",
      "state": "open",
      "created_at": "2025-09-18T15:31:02Z",
      "updated_at": "2026-02-26T22:33:28Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/25179",
      "labels": [
        "performance",
        "torch.compile"
      ]
    },
    {
      "number": 35089,
      "title": "[RFC]: In-Tree AMD Zen CPU Backend via zentorch",
      "author": "amd-lalithnc",
      "state": "open",
      "created_at": "2026-02-23T09:36:32Z",
      "updated_at": "2026-02-26T22:23:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35089",
      "labels": [
        "rocm",
        "RFC",
        "cpu"
      ]
    },
    {
      "number": 35133,
      "title": "[CI Failure]:  mi355_4: LoRA TP Test (Distributed)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-23T18:34:13Z",
      "updated_at": "2026-02-26T21:01:27Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35133",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 32864,
      "title": "[Bug] [ROCm]: Loading Qwen3-MoE-MXFP4 Weights in v0.14.",
      "author": "tjtanaa",
      "state": "open",
      "created_at": "2026-01-22T15:31:23Z",
      "updated_at": "2026-02-26T20:25:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/32864",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35169,
      "title": "[Bug]: Memory Access Fault during Step-3.5-Flash inference (ROCM)",
      "author": "ColinZ22",
      "state": "open",
      "created_at": "2026-02-24T04:41:04Z",
      "updated_at": "2026-02-26T17:31:36Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35169",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35132,
      "title": "[CI Failure][ROCm]:  CrossLayer KV layout Distributed NixlConnector PD accuracy tests (4 GPUs)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-23T18:28:34Z",
      "updated_at": "2026-02-26T17:03:27Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35132",
      "labels": [
        "rocm",
        "ci-failure"
      ]
    },
    {
      "number": 34028,
      "title": "[Usage]: Unable to load Mistral-Small-3.2-24B-Instruct-2506-FP8 due to \"Value error, Found unknown quantization\", but the same configs worked for vllm v0.11.0",
      "author": "gabinguo",
      "state": "open",
      "created_at": "2026-02-06T23:29:20Z",
      "updated_at": "2026-02-26T16:26:17Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34028",
      "labels": [
        "usage"
      ]
    }
  ]
}