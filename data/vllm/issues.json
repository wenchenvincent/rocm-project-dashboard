{
  "collected_at": "2026-02-26T08:33:14Z",
  "issues": [
    {
      "number": 19197,
      "title": "[Bug]: \u6a21\u578b\u8fd0\u884c\u671f\u95f4\uff0c\u62a5\u9519TimeoutError: RPC call to execute_model timed out.\uff0c\u5bfc\u81f4\u6a21\u578b\u9000\u51fa\u3002",
      "author": "nvliajia",
      "state": "open",
      "created_at": "2025-06-05T09:19:47Z",
      "updated_at": "2026-02-26T08:05:20Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/19197",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35344,
      "title": "[Bug]: Value error, Model architectures ['Qwen3_5MoeForConditionalGeneration'] are not supported for now",
      "author": "Dong09",
      "state": "open",
      "created_at": "2026-02-26T01:19:33Z",
      "updated_at": "2026-02-26T07:52:10Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35344",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 29478,
      "title": "[Bug]: Olmo-3 vLLM Online API generates gibberish",
      "author": "hanseungwook",
      "state": "open",
      "created_at": "2025-11-26T05:17:36Z",
      "updated_at": "2026-02-26T07:48:16Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29478",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35381,
      "title": "[Bug]: Not able to run Qwen3-8B-NVFP4 on B200",
      "author": "IKACE",
      "state": "open",
      "created_at": "2026-02-26T07:46:35Z",
      "updated_at": "2026-02-26T07:46:35Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35381",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 27699,
      "title": "[Bug]: Inductor fails to fuse pointwise ops with sequence parallelism + async TP",
      "author": "ProExpertProg",
      "state": "open",
      "created_at": "2025-10-29T00:26:57Z",
      "updated_at": "2026-02-26T06:49:04Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/27699",
      "labels": [
        "bug",
        "torch.compile",
        "keep-open"
      ]
    },
    {
      "number": 33338,
      "title": "[Bug]: Crash when using presence_penalty with Qwen3-VL in v0.11.0",
      "author": "Lrcx",
      "state": "open",
      "created_at": "2026-01-29T11:54:48Z",
      "updated_at": "2026-02-26T06:44:34Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33338",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 32730,
      "title": "[Bug]: CPU tests hit CUDA path when VLLM_TARGET_DEVICE=cpu [CPU Backend]",
      "author": "wjhrdy",
      "state": "open",
      "created_at": "2026-01-20T22:18:02Z",
      "updated_at": "2026-02-26T06:20:35Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/32730",
      "labels": [
        "bug",
        "cpu"
      ]
    },
    {
      "number": 35089,
      "title": "[RFC]: In-Tree AMD Zen CPU Backend via zentorch",
      "author": "amd-lalithnc",
      "state": "open",
      "created_at": "2026-02-23T09:36:32Z",
      "updated_at": "2026-02-26T06:10:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35089",
      "labels": [
        "rocm",
        "RFC",
        "cpu"
      ]
    },
    {
      "number": 35349,
      "title": "[Bug]: MiniMax-2.1: Missing `<think>` tag in the LLM response after tool calls succeed.",
      "author": "stingoChen",
      "state": "open",
      "created_at": "2026-02-26T02:39:49Z",
      "updated_at": "2026-02-26T06:02:18Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35349",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35191,
      "title": "[Bug]: Qwen3.5 397B FP8 fills 1TB RAM and OOM killed with high-concurrency multimodal requests",
      "author": "FWao",
      "state": "open",
      "created_at": "2026-02-24T12:45:47Z",
      "updated_at": "2026-02-26T03:58:44Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35191",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35255,
      "title": "[Bug]: CUDA Error 803 on host with driver 590.48: `system has unsupported display driver / cuda driver combination",
      "author": "git-jxj",
      "state": "open",
      "created_at": "2026-02-25T03:18:30Z",
      "updated_at": "2026-02-26T03:22:38Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35255",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35321,
      "title": "[Feature]: Encoder self-attention for RocmAttentionImpl",
      "author": "micah-wil",
      "state": "open",
      "created_at": "2026-02-25T18:44:39Z",
      "updated_at": "2026-02-26T03:14:03Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35321",
      "labels": [
        "feature request",
        "rocm"
      ]
    },
    {
      "number": 32864,
      "title": "[Bug] [ROCm]: Loading Qwen3-MoE-MXFP4 Weights in v0.14.",
      "author": "tjtanaa",
      "state": "open",
      "created_at": "2026-01-22T15:31:23Z",
      "updated_at": "2026-02-26T02:58:10Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/32864",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 34205,
      "title": "[Bug]: Set env ROCP_TOOL_ATTACH=1 caused vllm server stopped",
      "author": "BigFaceBoy",
      "state": "open",
      "created_at": "2026-02-10T03:50:51Z",
      "updated_at": "2026-02-26T02:51:42Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34205",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35169,
      "title": "[Bug]: Memory Access Fault during Step-3.5-Flash inference (ROCM)",
      "author": "ColinZ22",
      "state": "open",
      "created_at": "2026-02-24T04:41:04Z",
      "updated_at": "2026-02-26T02:46:10Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35169",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 20216,
      "title": "[Bug]: When inferring Qwen3-32B-AWQ with vllm0.9.2, an error message appears: Quantization scheme is not supported",
      "author": "HelloCard",
      "state": "open",
      "created_at": "2025-06-28T18:06:35Z",
      "updated_at": "2026-02-26T02:17:19Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/20216",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 23577,
      "title": "[Bug]: default_weight_loader receives unexpected `weight_name` kwarg in GPT-OSS load_weights",
      "author": "cmao-at",
      "state": "open",
      "created_at": "2025-08-25T18:05:57Z",
      "updated_at": "2026-02-26T02:16:52Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/23577",
      "labels": [
        "bug",
        "unstale"
      ]
    },
    {
      "number": 25850,
      "title": "[Bug]: vLLM subprocesses remain alive after parent process/server exits (MCP client exit \u2192 vLLM still alive)",
      "author": "xhd0728",
      "state": "open",
      "created_at": "2025-09-29T03:38:58Z",
      "updated_at": "2026-02-26T02:16:34Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/25850",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 27890,
      "title": "\"fatal error: Python.h: No such file or directory\" upon vllm startup after a clean install",
      "author": "kha84",
      "state": "open",
      "created_at": "2025-10-31T16:25:06Z",
      "updated_at": "2026-02-26T02:15:39Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/27890",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 29566,
      "title": "[Bug]: disagg_example_p2p_nccl_xpyd.sh RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}",
      "author": "ZRJ026",
      "state": "open",
      "created_at": "2025-11-27T02:32:45Z",
      "updated_at": "2026-02-26T02:14:15Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29566",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 34399,
      "title": "[Bug]: Nemotron 3 (all quants) take a LONG time to load",
      "author": "jiangwu300",
      "state": "open",
      "created_at": "2026-02-12T03:44:51Z",
      "updated_at": "2026-02-26T00:21:12Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34399",
      "labels": [
        "bug",
        "torch.compile"
      ]
    },
    {
      "number": 32335,
      "title": "[Feature]: Extract KV-Cache update from all attention backends",
      "author": "ElizaWszola",
      "state": "open",
      "created_at": "2026-01-14T14:24:56Z",
      "updated_at": "2026-02-25T21:02:49Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/32335",
      "labels": [
        "help wanted",
        "good first issue",
        "feature request"
      ]
    },
    {
      "number": 35319,
      "title": "[Bug]: Multi-Node inference with PP > 1 crashes after processing completions request with non-None `logprobs` parameter.",
      "author": "JeanPaulShapo",
      "state": "open",
      "created_at": "2026-02-25T18:31:26Z",
      "updated_at": "2026-02-25T18:34:35Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35319",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35163,
      "title": "[Bug]: AMD docker image still using torch 2.9 despite 2.10.0 in `requirements/rocm-build.txt`",
      "author": "mikaylagawarecki",
      "state": "open",
      "created_at": "2026-02-24T01:47:25Z",
      "updated_at": "2026-02-25T17:59:53Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35163",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 33654,
      "title": "[Bug]: The content of response from Kimi-K2.5 is empty.",
      "author": "WangTuoxytt",
      "state": "open",
      "created_at": "2026-02-03T06:23:40Z",
      "updated_at": "2026-02-25T16:25:08Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33654",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35138,
      "title": "[Bug]: Qwen/Qwen3.5-397B-A17B-FP8 and Qwen/Qwen3.5-397B-A17B has accuracy issues when running with Flashinfer Attention backend on Blackwell.",
      "author": "xinli-sw",
      "state": "open",
      "created_at": "2026-02-23T19:06:48Z",
      "updated_at": "2026-02-25T14:58:02Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35138",
      "labels": [
        "bug",
        "nvidia"
      ]
    },
    {
      "number": 27157,
      "title": "[Bug]: Qwen3-VL-30B-A3B-Instruct keeps outputting the same phrases over and over",
      "author": "kozanryusui",
      "state": "open",
      "created_at": "2025-10-18T23:27:10Z",
      "updated_at": "2026-02-25T13:52:46Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/27157",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35286,
      "title": "[Bug]: Qwen3.5-MoE failed with enable_lora",
      "author": "hjh0119",
      "state": "open",
      "created_at": "2026-02-25T11:42:35Z",
      "updated_at": "2026-02-25T12:57:12Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35286",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35288,
      "title": "[Bug]: MTP speculative decoding produces corrupted output at concurrency >= 4 (V1 engine)",
      "author": "dkremez",
      "state": "open",
      "created_at": "2026-02-25T11:52:46Z",
      "updated_at": "2026-02-25T11:52:46Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35288",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35028,
      "title": "[Bug]: RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmEx",
      "author": "shahizat",
      "state": "open",
      "created_at": "2026-02-21T17:55:04Z",
      "updated_at": "2026-02-25T10:20:45Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35028",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35266,
      "title": "[Bug]: Missing opening brace for Qwen3.5 streaming tool calls",
      "author": "AsterisMono",
      "state": "open",
      "created_at": "2026-02-25T06:35:20Z",
      "updated_at": "2026-02-25T07:19:14Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35266",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 30679,
      "title": "[RFC]: Replace `torch.cuda` API with `torch.accelerator` for better hardware compatiblity.",
      "author": "jikunshang",
      "state": "open",
      "created_at": "2025-12-15T08:23:45Z",
      "updated_at": "2026-02-25T06:49:05Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/30679",
      "labels": [
        "RFC"
      ]
    },
    {
      "number": 33638,
      "title": "[Bug]: DeepSeekV3.1 with fp8 kvcache in v0.15.0 produces garbled output",
      "author": "lyg95",
      "state": "open",
      "created_at": "2026-02-03T03:26:24Z",
      "updated_at": "2026-02-25T06:33:10Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33638",
      "labels": [
        "bug",
        "deepseek"
      ]
    },
    {
      "number": 34449,
      "title": "[Bug]: GLM-5-FP8 malformed tool calls",
      "author": "TALLEC-Scott",
      "state": "open",
      "created_at": "2026-02-12T18:20:25Z",
      "updated_at": "2026-02-25T03:23:52Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34449",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 28003,
      "title": "[Usage]:",
      "author": "amitmvyas",
      "state": "open",
      "created_at": "2025-11-03T21:19:15Z",
      "updated_at": "2026-02-25T02:15:42Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/28003",
      "labels": [
        "usage",
        "stale"
      ]
    },
    {
      "number": 34994,
      "title": "[Feature]: Infrastructure Improvements for ROCm CI",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-20T22:35:58Z",
      "updated_at": "2026-02-24T23:46:19Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34994",
      "labels": [
        "feature request",
        "rocm"
      ]
    },
    {
      "number": 35235,
      "title": "[CI Failure]:  mi355_1: Multi-Modal Models Test (Extended) 1",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-24T21:17:28Z",
      "updated_at": "2026-02-24T21:18:41Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35235",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 35233,
      "title": "[CI Failure]:  mi355_1: Language Models Test (Extended Generation)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-24T21:01:41Z",
      "updated_at": "2026-02-24T21:02:17Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35233",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 29541,
      "title": "[CI Failure]: mi325_1: Entrypoints Integration Test (API Server 1)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2025-11-26T20:07:54Z",
      "updated_at": "2026-02-24T20:59:13Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29541",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 35133,
      "title": "[CI Failure]:  mi355_4: LoRA TP Test (Distributed)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-23T18:34:13Z",
      "updated_at": "2026-02-24T19:56:33Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35133",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 33748,
      "title": "[Bug][Infrastructure]: Inconsistent Docker Image Versioning and Missing Tags on Docker Hub",
      "author": "abeltre1",
      "state": "open",
      "created_at": "2026-02-04T01:42:57Z",
      "updated_at": "2026-02-24T14:16:00Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33748",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35128,
      "title": "[CI Failure]:  mi355_1: Language Models Tests (Standard)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-23T18:16:36Z",
      "updated_at": "2026-02-23T23:38:55Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35128",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 32412,
      "title": "[RFC]: online quantization user facing API",
      "author": "vkuzo",
      "state": "open",
      "created_at": "2026-01-15T12:58:44Z",
      "updated_at": "2026-02-23T19:33:20Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/32412",
      "labels": [
        "RFC"
      ]
    },
    {
      "number": 34851,
      "title": "[Feature]: Refactor Quark MoE and mxfp4 MoE to align with MoE oracle/MK",
      "author": "BowenBao",
      "state": "open",
      "created_at": "2026-02-18T23:27:55Z",
      "updated_at": "2026-02-23T19:28:39Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34851",
      "labels": [
        "feature request"
      ]
    },
    {
      "number": 35126,
      "title": "[CI Failure]:  mi355_1: Kernels MoE Test %N",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-23T18:11:06Z",
      "updated_at": "2026-02-23T19:10:03Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35126",
      "labels": [
        "rocm",
        "ci-failure"
      ]
    },
    {
      "number": 35129,
      "title": "[CI Failure]:  mi355_4: 2 Node Tests (4 GPUs in total)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-23T18:18:18Z",
      "updated_at": "2026-02-23T18:56:35Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35129",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 35132,
      "title": "[CI Failure][ROCm]:  CrossLayer KV layout Distributed NixlConnector PD accuracy tests (4 GPUs)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-23T18:28:34Z",
      "updated_at": "2026-02-23T18:29:10Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35132",
      "labels": [
        "rocm",
        "ci-failure"
      ]
    },
    {
      "number": 26480,
      "title": "[Bug][v0.11.0]: gpt-oss-120b generates with no output",
      "author": "AlessandroSpallina",
      "state": "open",
      "created_at": "2025-10-09T09:45:22Z",
      "updated_at": "2026-02-23T16:35:31Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/26480",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35057,
      "title": "[Bug]: Qwen3.5 `scheduler_metadata must have shape (metadata_size)` with Decode Context Parallel (DCP)",
      "author": "ehfd",
      "state": "open",
      "created_at": "2026-02-22T15:30:54Z",
      "updated_at": "2026-02-23T10:17:20Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35057",
      "labels": [
        "bug"
      ]
    }
  ]
}