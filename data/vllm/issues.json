{
  "collected_at": "2026-02-27T08:29:03Z",
  "issues": [
    {
      "number": 35477,
      "title": "[Usage]: How to test the performance of a encoder-only BertModel using `vllm bench serve`",
      "author": "duanshengliu",
      "state": "open",
      "created_at": "2026-02-27T04:36:40Z",
      "updated_at": "2026-02-27T08:19:22Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35477",
      "labels": [
        "usage"
      ]
    },
    {
      "number": 35191,
      "title": "[Bug]: Qwen3.5 397B FP8 fills 1TB RAM and OOM killed with high-concurrency multimodal requests",
      "author": "FWao",
      "state": "open",
      "created_at": "2026-02-24T12:45:47Z",
      "updated_at": "2026-02-27T07:17:16Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35191",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 34154,
      "title": "[Bug]:",
      "author": "umarinkovic",
      "state": "open",
      "created_at": "2026-02-09T17:41:29Z",
      "updated_at": "2026-02-27T07:17:15Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34154",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35319,
      "title": "[Bug]: Multi-Node inference with PP > 1 crashes after processing completions request with non-None `logprobs` parameter.",
      "author": "JeanPaulShapo",
      "state": "open",
      "created_at": "2026-02-25T18:31:26Z",
      "updated_at": "2026-02-27T07:16:21Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35319",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35286,
      "title": "[Bug]: Qwen3.5-MoE failed with enable_lora",
      "author": "hjh0119",
      "state": "open",
      "created_at": "2026-02-25T11:42:35Z",
      "updated_at": "2026-02-27T06:40:08Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35286",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 31990,
      "title": "[Bug]: [H200] Qwen3-Next-80B-A3B-Instruct-FP8 TP1 DP4 EP4 CUDA illegal memory error",
      "author": "jhaotingc",
      "state": "open",
      "created_at": "2026-01-08T19:34:00Z",
      "updated_at": "2026-02-27T06:36:53Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/31990",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35344,
      "title": "[Bug]: Value error, Model architectures ['Qwen3_5MoeForConditionalGeneration'] are not supported for now",
      "author": "Dong09",
      "state": "open",
      "created_at": "2026-02-26T01:19:33Z",
      "updated_at": "2026-02-27T05:21:09Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35344",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35464,
      "title": "[Bug]: Fix GLM-OCR text_config model_type handling",
      "author": "iron-shaper",
      "state": "open",
      "created_at": "2026-02-27T02:31:14Z",
      "updated_at": "2026-02-27T02:33:50Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35464",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35465,
      "title": "[Bug]: No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization)",
      "author": "DongZhaoXiong",
      "state": "open",
      "created_at": "2026-02-27T02:32:50Z",
      "updated_at": "2026-02-27T02:32:50Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35465",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35391,
      "title": "[Bug]: Value error, Model architectures ['Qwen3_5ForConditionalGeneration'] are not supported for now",
      "author": "Iam-a-Savage-coder",
      "state": "open",
      "created_at": "2026-02-26T10:08:09Z",
      "updated_at": "2026-02-27T02:30:53Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35391",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 23816,
      "title": "[Bug]: multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 8 leaked shared_memory objects to clean up at shutdow",
      "author": "gkm0120",
      "state": "open",
      "created_at": "2025-08-28T09:09:39Z",
      "updated_at": "2026-02-27T02:21:32Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/23816",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 29670,
      "title": "[Bug]: single node 8 h20,ebo failed!",
      "author": "bleedingfight",
      "state": "open",
      "created_at": "2025-11-28T11:15:07Z",
      "updated_at": "2026-02-27T02:18:28Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29670",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 29676,
      "title": "[Bug]: First call to llama model takes too much time compared to subsequent ones",
      "author": "ozancaglayan",
      "state": "open",
      "created_at": "2025-11-28T13:44:13Z",
      "updated_at": "2026-02-27T02:18:27Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29676",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 35414,
      "title": "[Bug]: 4*2080ti 22g deploy Qwen3.5-35B-A3B fail:2080 Ti does not support bfloat16",
      "author": "chuanSir123",
      "state": "open",
      "created_at": "2026-02-26T15:47:42Z",
      "updated_at": "2026-02-27T02:10:54Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35414",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 34641,
      "title": "[ROCm] Default VLLM_ROCM_USE_AITER_FP4BMM=True crashes on MI300X (gfx942)",
      "author": "khairulkabir1661",
      "state": "open",
      "created_at": "2026-02-16T19:02:38Z",
      "updated_at": "2026-02-27T01:59:36Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34641",
      "labels": [
        "rocm"
      ]
    },
    {
      "number": 35163,
      "title": "[Bug]: AMD docker image still using torch 2.9 despite 2.10.0 in `requirements/rocm-build.txt`",
      "author": "mikaylagawarecki",
      "state": "open",
      "created_at": "2026-02-24T01:47:25Z",
      "updated_at": "2026-02-27T01:56:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35163",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 34067,
      "title": "[Feature]: nightly docker and wheels for ROCm",
      "author": "hongxiayang",
      "state": "open",
      "created_at": "2026-02-07T21:31:17Z",
      "updated_at": "2026-02-27T01:43:44Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34067",
      "labels": [
        "feature request",
        "rocm"
      ]
    },
    {
      "number": 34118,
      "title": "[Feature]: [ROCm]: GPTQ INT4 MoE kernel fails on non-SiLU activations - no Marlin fallback on AMD",
      "author": "ehartford",
      "state": "open",
      "created_at": "2026-02-09T05:28:24Z",
      "updated_at": "2026-02-27T01:32:00Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34118",
      "labels": [
        "feature request",
        "rocm"
      ]
    },
    {
      "number": 34256,
      "title": "[Model Performance SIG]: Improve MoE Oracle",
      "author": "robertgshaw2-redhat",
      "state": "open",
      "created_at": "2026-02-10T16:49:31Z",
      "updated_at": "2026-02-27T00:43:46Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34256",
      "labels": [
        "feature request",
        "rocm",
        "model-bash"
      ]
    },
    {
      "number": 34399,
      "title": "[Bug]: Nemotron 3 (all quants) take a LONG time to load",
      "author": "jiangwu300",
      "state": "open",
      "created_at": "2026-02-12T03:44:51Z",
      "updated_at": "2026-02-26T22:38:52Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34399",
      "labels": [
        "bug",
        "torch.compile"
      ]
    },
    {
      "number": 25179,
      "title": "[Performance]: Custom fused kernel tracking",
      "author": "ProExpertProg",
      "state": "open",
      "created_at": "2025-09-18T15:31:02Z",
      "updated_at": "2026-02-26T22:33:28Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/25179",
      "labels": [
        "performance",
        "torch.compile"
      ]
    },
    {
      "number": 35089,
      "title": "[RFC]: In-Tree AMD Zen CPU Backend via zentorch",
      "author": "amd-lalithnc",
      "state": "open",
      "created_at": "2026-02-23T09:36:32Z",
      "updated_at": "2026-02-26T22:23:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35089",
      "labels": [
        "rocm",
        "RFC",
        "cpu"
      ]
    },
    {
      "number": 35133,
      "title": "[CI Failure]:  mi355_4: LoRA TP Test (Distributed)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-23T18:34:13Z",
      "updated_at": "2026-02-26T21:01:27Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35133",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 32864,
      "title": "[Bug] [ROCm]: Loading Qwen3-MoE-MXFP4 Weights in v0.14.",
      "author": "tjtanaa",
      "state": "open",
      "created_at": "2026-01-22T15:31:23Z",
      "updated_at": "2026-02-26T20:25:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/32864",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35169,
      "title": "[Bug]: Memory Access Fault during Step-3.5-Flash inference (ROCM)",
      "author": "ColinZ22",
      "state": "open",
      "created_at": "2026-02-24T04:41:04Z",
      "updated_at": "2026-02-26T17:31:36Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35169",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35132,
      "title": "[CI Failure][ROCm]:  CrossLayer KV layout Distributed NixlConnector PD accuracy tests (4 GPUs)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-23T18:28:34Z",
      "updated_at": "2026-02-26T17:03:27Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35132",
      "labels": [
        "rocm",
        "ci-failure"
      ]
    },
    {
      "number": 34028,
      "title": "[Usage]: Unable to load Mistral-Small-3.2-24B-Instruct-2506-FP8 due to \"Value error, Found unknown quantization\", but the same configs worked for vllm v0.11.0",
      "author": "gabinguo",
      "state": "open",
      "created_at": "2026-02-06T23:29:20Z",
      "updated_at": "2026-02-26T16:26:17Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34028",
      "labels": [
        "usage"
      ]
    },
    {
      "number": 35321,
      "title": "[Feature]: Encoder self-attention for RocmAttentionImpl",
      "author": "micah-wil",
      "state": "open",
      "created_at": "2026-02-25T18:44:39Z",
      "updated_at": "2026-02-26T15:28:20Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35321",
      "labels": [
        "feature request",
        "rocm"
      ]
    },
    {
      "number": 33338,
      "title": "[Bug]: Crash when using presence_penalty with Qwen3-VL in v0.11.0",
      "author": "Lrcx",
      "state": "open",
      "created_at": "2026-01-29T11:54:48Z",
      "updated_at": "2026-02-26T14:39:06Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33338",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35403,
      "title": "[Bug]: Ray Compiled DAG timeout/deadlock during VLM forward pass with PP>1 and high-res images",
      "author": "emricksini-h",
      "state": "open",
      "created_at": "2026-02-26T14:25:49Z",
      "updated_at": "2026-02-26T14:25:49Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35403",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35266,
      "title": "[Bug]: Missing opening brace for Qwen3.5 streaming tool calls",
      "author": "AsterisMono",
      "state": "open",
      "created_at": "2026-02-25T06:35:20Z",
      "updated_at": "2026-02-26T11:22:38Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35266",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 33099,
      "title": "[Bug]: vllm Requests stuck indefinitely",
      "author": "joules-zapata-pfh",
      "state": "open",
      "created_at": "2026-01-26T15:59:00Z",
      "updated_at": "2026-02-26T10:39:14Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33099",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 27641,
      "title": "[Bug]: Streaming tool call randomly failed when using gpt-oss-120b/20b",
      "author": "dayeguilaiye",
      "state": "open",
      "created_at": "2025-10-28T08:18:34Z",
      "updated_at": "2026-02-26T10:14:20Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/27641",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 32730,
      "title": "[Bug]: CPU tests hit CUDA path when VLLM_TARGET_DEVICE=cpu [CPU Backend]",
      "author": "wjhrdy",
      "state": "open",
      "created_at": "2026-01-20T22:18:02Z",
      "updated_at": "2026-02-26T09:46:58Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/32730",
      "labels": [
        "bug",
        "cpu"
      ]
    },
    {
      "number": 35028,
      "title": "[Bug]: RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmEx",
      "author": "shahizat",
      "state": "open",
      "created_at": "2026-02-21T17:55:04Z",
      "updated_at": "2026-02-26T09:22:05Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35028",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35387,
      "title": "[Performance]: MTP causes 76% latency regression on Qwen3-Next-80B-A3B-Instruct-FP8",
      "author": "tdoublep",
      "state": "open",
      "created_at": "2026-02-26T09:21:10Z",
      "updated_at": "2026-02-26T09:21:29Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35387",
      "labels": [
        "performance"
      ]
    },
    {
      "number": 19197,
      "title": "[Bug]: \u6a21\u578b\u8fd0\u884c\u671f\u95f4\uff0c\u62a5\u9519TimeoutError: RPC call to execute_model timed out.\uff0c\u5bfc\u81f4\u6a21\u578b\u9000\u51fa\u3002",
      "author": "nvliajia",
      "state": "open",
      "created_at": "2025-06-05T09:19:47Z",
      "updated_at": "2026-02-26T08:05:20Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/19197",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 29478,
      "title": "[Bug]: Olmo-3 vLLM Online API generates gibberish",
      "author": "hanseungwook",
      "state": "open",
      "created_at": "2025-11-26T05:17:36Z",
      "updated_at": "2026-02-26T07:48:16Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29478",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35381,
      "title": "[Bug]: Not able to run Qwen3-8B-NVFP4 on B200",
      "author": "IKACE",
      "state": "open",
      "created_at": "2026-02-26T07:46:35Z",
      "updated_at": "2026-02-26T07:46:35Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35381",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 27699,
      "title": "[Bug]: Inductor fails to fuse pointwise ops with sequence parallelism + async TP",
      "author": "ProExpertProg",
      "state": "open",
      "created_at": "2025-10-29T00:26:57Z",
      "updated_at": "2026-02-26T06:49:04Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/27699",
      "labels": [
        "bug",
        "torch.compile",
        "keep-open"
      ]
    },
    {
      "number": 34205,
      "title": "[Bug]: Set env ROCP_TOOL_ATTACH=1 caused vllm server stopped",
      "author": "BigFaceBoy",
      "state": "open",
      "created_at": "2026-02-10T03:50:51Z",
      "updated_at": "2026-02-26T02:51:42Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34205",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35138,
      "title": "[Bug]: Qwen/Qwen3.5-397B-A17B-FP8 and Qwen/Qwen3.5-397B-A17B has accuracy issues when running with Flashinfer Attention backend on Blackwell.",
      "author": "xinli-sw",
      "state": "open",
      "created_at": "2026-02-23T19:06:48Z",
      "updated_at": "2026-02-25T14:58:02Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35138",
      "labels": [
        "bug",
        "nvidia"
      ]
    },
    {
      "number": 30679,
      "title": "[RFC]: Replace `torch.cuda` API with `torch.accelerator` for better hardware compatiblity.",
      "author": "jikunshang",
      "state": "open",
      "created_at": "2025-12-15T08:23:45Z",
      "updated_at": "2026-02-25T06:49:05Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/30679",
      "labels": [
        "RFC"
      ]
    },
    {
      "number": 33638,
      "title": "[Bug]: DeepSeekV3.1 with fp8 kvcache in v0.15.0 produces garbled output",
      "author": "lyg95",
      "state": "open",
      "created_at": "2026-02-03T03:26:24Z",
      "updated_at": "2026-02-25T06:33:10Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33638",
      "labels": [
        "bug",
        "deepseek"
      ]
    },
    {
      "number": 28003,
      "title": "[Usage]:",
      "author": "amitmvyas",
      "state": "open",
      "created_at": "2025-11-03T21:19:15Z",
      "updated_at": "2026-02-25T02:15:42Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/28003",
      "labels": [
        "usage",
        "stale"
      ]
    },
    {
      "number": 34994,
      "title": "[Feature]: Infrastructure Improvements for ROCm CI",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-20T22:35:58Z",
      "updated_at": "2026-02-24T23:46:19Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34994",
      "labels": [
        "feature request",
        "rocm"
      ]
    },
    {
      "number": 35235,
      "title": "[CI Failure]:  mi355_1: Multi-Modal Models Test (Extended) 1",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-24T21:17:28Z",
      "updated_at": "2026-02-24T21:18:41Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35235",
      "labels": [
        "ci-failure"
      ]
    }
  ]
}