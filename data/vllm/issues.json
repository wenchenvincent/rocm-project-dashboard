{
  "collected_at": "2026-02-28T08:15:42Z",
  "issues": [
    {
      "number": 35521,
      "title": "[Bug]: Suffix decoding crashes with assert total_num_scheduled_tokens > 0",
      "author": "rosstang",
      "state": "open",
      "created_at": "2026-02-27T15:35:13Z",
      "updated_at": "2026-02-28T07:43:02Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35521",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35599,
      "title": "[Bug]: cpu version compile failed in 0.16.0",
      "author": "wanghualex1-wq",
      "state": "open",
      "created_at": "2026-02-28T07:40:54Z",
      "updated_at": "2026-02-28T07:40:54Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35599",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35344,
      "title": "[Bug]: Value error, Model architectures ['Qwen3_5MoeForConditionalGeneration'] are not supported for now",
      "author": "Dong09",
      "state": "open",
      "created_at": "2026-02-26T01:19:33Z",
      "updated_at": "2026-02-28T03:59:57Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35344",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 33638,
      "title": "[Bug]: DeepSeekV3.1 with fp8 kvcache in v0.15.0 produces garbled output",
      "author": "lyg95",
      "state": "open",
      "created_at": "2026-02-03T03:26:24Z",
      "updated_at": "2026-02-28T03:34:01Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33638",
      "labels": [
        "bug",
        "deepseek"
      ]
    },
    {
      "number": 27604,
      "title": "[Bug]: Is Flashinfer Attn backend supposed to work with FP8 KV cache on Hopper?",
      "author": "jmkuebler",
      "state": "open",
      "created_at": "2025-10-27T20:22:37Z",
      "updated_at": "2026-02-28T03:31:34Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/27604",
      "labels": [
        "bug",
        "stale",
        "nvidia"
      ]
    },
    {
      "number": 35191,
      "title": "[Bug]: Qwen3.5 397B FP8 fills 1TB RAM and OOM killed with high-concurrency multimodal requests",
      "author": "FWao",
      "state": "open",
      "created_at": "2026-02-24T12:45:47Z",
      "updated_at": "2026-02-28T03:05:11Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35191",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35569,
      "title": "[Bug]: [ROCm] ROCM_ATTN backend shows ~8.5% systematic score deviation on Qwen3-VL-Reranker pooling",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-28T02:31:46Z",
      "updated_at": "2026-02-28T03:04:20Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35569",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35507,
      "title": "[Bug]: Assertionerror when using OffloadingConnector: assert len(req.block_hashes) >= num_gpu_blocks",
      "author": "liunianxuxie",
      "state": "open",
      "created_at": "2026-02-27T12:20:49Z",
      "updated_at": "2026-02-28T02:49:49Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35507",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 25870,
      "title": "[Bug]: gpt-oss-120b failed with data-parallel-size >1",
      "author": "Yaomt",
      "state": "open",
      "created_at": "2025-09-29T09:36:08Z",
      "updated_at": "2026-02-28T02:16:41Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/25870",
      "labels": [
        "bug",
        "unstale"
      ]
    },
    {
      "number": 26646,
      "title": "[Bug]: KV cache can't be quantized for Qwen3-Next",
      "author": "drrros",
      "state": "open",
      "created_at": "2025-10-11T16:43:48Z",
      "updated_at": "2026-02-28T02:16:27Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/26646",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 29725,
      "title": "[Bug]: Responses API: Streaming returns ResponseTextDeltaEvent instead of ResponseFunctionCallArgumentsDeltaEvent for tool calls while using non-harmony models",
      "author": "sumitaryal",
      "state": "open",
      "created_at": "2025-11-29T12:12:14Z",
      "updated_at": "2026-02-28T02:13:55Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29725",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 29737,
      "title": "[Installation]: RM has detected an NVML/RM version mismatch",
      "author": "helxsz",
      "state": "open",
      "created_at": "2025-11-29T23:50:58Z",
      "updated_at": "2026-02-28T02:13:53Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29737",
      "labels": [
        "installation",
        "stale"
      ]
    },
    {
      "number": 35028,
      "title": "[Bug]: RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasGemmEx",
      "author": "shahizat",
      "state": "open",
      "created_at": "2026-02-21T17:55:04Z",
      "updated_at": "2026-02-27T23:21:17Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35028",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35391,
      "title": "[Bug]: Value error, Model architectures ['Qwen3_5ForConditionalGeneration'] are not supported for now",
      "author": "Iam-a-Savage-coder",
      "state": "open",
      "created_at": "2026-02-26T10:08:09Z",
      "updated_at": "2026-02-27T20:32:32Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35391",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35541,
      "title": "[Bug]: vLLM hangs indefinitely with low `num_gpu_blocks_override`",
      "author": "kvcache670",
      "state": "open",
      "created_at": "2026-02-27T19:45:33Z",
      "updated_at": "2026-02-27T20:09:55Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35541",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 24499,
      "title": "[Installation]: \u5728amd rocm6.4.2\u8bbe\u5907\u4e0a\u5b89\u88c5\u62a5\u9519",
      "author": "tanghui886",
      "state": "open",
      "created_at": "2025-09-09T10:32:26Z",
      "updated_at": "2026-02-27T19:17:42Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/24499",
      "labels": [
        "installation",
        "rocm",
        "unstale"
      ]
    },
    {
      "number": 35528,
      "title": "[Feature]: Support serving ModelOpt W4A8 MXFP4+FP8 checkpoints",
      "author": "zeryx",
      "state": "open",
      "created_at": "2026-02-27T16:47:50Z",
      "updated_at": "2026-02-27T17:07:33Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35528",
      "labels": [
        "feature request"
      ]
    },
    {
      "number": 33445,
      "title": "[RFC] Remove mandatory ray installation",
      "author": "yewentao256",
      "state": "open",
      "created_at": "2026-01-30T20:37:53Z",
      "updated_at": "2026-02-27T16:53:11Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/33445",
      "labels": []
    },
    {
      "number": 35504,
      "title": "[Bug]: qwen3-coder-next inference randomly hangs, accuracy degradation in 0.16.0+ with TP > 1 and  fuse_allreduce_rms=False (H100s on PCIe)",
      "author": "vitush93",
      "state": "open",
      "created_at": "2026-02-27T11:05:38Z",
      "updated_at": "2026-02-27T16:39:23Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35504",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35465,
      "title": "[Bug]: No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization)",
      "author": "DongZhaoXiong",
      "state": "open",
      "created_at": "2026-02-27T02:32:50Z",
      "updated_at": "2026-02-27T16:32:24Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35465",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35522,
      "title": "[Bug]: Music Flamingo `ValueError: Following weights were not initialized from checkpoint: {'audio_tower.pos_emb.freqs'}`",
      "author": "denadai2",
      "state": "open",
      "created_at": "2026-02-27T15:35:38Z",
      "updated_at": "2026-02-27T16:09:00Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35522",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 34154,
      "title": "[Bug]: Failing to run DeepSeek-OCR on Radeon GPUs (memory fault)",
      "author": "umarinkovic",
      "state": "open",
      "created_at": "2026-02-09T17:41:29Z",
      "updated_at": "2026-02-27T16:00:54Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34154",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35519,
      "title": "[Bug]: Qwen3.5 NVFP4 models crash on ARM64 GB10 DGX Spark (CUDA illegal instruction during generation)",
      "author": "EmilHaase",
      "state": "open",
      "created_at": "2026-02-27T15:11:48Z",
      "updated_at": "2026-02-27T15:39:45Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35519",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 34449,
      "title": "[Bug]: GLM-5-FP8 malformed tool calls",
      "author": "TALLEC-Scott",
      "state": "open",
      "created_at": "2026-02-12T18:20:25Z",
      "updated_at": "2026-02-27T14:26:21Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34449",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35286,
      "title": "[Bug]: Qwen3.5-MoE failed with enable_lora",
      "author": "hjh0119",
      "state": "open",
      "created_at": "2026-02-25T11:42:35Z",
      "updated_at": "2026-02-27T12:10:22Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35286",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35501,
      "title": "[Bug]: fp8_blockscale_gemm JIT compilation fails on vLLM Docker image \u2014 missing cublasLt.h, nvrtc.h, and -lnvrtc",
      "author": "onurguner",
      "state": "open",
      "created_at": "2026-02-27T10:16:10Z",
      "updated_at": "2026-02-27T11:18:09Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35501",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 32730,
      "title": "[Bug]: CPU tests hit CUDA path when VLLM_TARGET_DEVICE=cpu [CPU Backend]",
      "author": "wjhrdy",
      "state": "open",
      "created_at": "2026-01-20T22:18:02Z",
      "updated_at": "2026-02-27T09:18:06Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/32730",
      "labels": [
        "bug",
        "cpu"
      ]
    },
    {
      "number": 35319,
      "title": "[Bug]: Multi-Node inference with PP > 1 crashes after processing completions request with non-None `logprobs` parameter.",
      "author": "JeanPaulShapo",
      "state": "open",
      "created_at": "2026-02-25T18:31:26Z",
      "updated_at": "2026-02-27T07:16:21Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35319",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 31990,
      "title": "[Bug]: [H200] Qwen3-Next-80B-A3B-Instruct-FP8 TP1 DP4 EP4 CUDA illegal memory error",
      "author": "jhaotingc",
      "state": "open",
      "created_at": "2026-01-08T19:34:00Z",
      "updated_at": "2026-02-27T06:36:53Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/31990",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35464,
      "title": "[Bug]: Fix GLM-OCR text_config model_type handling",
      "author": "iron-shaper",
      "state": "open",
      "created_at": "2026-02-27T02:31:14Z",
      "updated_at": "2026-02-27T02:33:50Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35464",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 23816,
      "title": "[Bug]: multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 8 leaked shared_memory objects to clean up at shutdow",
      "author": "gkm0120",
      "state": "open",
      "created_at": "2025-08-28T09:09:39Z",
      "updated_at": "2026-02-27T02:21:32Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/23816",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 29670,
      "title": "[Bug]: single node 8 h20,ebo failed!",
      "author": "bleedingfight",
      "state": "open",
      "created_at": "2025-11-28T11:15:07Z",
      "updated_at": "2026-02-27T02:18:28Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29670",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 29676,
      "title": "[Bug]: First call to llama model takes too much time compared to subsequent ones",
      "author": "ozancaglayan",
      "state": "open",
      "created_at": "2025-11-28T13:44:13Z",
      "updated_at": "2026-02-27T02:18:27Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/29676",
      "labels": [
        "bug",
        "stale"
      ]
    },
    {
      "number": 34641,
      "title": "[ROCm] Default VLLM_ROCM_USE_AITER_FP4BMM=True crashes on MI300X (gfx942)",
      "author": "khairulkabir1661",
      "state": "open",
      "created_at": "2026-02-16T19:02:38Z",
      "updated_at": "2026-02-27T01:59:36Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34641",
      "labels": [
        "rocm"
      ]
    },
    {
      "number": 35163,
      "title": "[Bug]: AMD docker image still using torch 2.9 despite 2.10.0 in `requirements/rocm-build.txt`",
      "author": "mikaylagawarecki",
      "state": "open",
      "created_at": "2026-02-24T01:47:25Z",
      "updated_at": "2026-02-27T01:56:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35163",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 34118,
      "title": "[Feature]: [ROCm]: GPTQ INT4 MoE kernel fails on non-SiLU activations - no Marlin fallback on AMD",
      "author": "ehartford",
      "state": "open",
      "created_at": "2026-02-09T05:28:24Z",
      "updated_at": "2026-02-27T01:32:00Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34118",
      "labels": [
        "feature request",
        "rocm"
      ]
    },
    {
      "number": 34399,
      "title": "[Bug]: Nemotron 3 (all quants) take a LONG time to load",
      "author": "jiangwu300",
      "state": "open",
      "created_at": "2026-02-12T03:44:51Z",
      "updated_at": "2026-02-26T22:38:52Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34399",
      "labels": [
        "bug",
        "torch.compile"
      ]
    },
    {
      "number": 25179,
      "title": "[Performance]: Custom fused kernel tracking",
      "author": "ProExpertProg",
      "state": "open",
      "created_at": "2025-09-18T15:31:02Z",
      "updated_at": "2026-02-26T22:33:28Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/25179",
      "labels": [
        "performance",
        "torch.compile"
      ]
    },
    {
      "number": 35089,
      "title": "[RFC]: In-Tree AMD Zen CPU Backend via zentorch",
      "author": "amd-lalithnc",
      "state": "open",
      "created_at": "2026-02-23T09:36:32Z",
      "updated_at": "2026-02-26T22:23:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35089",
      "labels": [
        "rocm",
        "RFC",
        "cpu"
      ]
    },
    {
      "number": 35133,
      "title": "[CI Failure]:  mi355_4: LoRA TP Test (Distributed)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-23T18:34:13Z",
      "updated_at": "2026-02-26T21:01:27Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35133",
      "labels": [
        "ci-failure"
      ]
    },
    {
      "number": 32864,
      "title": "[Bug] [ROCm]: Loading Qwen3-MoE-MXFP4 Weights in v0.14.",
      "author": "tjtanaa",
      "state": "open",
      "created_at": "2026-01-22T15:31:23Z",
      "updated_at": "2026-02-26T20:25:43Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/32864",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35169,
      "title": "[Bug]: Memory Access Fault during Step-3.5-Flash inference (ROCM)",
      "author": "ColinZ22",
      "state": "open",
      "created_at": "2026-02-24T04:41:04Z",
      "updated_at": "2026-02-26T17:31:36Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35169",
      "labels": [
        "bug",
        "rocm"
      ]
    },
    {
      "number": 35132,
      "title": "[CI Failure][ROCm]:  CrossLayer KV layout Distributed NixlConnector PD accuracy tests (4 GPUs)",
      "author": "AndreasKaratzas",
      "state": "open",
      "created_at": "2026-02-23T18:28:34Z",
      "updated_at": "2026-02-26T17:03:27Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35132",
      "labels": [
        "rocm",
        "ci-failure"
      ]
    },
    {
      "number": 34028,
      "title": "[Usage]: Unable to load Mistral-Small-3.2-24B-Instruct-2506-FP8 due to \"Value error, Found unknown quantization\", but the same configs worked for vllm v0.11.0",
      "author": "gabinguo",
      "state": "open",
      "created_at": "2026-02-06T23:29:20Z",
      "updated_at": "2026-02-26T16:26:17Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34028",
      "labels": [
        "usage"
      ]
    },
    {
      "number": 35403,
      "title": "[Bug]: Ray Compiled DAG timeout/deadlock during VLM forward pass with PP>1 and high-res images",
      "author": "emricksini-h",
      "state": "open",
      "created_at": "2026-02-26T14:25:49Z",
      "updated_at": "2026-02-26T14:25:49Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35403",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 35266,
      "title": "[Bug]: Missing opening brace for Qwen3.5 streaming tool calls",
      "author": "AsterisMono",
      "state": "open",
      "created_at": "2026-02-25T06:35:20Z",
      "updated_at": "2026-02-26T11:22:38Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/35266",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 27641,
      "title": "[Bug]: Streaming tool call randomly failed when using gpt-oss-120b/20b",
      "author": "dayeguilaiye",
      "state": "open",
      "created_at": "2025-10-28T08:18:34Z",
      "updated_at": "2026-02-26T10:14:20Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/27641",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 19197,
      "title": "[Bug]: \u6a21\u578b\u8fd0\u884c\u671f\u95f4\uff0c\u62a5\u9519TimeoutError: RPC call to execute_model timed out.\uff0c\u5bfc\u81f4\u6a21\u578b\u9000\u51fa\u3002",
      "author": "nvliajia",
      "state": "open",
      "created_at": "2025-06-05T09:19:47Z",
      "updated_at": "2026-02-26T08:05:20Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/19197",
      "labels": [
        "bug"
      ]
    },
    {
      "number": 27699,
      "title": "[Bug]: Inductor fails to fuse pointwise ops with sequence parallelism + async TP",
      "author": "ProExpertProg",
      "state": "open",
      "created_at": "2025-10-29T00:26:57Z",
      "updated_at": "2026-02-26T06:49:04Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/27699",
      "labels": [
        "bug",
        "torch.compile",
        "keep-open"
      ]
    },
    {
      "number": 34205,
      "title": "[Bug]: Set env ROCP_TOOL_ATTACH=1 caused vllm server stopped",
      "author": "BigFaceBoy",
      "state": "open",
      "created_at": "2026-02-10T03:50:51Z",
      "updated_at": "2026-02-26T02:51:42Z",
      "html_url": "https://github.com/vllm-project/vllm/issues/34205",
      "labels": [
        "bug",
        "rocm"
      ]
    }
  ]
}