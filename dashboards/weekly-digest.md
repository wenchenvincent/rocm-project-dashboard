# Weekly Digest

Week of 2026-02-12 to 2026-02-19

## New Releases

- **vllm**: [v0.16.0](https://github.com/vllm-project/vllm/releases/tag/v0.16.0)

## PRs This Week

### pytorch
- Opened: [#175071](https://github.com/pytorch/pytorch/pull/175071) [ROCm] check if rocm_env.sh exists before sourcing (@CalebisGross)
- Opened: [#174896](https://github.com/pytorch/pytorch/pull/174896) [release 2.11] Triton Update 18.02.2026 (@atalman)
- Opened: [#175075](https://github.com/pytorch/pytorch/pull/175075) [ROCm] allow PYTORCH_AOTRITON_COMMIT env var to override git (@CalebisGross)
- Opened: [#175073](https://github.com/pytorch/pytorch/pull/175073) [inductor] Decompose mm to pointwise mul when K==1 (@romanmeta)
- Opened: [#175079](https://github.com/pytorch/pytorch/pull/175079) Link aotriton when USE_MEM_EFF_ATTENTION is enabled for ROCm (@CalebisGross)
- Opened: [#175148](https://github.com/pytorch/pytorch/pull/175148) [ROCm] Remove stale paths from hipify build script (@CalebisGross)
- Opened: [#175076](https://github.com/pytorch/pytorch/pull/175076) [ROCm] auto-detect GPU architecture for cpp extensions (@CalebisGross)
- Opened: [#175299](https://github.com/pytorch/pytorch/pull/175299) [benchmark] Skip pytorch_CycleGAN_and_pix2pix from inductor  (@pytorchbot)
- Opened: [#175303](https://github.com/pytorch/pytorch/pull/175303) [ROCM] Refactor BFloat16 implementation for native usage of  (@anatoliylitv)
- Opened: [#175097](https://github.com/pytorch/pytorch/pull/175097) [DO NOT REBASE][ROCm][Inductor] New Inductor benchmarker bas (@naromero77amd)
- Opened: [#175285](https://github.com/pytorch/pytorch/pull/175285) [Inductor] Allow subgraphs to be benchmarked with async pipe (@PaulZhang12)
- Opened: [#175286](https://github.com/pytorch/pytorch/pull/175286) [ROCm] No-fence in normalization kernel (@anatoliylitv)
- Opened: [#174835](https://github.com/pytorch/pytorch/pull/174835) [ROCm] Fix hipsparselt build integration and add TN mode sup (@gyllstromk)
- Opened: [#175159](https://github.com/pytorch/pytorch/pull/175159) [ROCm] forward fix #174087, take 4 (@pytorchbot)
- Opened: [#175055](https://github.com/pytorch/pytorch/pull/175055) [ROCm] Fix test_profiler_cuda_sync_events for ROCTracer (@roshanrateria)
- Opened: [#175162](https://github.com/pytorch/pytorch/pull/175162) Increase TransformerEncoderLayer gelu TF32 tolerance for ROC (@CalebisGross)
- Opened: [#175161](https://github.com/pytorch/pytorch/pull/175161) Increase test_Linear TF32 tolerance for ROCm (@CalebisGross)
- Opened: [#175056](https://github.com/pytorch/pytorch/pull/175056) [ROCm] Enable mixed-precision batchnorm tests with relaxed t (@roshanrateria)
- Opened: [#175053](https://github.com/pytorch/pytorch/pull/175053) [ROCm/CUDA] Fix bfloat16 reflection padding decomposition te (@roshanrateria)
- Opened: [#175050](https://github.com/pytorch/pytorch/pull/175050) [ROCm] Enable test_sac_ilp_case1 on MI300/MI350 (@roshanrateria)
- Opened: [#175180](https://github.com/pytorch/pytorch/pull/175180) [rocm] Fix build_amd.py failure when MSLK submodule is missi (@radeksm)
- Opened: [#175054](https://github.com/pytorch/pytorch/pull/175054) [ROCm] Fix test_compile_kernel_advanced TF32 precision misma (@roshanrateria)
- Opened: [#175095](https://github.com/pytorch/pytorch/pull/175095) Revert "[CI] Enable TIMM pretrained model caching on shared  (@jeffdaily)
- Opened: [#175094](https://github.com/pytorch/pytorch/pull/175094) Revert "[fix] DISABLED test_index (__main__.DistTensorOpsTes (@jeffdaily)
- Opened: [#175096](https://github.com/pytorch/pytorch/pull/175096) Update inductor expected accuracy files (@pytorchbot)
- Merged: [#172179](https://github.com/pytorch/pytorch/pull/172179) Bump fbgemm and torchrec pinned commit (@pytorchbot)

### jax
- Opened: [#35115](https://github.com/jax-ml/jax/pull/35115) [ROCm] Add hip_threefry2x32_ffi to stable custom call target (@AratiGanesh)
- Opened: [#35190](https://github.com/jax-ml/jax/pull/35190) [ROCm] rocm CI job with a job that executes the tests (@alekstheod)
- Opened: [#35111](https://github.com/jax-ml/jax/pull/35111) [ROCm] Add ROCm support for eigh export backwards compatibil (@AratiGanesh)
- Opened: [#35102](https://github.com/jax-ml/jax/pull/35102) [ROCm] Set release rpaths to rocm so targets (@alekstheod)
- Opened: [#35011](https://github.com/jax-ml/jax/pull/35011) [ROCm] Remove incorrect ROCm lowering for scaled_matmul to p (@Ruturaj4)
- Merged: [#31768](https://github.com/jax-ml/jax/pull/31768) [ROCm] Support lowering through PJRT_Triton_Extension (@amd-jianli12)
- Merged: [#34829](https://github.com/jax-ml/jax/pull/34829) [ROCm] Add ROCm LU solver to backward compatibility tests (@AratiGanesh)
- Merged: [#34929](https://github.com/jax-ml/jax/pull/34929) [ROCm] Modified `test_with_memory_space` to include ROCm tes (@tsrw2048)
- Merged: [#34966](https://github.com/jax-ml/jax/pull/34966) [ROCm] Skip test_batch_axis_sharding_jvp (@AratiGanesh)
- Merged: [#34561](https://github.com/jax-ml/jax/pull/34561) [ROCm] Enable ToeplitzSymmetricConstruction and condition nu (@tsrw2048)
- Merged: [#34870](https://github.com/jax-ml/jax/pull/34870) [ROCm] Add ROCm backward compatibility test for lu_pivots_to (@AratiGanesh)
- Merged: [#34501](https://github.com/jax-ml/jax/pull/34501) [ROCm] Enable cuda array interface test on ROCm (@magaonka-amd)
- Merged: [#34894](https://github.com/jax-ml/jax/pull/34894) Add ROCm backward compatibility test for cholesky solver (@AratiGanesh)
- Merged: [#33157](https://github.com/jax-ml/jax/pull/33157) [ROCm] Resolve undefined behavior in bitshift unit test (@mminutoli)
- Merged: [#34974](https://github.com/jax-ml/jax/pull/34974) [ROCm] Added ROCm tests to `device_test.py` unit test file. (@tsrw2048)
- Merged: [#34574](https://github.com/jax-ml/jax/pull/34574) [ROCm] Enable test_variadic_reduce_window on GPUs (@magaonka-amd)
- Merged: [#34500](https://github.com/jax-ml/jax/pull/34500) [ROCm] Fix KeyError for bytes_reservable_limit on ROCm (@magaonka-amd)
- Merged: [#34675](https://github.com/jax-ml/jax/pull/34675) [ROCm] Update Skip Reason Outputs (@gulsumgudukbay)
- Merged: [#34474](https://github.com/jax-ml/jax/pull/34474) [ROCm] Added support for GESVDJ on ROCm devices (@tsrw2048)
- Merged: [#34802](https://github.com/jax-ml/jax/pull/34802) [ROCm] Enable lax backend scipy tests on ROCm GPUs (@magaonka-amd)
- Merged: [#34774](https://github.com/jax-ml/jax/pull/34774) [ROCm] Enable lobpcg tests on ROCm platform (@magaonka-amd)
- Merged: [#34575](https://github.com/jax-ml/jax/pull/34575) [ROCm] Enabled the Triton Pallas tests to run for ROCm. (@tsrw2048)
- Merged: [#34893](https://github.com/jax-ml/jax/pull/34893) [ROCm] Enable test deviceless aot compile test on ROCm (@magaonka-amd)
- Merged: [#34494](https://github.com/jax-ml/jax/pull/34494) [ROCm] Enabled ROCm devices to default to Jacobi SVD on smal (@tsrw2048)
- Merged: [#34875](https://github.com/jax-ml/jax/pull/34875) [ROCm] Add ROCm backward compatibility test for threefry2x32 (@AratiGanesh)
- Merged: [#34832](https://github.com/jax-ml/jax/pull/34832) [ROCm] Add ROCm support to annotate_device_placement backwar (@AratiGanesh)
- Merged: [#34869](https://github.com/jax-ml/jax/pull/34869) [ROCm] Added back compatibility test for hipsolver_gesvd (@tsrw2048)

### vllm
- Opened: [#34879](https://github.com/vllm-project/vllm/pull/34879) [ROCm][CI] Removing all blocking labels from MI355 until sta (@AndreasKaratzas)
- Opened: [#34570](https://github.com/vllm-project/vllm/pull/34570) [ROCm][AITER] Fix aiter paged_attention_v1 decode for slidin (@AndreasKaratzas)
- Opened: [#34878](https://github.com/vllm-project/vllm/pull/34878) [ROCm][Test] Fix beam search determinism failures from batch (@AndreasKaratzas)
- Opened: [#34839](https://github.com/vllm-project/vllm/pull/34839) [ROCm][CI] Cleaning and restructuring amd-ci legacy pipeline (@AndreasKaratzas)
- Opened: [#34599](https://github.com/vllm-project/vllm/pull/34599) [ROCm][CI] Fix spec decode logprobs flakiness and parametriz (@AndreasKaratzas)
- Opened: [#34567](https://github.com/vllm-project/vllm/pull/34567) [CI] Fix ColBERT HF comparison tests on AMD CI + refactor (@AndreasKaratzas)
- Opened: [#34631](https://github.com/vllm-project/vllm/pull/34631) [ROCm] Make Whisper causal attention backend-agnostic (@laudney)
- Opened: [#34709](https://github.com/vllm-project/vllm/pull/34709) [ROCm] Enable wvSplitK skinny GEMM kernel for RDNA4/gfx1x de (@laudney)
- Opened: [#34632](https://github.com/vllm-project/vllm/pull/34632) [ROCm] Add MXFP4 inline dequant Triton kernel for RDNA4/gfx1 (@laudney)
- Opened: [#34644](https://github.com/vllm-project/vllm/pull/34644) [release 2.11] Update to torch 2.11-rc1 (@atalman)
- Opened: [#34848](https://github.com/vllm-project/vllm/pull/34848) [ROCm] Add extra step in config initialization to populate c (@gshtras)
- Opened: [#34647](https://github.com/vllm-project/vllm/pull/34647) [ROCm] Add hardware detection for FP4 BMM to prevent MI300X  (@khairulkabir1661)
- Opened: [#34735](https://github.com/vllm-project/vllm/pull/34735) [AMD][CI] Fix test_custom_allreduce for A100 testgroup (@rjrock)
- Opened: [#34652](https://github.com/vllm-project/vllm/pull/34652) [AMD][CI] Fix test new_weight_syncing/rlhf.py (@rjrock)
- Opened: [#34760](https://github.com/vllm-project/vllm/pull/34760) Add platform method to enable custom collective ops registra (@nkm-meta)
- Opened: [#34655](https://github.com/vllm-project/vllm/pull/34655) [CI][AMD][BugFix] Skip tests in test_unquantized_backend_sel (@rasmith)
- Opened: [#34776](https://github.com/vllm-project/vllm/pull/34776) [WIP] Add Warmup to `vllm bench throughput` (@micah-wil)
- Opened: [#34455](https://github.com/vllm-project/vllm/pull/34455) [Bugfix] Remove assert causing hipErrorStreamCaptureUnsuppor (@JadenMathias)
- Opened: [#34825](https://github.com/vllm-project/vllm/pull/34825) [CI] temporarily disable multi-node tests (@robertgshaw2-redhat)
- Opened: [#34801](https://github.com/vllm-project/vllm/pull/34801) [ROCm][Kernel] Add fused MoE exllama GEMM for compressed-ten (@mgehre-amd)
- Opened: [#34677](https://github.com/vllm-project/vllm/pull/34677) [Bugfix][CPU] Fix basic unit tests failing in CPU platforms (@jasonyanwenl)
- Opened: [#34753](https://github.com/vllm-project/vllm/pull/34753) [ROCm][CI] Removed hard-coded attn backend requirement for Q (@AndreasKaratzas)
- Opened: [#34653](https://github.com/vllm-project/vllm/pull/34653) [BugFix] [Build] fix string literals comparison in indexer_k (@hongxiayang)
- Opened: [#34726](https://github.com/vllm-project/vllm/pull/34726) [ROCm] Enable DBO (Dynamic Batch Optimization) on ROCm (@raviguptaamd)
- Opened: [#34741](https://github.com/vllm-project/vllm/pull/34741) [ROCm] Enable FP8 KV-cache and relax constraints for RDNA4 c (@laudney)
- Opened: [#34507](https://github.com/vllm-project/vllm/pull/34507) [Bugfix] Fix fused MoE int32 overflow in stride*offset witho (@haosdent)
- Opened: [#34692](https://github.com/vllm-project/vllm/pull/34692) [ROCm] Enable DeepEP ROCm as all2allbackend for AMD GPUs.  (@lcskrishna)
- Opened: [#34688](https://github.com/vllm-project/vllm/pull/34688) [ROCm] Enable bitsandbytes quantization support on ROCm (@Abdennacer-Badaoui)
- Opened: [#34566](https://github.com/vllm-project/vllm/pull/34566) [CI][Metrics] Stabilize tests with polling and subprocess gu (@AndreasKaratzas)
- Opened: [#34589](https://github.com/vllm-project/vllm/pull/34589) [ROCm][CI] Fix plugins test group; updating terratorch and d (@AndreasKaratzas)
- Opened: [#34629](https://github.com/vllm-project/vllm/pull/34629) Targeting the MI355 agent pool with all existing tests (@Alexei-V-Ivanov-AMD)
- Opened: [#34590](https://github.com/vllm-project/vllm/pull/34590) [CI][Frontend] Return 422 instead of 500 for invalid Anthrop (@AndreasKaratzas)
- Opened: [#34468](https://github.com/vllm-project/vllm/pull/34468) [CI][Entrypoints] Validate detokenize token IDs to prevent i (@AndreasKaratzas)
- Opened: [#34537](https://github.com/vllm-project/vllm/pull/34537) [Kernels] Fix Helion GPU utils to use platform-agnostic devi (@AndreasKaratzas)
- Opened: [#34538](https://github.com/vllm-project/vllm/pull/34538) [ROCm][CI] Guard sparse MLA backend imports for ROCm compati (@AndreasKaratzas)
- Opened: [#34543](https://github.com/vllm-project/vllm/pull/34543) [Bugfix] Fix ROCm UVA CPU weight offloading broken by #32993 (@AndreasKaratzas)
- Opened: [#34454](https://github.com/vllm-project/vllm/pull/34454) [Bugfix]: Fix structured output in multi-turn gpt-oss (@bbrowning)
- Opened: [#34447](https://github.com/vllm-project/vllm/pull/34447) [ROCm][CI] Pin TorchCodec to v0.10.0 for ROCm compatibility (@AndreasKaratzas)
- Opened: [#34431](https://github.com/vllm-project/vllm/pull/34431) [ROCm][quantization] improve OCP weight quant parser robust (@xuebwang-amd)
- Merged: [#34181](https://github.com/vllm-project/vllm/pull/34181) [CI][AMD][BugFix] Use torch.testing.assert_close instead of  (@rasmith)
- Merged: [#34228](https://github.com/vllm-project/vllm/pull/34228) Add unit tests for fp8 output fusion of triton_attn (@bringlein)
- Merged: [#34324](https://github.com/vllm-project/vllm/pull/34324) Fixed whisper CPU test that does not spawn properly. (@almayne)
- Merged: [#34320](https://github.com/vllm-project/vllm/pull/34320) [Bugfix] Fix Dynamo unexpected keyword argument  (@samutamm)
- Merged: [#34279](https://github.com/vllm-project/vllm/pull/34279) [Bugfix] Fix fused MoE IMA (sans chunking) by using int64 fo (@tlrmchlsmth)
- Merged: [#33493](https://github.com/vllm-project/vllm/pull/33493) Perf tuning and expansion of cases covered for wvSplitKrc (@amd-hhashemi)
- Merged: [#32183](https://github.com/vllm-project/vllm/pull/32183) [MM Encoder] Add Triton ViT attention backend (@Isotr0py)
- Merged: [#34378](https://github.com/vllm-project/vllm/pull/34378) Use paged_attention_v1 for sliding window decode in rocm_ait (@iseeyuan)
- Merged: [#34294](https://github.com/vllm-project/vllm/pull/34294) [CI] Heavy refactoring of Voxtral multimodal audio model tes (@AndreasKaratzas)
- Merged: [#32993](https://github.com/vllm-project/vllm/pull/32993) [Feature] Support CPU Offloading without Pytorch Pinned Memo (@wzhao18)
- Merged: [#34047](https://github.com/vllm-project/vllm/pull/34047) [ROCm][CI] Fix serving tokens test failures (@AndreasKaratzas)
- Merged: [#32458](https://github.com/vllm-project/vllm/pull/32458) [CI][BugFix] Fix silent failure in shellcheck hook and basel (@junuxyz)
- Merged: [#34130](https://github.com/vllm-project/vllm/pull/34130) [Perf] fused_moe: add int4_w4a16 benchmark support and tunin (@mgehre-amd)
- Merged: [#29556](https://github.com/vllm-project/vllm/pull/29556) [CI/Build] Skip ray tests on ROCm (@rjrock)
- Merged: [#31834](https://github.com/vllm-project/vllm/pull/31834) [CI/Build] Enable test_kv_cache_events_dp for AMD (@rjrock)
- Merged: [#30272](https://github.com/vllm-project/vllm/pull/30272) [CI/Build] Use spawn subprocess for ROCm (@rjrock)
- Merged: [#8515](https://github.com/vllm-project/vllm/pull/8515) [Model] Add mistral function calling format to all models lo (@patrickvonplaten)
- Merged: [#34192](https://github.com/vllm-project/vllm/pull/34192) [ROCm] Enable MXFP4 MoE weight pre-shuffling on gfx950 and u (@dllehr-amd)
- Merged: [#33626](https://github.com/vllm-project/vllm/pull/33626) [ci] Integrate AMD tests into CI (@khluu)

### sglang
- Opened: [#18930](https://github.com/sgl-project/sglang/pull/18930) [AMD] Unit tests for mtp in GLM-4.7  (@almaslof)
- Opened: [#18992](https://github.com/sgl-project/sglang/pull/18992) [AMD] Enable ROCm kvcache JIT path and add AMD CI coverage. (@hubertlu-tw)
- Opened: [#18911](https://github.com/sgl-project/sglang/pull/18911) [AMD] [DO NOT MERGE] [GLM-5 Day 0] Add GLM-5 nightly test (@michaelzhang-ai)
- Opened: [#18978](https://github.com/sgl-project/sglang/pull/18978) [AMD]  Fix mi35x dsv32 mtp nightly (@bingxche)
- Opened: [#18982](https://github.com/sgl-project/sglang/pull/18982) [Doc] Add `flashinfer_deepgemm` to `--fp8-gemm-backend` (@mmangkad)
- Opened: [#18951](https://github.com/sgl-project/sglang/pull/18951) [DO NOT MERGE] [AMD] Fix MI35x DeepSeek-V3.2 MTP nightly tes (@michaelzhang-ai)
- Opened: [#18972](https://github.com/sgl-project/sglang/pull/18972) [AMD] ROCm7.2: Add /sgl-workspace/aiter to PYTHONPATH (@HaiShaw)
- Opened: [#18811](https://github.com/sgl-project/sglang/pull/18811) [AMD] fix: hip rotary fallback avoiding CUDA JIT (@alphabetc1)
- Opened: [#18862](https://github.com/sgl-project/sglang/pull/18862) Update torch to 2.10.0 (@Fridge003)
- Opened: [#18919](https://github.com/sgl-project/sglang/pull/18919) [bugfix?] update outdated unittest document (@SoluMilken)
- Opened: [#18734](https://github.com/sgl-project/sglang/pull/18734) [Docs] Refactor SGLang Diffusion Docs (@qianyue76)
- Opened: [#18916](https://github.com/sgl-project/sglang/pull/18916) [TorchAO] Enable TorchAO LinearMethod and TorchAOConfig (@ZhiweiYan-96)
- Opened: [#18761](https://github.com/sgl-project/sglang/pull/18761) [AMD] Fix nightly 1-GPU test failures and bench_serving regr (@michaelzhang-ai)
- Opened: [#18920](https://github.com/sgl-project/sglang/pull/18920) ROCm use rotary_embedding from sgl-kernel (@HaiShaw)
- Opened: [#18922](https://github.com/sgl-project/sglang/pull/18922) Revert "[AMD] Fix RotaryEmbedding crash on AMD/ROCm (regress (@HaiShaw)
- Opened: [#18903](https://github.com/sgl-project/sglang/pull/18903) [AMD] Fix RotaryEmbedding crash on AMD/ROCm (regression from (@michaelzhang-ai)
- Opened: [#18860](https://github.com/sgl-project/sglang/pull/18860) update pre-commit config (@SoluMilken)
- Opened: [#18753](https://github.com/sgl-project/sglang/pull/18753) [AMD] Fix/qwen3 5 amd rope cutedsl fallback (@andyluo7)
- Opened: [#18836](https://github.com/sgl-project/sglang/pull/18836) [AMD] Fix sgl-model-gateway Build Errors in ROCm Docker Rele (@bingxche)
- Opened: [#18805](https://github.com/sgl-project/sglang/pull/18805) add testcase for Qwen3 235b Instruct 2507 (@mqhc2020)
- Opened: [#18733](https://github.com/sgl-project/sglang/pull/18733) Add DeepSeek V32 PD disaggregation test (@ShangmingCai)
- Opened: [#18788](https://github.com/sgl-project/sglang/pull/18788) Cleanup unused rerun stages (@ispobock)
- Opened: [#18654](https://github.com/sgl-project/sglang/pull/18654) [schedule] Fix streaming return of customized_info (@yinghai)
- Opened: [#18716](https://github.com/sgl-project/sglang/pull/18716) [AMD] Fix Multimodal Test 1 GPU (@bingxche)
- Opened: [#18741](https://github.com/sgl-project/sglang/pull/18741) Build ROCm7.2 Image with latest AITER v0.1.10.post3 (@HaiShaw)
- Opened: [#18738](https://github.com/sgl-project/sglang/pull/18738) [AMD] Test aiter regression (@yctseng0211)
- Opened: [#18708](https://github.com/sgl-project/sglang/pull/18708) Revert: [diffusion] fix: fix fsdp #18187 (@bingxche)
- Opened: [#18684](https://github.com/sgl-project/sglang/pull/18684) [AMD] Pad MoE weights and scales (@mqhc2020)
- Opened: [#18707](https://github.com/sgl-project/sglang/pull/18707) [AMD] reset AMD image release time and reduce CI queue time (@yctseng0211)
- Opened: [#18698](https://github.com/sgl-project/sglang/pull/18698) [AMD] Enable release image build for ROCm 7.2.0 (@akao-amd)
- Merged: [#18252](https://github.com/sgl-project/sglang/pull/18252) [4/N] Quantization Refactor: Quark MoE schemes (@TamirBaydasov)
- Merged: [#17993](https://github.com/sgl-project/sglang/pull/17993) [3/N] Quantization Refactor: ModelSlim MoE schemes (@TamirBaydasov)
- Merged: [#18395](https://github.com/sgl-project/sglang/pull/18395) [Doc] Convert the speculative decoding notebook to markdow (@alphabetc1)
- Merged: [#17503](https://github.com/sgl-project/sglang/pull/17503) [2/N] Quantization Refactor: Compressed tensors MoE schemes (@TamirBaydasov)
- Merged: [#18437](https://github.com/sgl-project/sglang/pull/18437) [AMD] MORI-EP inter kernel type switch (@Duyi-Wang)
- Merged: [#18496](https://github.com/sgl-project/sglang/pull/18496) [FIX] Correct JIT kernel compilation on newer GPUs with outd (@muse-coder)
- Merged: [#18602](https://github.com/sgl-project/sglang/pull/18602) [CI] feat: add early exit to wait_for_server when process di (@alphabetc1)
- Merged: [#18456](https://github.com/sgl-project/sglang/pull/18456) [diffusion][MUSA] fix: MUSA platform breakage caused by PR # (@yeahdongcn)
- Merged: [#18480](https://github.com/sgl-project/sglang/pull/18480) Added cuda availability guard (@mattteochen)
- Merged: [#18619](https://github.com/sgl-project/sglang/pull/18619) [diffusion] feat: support tp for qwen-image-edit-2511 (@xiaoyewww)
- Merged: [#18607](https://github.com/sgl-project/sglang/pull/18607) [AMD] Fix accuracy issue when running TP4 dsv3 model with mt (@1am9trash)
- Merged: [#18187](https://github.com/sgl-project/sglang/pull/18187) [diffusion] fix: fix fsdp (@mickqian)
- Merged: [#18500](https://github.com/sgl-project/sglang/pull/18500) [Flashinfer Autotune] Fix FlashInfer FP4 MoE autotuning cras (@YAMY1234)
- Merged: [#17799](https://github.com/sgl-project/sglang/pull/17799) [AMD] rocm 7.2 image release, PR test, Nightly Test (@yctseng0211)
- Merged: [#18095](https://github.com/sgl-project/sglang/pull/18095) [diffusion] docs: consolidate diffusion documentation into d (@qianyue76)

### triton
- Opened: [#9509](https://github.com/triton-lang/triton/pull/9509) [AMD] Enable supportBitwidth{16|32}Elementwise in TargetInfo (@antiagainst)
- Opened: [#9506](https://github.com/triton-lang/triton/pull/9506) [AMD] Fix TensorDescType shared memory size for WS captures (@PMylon)
- Opened: [#9502](https://github.com/triton-lang/triton/pull/9502) [AMD][BACKEND] Cherry pick pr 9487 to rel 3.7 (@AmdSampsa)
- Opened: [#9496](https://github.com/triton-lang/triton/pull/9496) [AMD][gfx1250] Fix tensordesc index after kernel launch chan (@antiagainst)
- Opened: [#9442](https://github.com/triton-lang/triton/pull/9442) [AMD][BACKEND] Fix OOM bug in pipelining with padded layout  (@pabloantoniom)
- Opened: [#9494](https://github.com/triton-lang/triton/pull/9494) Revert "[AMD] Don't use s_waitcnt to lower global barrier fo (@antiagainst)
- Opened: [#9490](https://github.com/triton-lang/triton/pull/9490) [AMD]Fix a bug about CGA-layout in AccelerateAMDMatmul.  (@yangshuxin)
- Opened: [#9455](https://github.com/triton-lang/triton/pull/9455) [AMD] Enable floating-point sanitizer (FpSan) support (@kelesvol)
- Opened: [#9487](https://github.com/triton-lang/triton/pull/9487) [AMD][BACKEND] Properly handle PointerTypes in v_perm Conver (@AlexAUT)
- Opened: [#9449](https://github.com/triton-lang/triton/pull/9449) [AMD] Added hw FP upcast conversions for gfx1250 (@ravil-mobile)
- Opened: [#9476](https://github.com/triton-lang/triton/pull/9476) [AMD] Disable True16 for assembler on gfx11 (#9447) (@jataylo)
- Opened: [#9448](https://github.com/triton-lang/triton/pull/9448) [AMD] Add enableMISched option to MIR swap pipeline (@tyb0807)
- Opened: [#9447](https://github.com/triton-lang/triton/pull/9447) [AMD] Disable True16 for assembler on gfx11 (@ptrojahn)
- Opened: [#9440](https://github.com/triton-lang/triton/pull/9440) [AMD] Update gfx1250 MXFP FA example kernel (@borontion)
- Merged: [#9374](https://github.com/triton-lang/triton/pull/9374) Reapply "[AMD] Introduce PartitionedSharedEncodingAttr" (#93 (@plognjen)
- Merged: [#9302](https://github.com/triton-lang/triton/pull/9302) [AMD][gfx1250] Support TDM in software pipelining  (@yangshuxin)

### migraphx
- Opened: [#4621](https://github.com/ROCm/AMDMIGraphX/pull/4621) [AIMIGRAPHX-571] Rewrite convolutions to GEMMs for constant  (@eddieliao)
- Opened: [#4611](https://github.com/ROCm/AMDMIGraphX/pull/4611) Improve the logic for reaches() in find_splits (@aarushjain29)
- Opened: [#4620](https://github.com/ROCm/AMDMIGraphX/pull/4620) [AIMIGRAPHX-542] implement argmin and argmax as reduce ops (@bdevorem)
- Opened: [#4616](https://github.com/ROCm/AMDMIGraphX/pull/4616) [AIMIGRAPHX-544] Parallel compilation for dynamic graphs (@shivadbhavsar)
- Opened: [#4619](https://github.com/ROCm/AMDMIGraphX/pull/4619) AIMIGRAPHX-578 Reintroduce blaze for better ref gemm perform (@kahmed10)
- Opened: [#4617](https://github.com/ROCm/AMDMIGraphX/pull/4617) Fuse GQA local window as kv-cache attention (@turneram)
- Opened: [#4613](https://github.com/ROCm/AMDMIGraphX/pull/4613) [7.2.1] Backport Inference Server improvements (@causten)
- Opened: [#4609](https://github.com/ROCm/AMDMIGraphX/pull/4609) Propagate constant optimization (@pnikolic-amd)
- Opened: [#4614](https://github.com/ROCm/AMDMIGraphX/pull/4614) Onnxruntime Weekly Sync 2026-02-13 (@github-actions[bot])
- Opened: [#4606](https://github.com/ROCm/AMDMIGraphX/pull/4606) Refactor rnn ops to op builders (@pfultz2)
- Opened: [#4608](https://github.com/ROCm/AMDMIGraphX/pull/4608) Use rocBLAS GEMV for skinny GEMM (M=1 or N=1) to improve per (@klin2024)
- Opened: [#4607](https://github.com/ROCm/AMDMIGraphX/pull/4607) Optimize 1x1 and Depthwise Convolution for Small Shapes (@klin2024)
- Opened: [#4615](https://github.com/ROCm/AMDMIGraphX/pull/4615) rocMLIR Weekly Sync 2026-02-15 (@github-actions[bot])
- Opened: [#4610](https://github.com/ROCm/AMDMIGraphX/pull/4610) Fix bug in gather rewrite with nhwc shapes (@pfultz2)
- Merged: [#4410](https://github.com/ROCm/AMDMIGraphX/pull/4410) clamping the scale (@aarushjain29)
- Merged: [#4510](https://github.com/ROCm/AMDMIGraphX/pull/4510) [BugFix] - Fix tile byte size overflow for LDS memory when p (@ivarusic-amd)
- Merged: [#4362](https://github.com/ROCm/AMDMIGraphX/pull/4362) disable matching for dynamic shapes (@shivadbhavsar)
- Merged: [#4443](https://github.com/ROCm/AMDMIGraphX/pull/4443) [AIMIGRAPHX-326] Fix "reduce_sum: axes: value out of range"  (@pfultz2)
- Merged: [#4445](https://github.com/ROCm/AMDMIGraphX/pull/4445) Show attributes in onnx trace (@pfultz2)
- Merged: [#4393](https://github.com/ROCm/AMDMIGraphX/pull/4393) Flash decoding round 1; AIMIGRAPHX-242 (@bdevorem)
- Merged: [#4396](https://github.com/ROCm/AMDMIGraphX/pull/4396) Refactor GroupQueryAttention (@turneram)
- Merged: [#4600](https://github.com/ROCm/AMDMIGraphX/pull/4600) Have eliminate_pad skip over non-constant padding, ref tests (@CharlieL7)
- Merged: [#4567](https://github.com/ROCm/AMDMIGraphX/pull/4567) Filter zero arg operators during ONNX Parsing (@TedThemistokleous)

### aiter
- Opened: [#2062](https://github.com/ROCm/aiter/pull/2062) Add ENABLE_CK=0 build option for Triton-only builds (@sunway513)
- Opened: [#2049](https://github.com/ROCm/aiter/pull/2049) [TRITON] Add smoothquant int8 MoE kernel (@nsusanto)
- Opened: [#2044](https://github.com/ROCm/aiter/pull/2044) [TEST] Flash Attention Integration CI from fork (@micmelesse)
- Opened: [#2060](https://github.com/ROCm/aiter/pull/2060) GFX1250 Kernels - GEMMa8w8 blockscale (@amirumoAMD)
- Opened: [#2056](https://github.com/ROCm/aiter/pull/2056) Enabling FPMX4 GEMM on non-FPMX4 devices (Navi31 in particul (@ekuznetsov139)
- Opened: [#2053](https://github.com/ROCm/aiter/pull/2053) Support per_block for Pa PS (@ZhangLirong-amd)
- Opened: [#2055](https://github.com/ROCm/aiter/pull/2055) Silence certain warnings stemming from CK (@Micky774)
- Opened: [#2052](https://github.com/ROCm/aiter/pull/2052) Prepare repository for size optimization (@sunway513)
- Opened: [#2050](https://github.com/ROCm/aiter/pull/2050) Add Model Benchmarking Tool (@lucas-santos-amd)
- Opened: [#2047](https://github.com/ROCm/aiter/pull/2047) GFX1250 Gluon MoE A4W4 Kernel (@farlukas)
- Opened: [#2042](https://github.com/ROCm/aiter/pull/2042) upload mla_a8w8_qh64_qseqlen4_gqaratio16 co in MI300 (@minmengdie)
- Opened: [#2039](https://github.com/ROCm/aiter/pull/2039) Introduce HipKittens based nhead=128 MLA Kernel (@ruanjm)
- Opened: [#2057](https://github.com/ROCm/aiter/pull/2057) hotfix a8w8 gemm config (@valarLip)
- Opened: [#2048](https://github.com/ROCm/aiter/pull/2048) [Gluon] Unified Attention 3D development for gfx12 (@k50112113)
- Opened: [#2040](https://github.com/ROCm/aiter/pull/2040) [OPUS] enhance opus UT by adding more tests (@carlushuang)
- Opened: [#2045](https://github.com/ROCm/aiter/pull/2045) [TRITON] fav3 sage optmization (@Chi-Chu319)
- Opened: [#2041](https://github.com/ROCm/aiter/pull/2041) Rocking/fix benchmark mha fwd (@rocking5566)
- Opened: [#2043](https://github.com/ROCm/aiter/pull/2043) Fix qk_norm_rope_cache_quant ut (@ganyi1996ppo)
- Opened: [#2051](https://github.com/ROCm/aiter/pull/2051) Prepare repository for size optimization (@sunway513)
- Merged: [#1954](https://github.com/ROCm/aiter/pull/1954) feat(ck_tile): add a8w8 blockscale gemm with preshuffleB sup (@kensclin)
- Merged: [#2016](https://github.com/ROCm/aiter/pull/2016) tune triton gemm kernel for MI355 DSV3 DP+EP configuration (@inkcherry)
- Merged: [#2035](https://github.com/ROCm/aiter/pull/2035) Fix accuracy issues in top-p sampling kernels (@kliuae)
- Merged: [#1966](https://github.com/ROCm/aiter/pull/1966) Fix known issues in mha cpp api (@slippedJim)
- Merged: [#2030](https://github.com/ROCm/aiter/pull/2030) fix: correct duplicate knl_name in mla_asm.csv causing PP8 f (@chun-wan)
- Merged: [#2026](https://github.com/ROCm/aiter/pull/2026) remove asm mask type (@slippedJim)
- Merged: [#2024](https://github.com/ROCm/aiter/pull/2024) [gfx942]Add new GEMM configuration files for DSKR1 (@zhentaocc)
- Merged: [#1973](https://github.com/ROCm/aiter/pull/1973) Defer expensive build operations to build_ext.run() (@paradigm)
- Merged: [#2009](https://github.com/ROCm/aiter/pull/2009) enable hd192_128 cas br kernel in python test (@JaxChen29)
- Merged: [#1957](https://github.com/ROCm/aiter/pull/1957) fix mha fwd_v3 _s_buff_Q/K/V/D address overflow (@minmengdie)
- Merged: [#2037](https://github.com/ROCm/aiter/pull/2037) Add MI355X tuned GEMM configs for FP4 and FP8 (@sunway513)
- Merged: [#1978](https://github.com/ROCm/aiter/pull/1978) add rmsnorm CK_TILE_FLOAT_TO_BFLOAT16_DEFAULT compile config (@zhyajie)

### atom
- Opened: [#224](https://github.com/ROCm/ATOM/pull/224) Add Dockerfile.clean + fix linear.py shard_offset bug (@sunway513)
- Opened: [#220](https://github.com/ROCm/ATOM/pull/220) Enable Triton MXFP4 MoE on gfx950 for GPT-OSS (@ChuanLi1101)
- Opened: [#223](https://github.com/ROCm/ATOM/pull/223) Add Quark GLM4.7-MXFP4 support (@thpereir)
- Opened: [#222](https://github.com/ROCm/ATOM/pull/222) Fix prefix caching crash: recalculate num_new_tokens after b (@ChuanLi1101)
- Opened: [#212](https://github.com/ROCm/ATOM/pull/212) Fix CI container name collision for parallel matrix jobs (@sunway513)
- Opened: [#218](https://github.com/ROCm/ATOM/pull/218) Enable AllReduce+RMSNorm fusion for GPT-OSS model (@ChuanLi1101)
- Opened: [#210](https://github.com/ROCm/ATOM/pull/210) CI: Add thresholds for models accuracy tests (@gyohuangxin)
- Opened: [#208](https://github.com/ROCm/ATOM/pull/208) Not yet ready for review - Add support for Kimi-K2 and Kimi- (@thpereir)
- Opened: [#219](https://github.com/ROCm/ATOM/pull/219) mtp refine (@valarLip)
- Opened: [#209](https://github.com/ROCm/ATOM/pull/209) Fix exclude layer (@ZhangLirong-amd)
- Opened: [#217](https://github.com/ROCm/ATOM/pull/217) Kill all Docker containers before 8gpu workloads launch (@okakarpa)
- Opened: [#216](https://github.com/ROCm/ATOM/pull/216) Revert PR #215: Remove kill-containers workflow (@okakarpa)
- Opened: [#215](https://github.com/ROCm/ATOM/pull/215) Add workflow to kill Docker containers and check ROCm on MI3 (@okakarpa)
- Opened: [#214](https://github.com/ROCm/ATOM/pull/214) Revert PR #213: Remove kill-containers workflow (@okakarpa)
- Opened: [#213](https://github.com/ROCm/ATOM/pull/213) Add workflow to kill Docker containers and check ROCm on MI3 (@okakarpa)
- Opened: [#211](https://github.com/ROCm/ATOM/pull/211) Print debug logs for inference workload (@dhonnappa-amd)
- Merged: [#204](https://github.com/ROCm/ATOM/pull/204) Add GPU-free unit test suite for core engine components (@sunway513)
- Merged: [#171](https://github.com/ROCm/ATOM/pull/171) Support Qwen3-Next on ATOM Framework (@PerryZhang01)

### mori
- Opened: [#171](https://github.com/ROCm/mori/pull/171) Fix: support runtime hidden_dim for dispatch/combine (@isytwu)
- Opened: [#170](https://github.com/ROCm/mori/pull/170) Optimize: EP4 intranode kernel for FP4 dispatch + FP8 combin (@jhchouuu)
- Opened: [#169](https://github.com/ROCm/mori/pull/169) Feat: Enable intra-node FP4 dispatch and BF16 cast to FP8 co (@isytwu)

## New Issues This Week

### pytorch
- [#175189](https://github.com/pytorch/pytorch/issues/175189) [MPS] BatchNorm2d backward produces wildly wrong weight grad (@npinto)
- [#175190](https://github.com/pytorch/pytorch/issues/175190) [MPS] AvgPool2d/AdaptiveAvgPool2d backward crashes (SIGABRT) (@npinto)
- [#175282](https://github.com/pytorch/pytorch/issues/175282) `test_tensorinv_cuda_float32` fails across device types (@gderossi)
- [#174949](https://github.com/pytorch/pytorch/issues/174949) [vllm] CUBLAS_STATUS_INVALID_VALUE in cublasGemmEx after upg (@ZJY0516)
- [#175250](https://github.com/pytorch/pytorch/issues/175250) Triton OutofMemoryError when fusing multiple ops (@fynnsu)
- [#175211](https://github.com/pytorch/pytorch/issues/175211) CUDA/ROCm/Accelerator testing should replace get_device_capa (@jeffdaily)
- [#175145](https://github.com/pytorch/pytorch/issues/175145) [torch.disitributed] Persistent Deadlock when overlapping NC (@ilmarkov)
- [#174884](https://github.com/pytorch/pytorch/issues/174884) Inductor BF16 training crashes with autograd INTERNAL ASSERT (@kumartanmay-28)
- [#175025](https://github.com/pytorch/pytorch/issues/175025) Mega-Cache generate mismatched guard states when loading the (@liyineeek-source)
- [#175160](https://github.com/pytorch/pytorch/issues/175160) check if file exsits before hipifying (@trixirt)
- [#174984](https://github.com/pytorch/pytorch/issues/174984) `torch.fft.rfft` errors out with MKL FFT error when given an (@SilentTester73)
- [#175064](https://github.com/pytorch/pytorch/issues/175064) DISABLED test_index (__main__.DistTensorOpsTest) (@seemethere)
- [#174986](https://github.com/pytorch/pytorch/issues/174986) `torch.istft` returns an unhelpful internal error (@SilentTester73)
- [#174985](https://github.com/pytorch/pytorch/issues/174985) `torch.isclose` fails with a broadcast when comparing with ` (@SilentTester73)
- [#174939](https://github.com/pytorch/pytorch/issues/174939) `torch.nn.functional.scaled_dot_product_attention` crashes w (@Nyovelt)

### vllm
- [#34759](https://github.com/vllm-project/vllm/issues/34759) [Bug]: nvidia/Llama-3.3-70B-Instruct-NVFP4 Degraded / Gibber (@frankwang28)
- [#34619](https://github.com/vllm-project/vllm/issues/34619) [Bug]: Qwen3.5. `illegal memory access` (@vadiklyutiy)
- [#34817](https://github.com/vllm-project/vllm/issues/34817) [Bug]: Trying to run gpt-oss-120b on rtx pro 6000 (@chadbek)
- [#34859](https://github.com/vllm-project/vllm/issues/34859) [Bug]: missing shards from quantized checkpoint fails silent (@andrea-fasoli)
- [#34470](https://github.com/vllm-project/vllm/issues/34470) [Bug]: NVIDIA Jetson Thor: Value 'sm_110a' is not defined fo (@Kaweees)
- [#34781](https://github.com/vllm-project/vllm/issues/34781) [Feature]: parity with cuda - ROCm Kimi K2.5 disagg PD +wide (@functionstackx)
- [#34755](https://github.com/vllm-project/vllm/issues/34755) Qwen3-Coder-Next-FP8 with tool calling causes system hard-fr (@zaidorx)
- [#34583](https://github.com/vllm-project/vllm/issues/34583) [Bug] Missing Vocabulary Validation for MTP and Eagle Specul (@amadhan882)
- [#34812](https://github.com/vllm-project/vllm/issues/34812) [Bug]: GraniteMoeHybridModel not applying embedding_multipli (@gabe-l-hart)
- [#34573](https://github.com/vllm-project/vllm/issues/34573) [Installation/Runtime]: Linux ROCM7 /  RuntimeError: No HIP  (@NickJLange)
- [#34792](https://github.com/vllm-project/vllm/issues/34792) [Bug]: setting VLLM_LOGGING_LEVEL=debug breaks tool calling (@dtrifiro)
- [#34650](https://github.com/vllm-project/vllm/issues/34650) Bug: Speculative Decoding (MTP) Causes </think> Detection Fa (@cicirori)
- [#34752](https://github.com/vllm-project/vllm/issues/34752) [Bug]: Improve `--kv-cache-dtype` behavior when checkpoint s (@pavanimajety)
- [#34607](https://github.com/vllm-project/vllm/issues/34607) [Bug]: specualative decoding error in 0.15.1 (@hocop)
- [#34437](https://github.com/vllm-project/vllm/issues/34437) [Bug]: Qwen3 Next with heterogeneous GPU (FP8 overflow?) (@Nepherpitou)
- [#34641](https://github.com/vllm-project/vllm/issues/34641) [ROCm] Default VLLM_ROCM_USE_AITER_FP4BMM=True crashes on MI (@khairulkabir1661)

### sglang
- [#18749](https://github.com/sgl-project/sglang/issues/18749) [Bug] DP Attention FP4 Disagg AMD is broken (@functionstackx)
- [#18812](https://github.com/sgl-project/sglang/issues/18812) [Bug] RotaryEmbedding fallback requires CUDA_HOME on HIP (@alphabetc1)

### migraphx
- [#4618](https://github.com/ROCm/AMDMIGraphX/issues/4618) [Issue]: MIGraphX Dynamic Shape Issue ONNXRuntime (@DiarmuidKelly)

### aiter
- [#2061](https://github.com/ROCm/aiter/issues/2061) [Bug] Custom all-reduce IPC buffers use fixed VA, conflict w (@jhinpan)
- [#2059](https://github.com/ROCm/aiter/issues/2059) [Issue]: GLM-5 aiter fused_moe with SGLang + MI355 (@ozziemoreno)
- [#2054](https://github.com/ROCm/aiter/issues/2054) [Feature]: Migrate Python bindings from pybind11 to apache-t (@carlushuang)
- [#2046](https://github.com/ROCm/aiter/issues/2046) Questions about topk_per_row_kernel in topk_plain_kernels.cu (@zjin-lcf)

### atom
- [#221](https://github.com/ROCm/ATOM/issues/221) [Issue]: ATOM fails on Qwen3 model when the flag "--enable_p (@vecheruk-amd)
