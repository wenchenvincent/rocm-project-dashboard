# Weekly Digest

Week of 2026-02-21 to 2026-02-28

## New Releases

- **vllm**: [v0.16.0](https://github.com/vllm-project/vllm/releases/tag/v0.16.0)
- **sglang**: [v0.5.9](https://github.com/sgl-project/sglang/releases/tag/v0.5.9)

## PRs This Week

### pytorch
- Opened: [#175553](https://github.com/pytorch/pytorch/pull/175553) [TESTING] Triton37 ROCm testing PR (@iupaikov-amd)
- Opened: [#175870](https://github.com/pytorch/pytorch/pull/175870) [BE] Apply PEP 604 type annotations to rest of torch/_* (@Lucaskabela)
- Opened: [#176046](https://github.com/pytorch/pytorch/pull/176046) [inductor][UT] Fix test_pattern_matcher benchmark environmen (@naromero77amd)
- Opened: [#175555](https://github.com/pytorch/pytorch/pull/175555) [BE] Apply pep604 to `torch/_dynamo` (@Lucaskabela)
- Opened: [#175765](https://github.com/pytorch/pytorch/pull/175765) Handle FakeScriptObject in save_graph_repro (@aorenste)
- Opened: [#175883](https://github.com/pytorch/pytorch/pull/175883) [opaque obj] Fix check (@angelayi)
- Opened: [#175510](https://github.com/pytorch/pytorch/pull/175510) Mark DeviceMesh.__init__ as skip (@angelayi)
- Opened: [#175593](https://github.com/pytorch/pytorch/pull/175593) [vision hash update] update the pinned vision hash (@pytorchupdatebot)
- Opened: [#175776](https://github.com/pytorch/pytorch/pull/175776) [DTensor] fix max.dim/min.dim strategy (@pianpwk)
- Opened: [#175753](https://github.com/pytorch/pytorch/pull/175753) [ROCm] Skipped all the tests that fail on ROCm with latest t (@iupaikov-amd)
- Opened: [#175992](https://github.com/pytorch/pytorch/pull/175992) [ROCm][CI] fix flaky test_ddp_graphs (@jithunnair-amd)
- Opened: [#175960](https://github.com/pytorch/pytorch/pull/175960) [DTensor] Auto-append output placement for .out variant ops (@anshul-si)
- Opened: [#175935](https://github.com/pytorch/pytorch/pull/175935) [fx] Fix edge case (@angelayi)
- Opened: [#175698](https://github.com/pytorch/pytorch/pull/175698) return float("inf")  if having invalid config for codegen tr (@ZixinYang)
- Opened: [#175959](https://github.com/pytorch/pytorch/pull/175959) [DTensor] Move .out pointwise ops into their category lists (@anshul-si)
- Opened: [#175468](https://github.com/pytorch/pytorch/pull/175468) [DO NOT MERGE] Test AMD Capacity. (@saienduri)
- Opened: [#175701](https://github.com/pytorch/pytorch/pull/175701) Make hipify input file reading more robust (@davispuh)
- Opened: [#175849](https://github.com/pytorch/pytorch/pull/175849) [ROCm] Check for atleast one compilation for each rank (@anvishwa-amd)
- Opened: [#175834](https://github.com/pytorch/pytorch/pull/175834) [ROCm] Retry heuristic query once on non-success status (@jagadish-amd)
- Opened: [#175832](https://github.com/pytorch/pytorch/pull/175832) [ROCm] enable autotune cache UT (@anvishwa-amd)
- Opened: [#175810](https://github.com/pytorch/pytorch/pull/175810) [rocm][hipsparselt] Enable hipsparselt in caffe2 HIP builds (@gyllstromk)
- Opened: [#175784](https://github.com/pytorch/pytorch/pull/175784) [ROCm][CI] Use gfx942 for rocm nightly binaries (@amdfaa)
- Opened: [#175868](https://github.com/pytorch/pytorch/pull/175868) [ROCm] Unskip matmul accuracy tests for hipblas/hipblaslt (@ethanwee1)
- Opened: [#175759](https://github.com/pytorch/pytorch/pull/175759) [CI][ROCm] Use sccache tip-of-tree for theRock nightly build (@ethanwee1)
- Opened: [#175766](https://github.com/pytorch/pytorch/pull/175766) [ROCm] Added CUDA check to test_pattern_matcher (@pytorchbot)
- Opened: [#175767](https://github.com/pytorch/pytorch/pull/175767) [ROCm][CI] Upgrade ROCm CI to 7.2 - 4/N (@pytorchbot)
- Opened: [#175559](https://github.com/pytorch/pytorch/pull/175559) [Do not Merge] Revert "[ROCm] Upgrade GCC version to version (@jagadish-amd)

### jax
- Opened: [#35288](https://github.com/jax-ml/jax/pull/35288) [ROCm] Implement Mosaic GPU detection and Auto-Skips (@gulsumgudukbay)
- Opened: [#35435](https://github.com/jax-ml/jax/pull/35435) [ROCm] Use custom repo to bring rocm plugin wheels (@alekstheod)
- Opened: [#35373](https://github.com/jax-ml/jax/pull/35373) [ROCm] Replace abseiltest with py test in rocm bazel (@alekstheod)
- Merged: [#35102](https://github.com/jax-ml/jax/pull/35102) [ROCm] Set release rpaths to rocm so targets (@alekstheod)
- Merged: [#34601](https://github.com/jax-ml/jax/pull/34601) [ROCm] Enable neural network tests on ROCm (@phambinhfin)
- Merged: [#31768](https://github.com/jax-ml/jax/pull/31768) [ROCm] Support lowering through PJRT_Triton_Extension (@amd-jianli12)

### vllm
- Opened: [#35280](https://github.com/vllm-project/vllm/pull/35280) custom dataset img support base64 (@flutist)
- Opened: [#35596](https://github.com/vllm-project/vllm/pull/35596) [ROCm][Quantization] Enable moe_wna16 on ROCm via Triton fal (@brucechanglongxu)
- Opened: [#35245](https://github.com/vllm-project/vllm/pull/35245) [ROCm][WIP]: Fused aiter rope kvcache mla (@Rohan138)
- Opened: [#35597](https://github.com/vllm-project/vllm/pull/35597) [ROCm][Quantization] Enable compressed-tensors WNA16 test on (@brucechanglongxu)
- Opened: [#35595](https://github.com/vllm-project/vllm/pull/35595) [ROCm][Quantization] Enable experts_int8 on ROCm (@brucechanglongxu)
- Opened: [#35198](https://github.com/vllm-project/vllm/pull/35198) [ROCm] [Release] Change the package from `aiter` to `amd-ait (@tjtanaa)
- Opened: [#35571](https://github.com/vllm-project/vllm/pull/35571) [ROCm][CI] Parametrize vision score tests across attention b (@AndreasKaratzas)
- Opened: [#35580](https://github.com/vllm-project/vllm/pull/35580) [CI] Defining extended V1 e2e + engine tests (@AndreasKaratzas)
- Opened: [#35071](https://github.com/vllm-project/vllm/pull/35071) [ROCm][CI] Expose tests to AMD production CI and fix amdsmi  (@AndreasKaratzas)
- Opened: [#35069](https://github.com/vllm-project/vllm/pull/35069) [ROCm] Derive device capability from GCN arch string without (@AndreasKaratzas)
- Opened: [#35170](https://github.com/vllm-project/vllm/pull/35170) [ROCm][CI] Adding infiniband mappings for moriio tests (@AndreasKaratzas)
- Opened: [#35152](https://github.com/vllm-project/vllm/pull/35152) [ROCm][CI] Disable skinny GEMMs in language model standard t (@AndreasKaratzas)
- Opened: [#35560](https://github.com/vllm-project/vllm/pull/35560) [WIP][Bugfix][ROCm] Fix MXFP4 online quantization for MoE mo (@SandishKumarHN)
- Opened: [#35466](https://github.com/vllm-project/vllm/pull/35466) [CI/Build] CPU release supports both of AVX2 and AVX512 (@majian4work)
- Opened: [#35527](https://github.com/vllm-project/vllm/pull/35527) [ROCm] Add `stablelm` Head Size 80 To Supported Head Sizes F (@micah-wil)
- Opened: [#35553](https://github.com/vllm-project/vllm/pull/35553) [ROCm][CI] Fix tool use test stability - disable skinny GEMM (@AndreasKaratzas)
- Opened: [#35105](https://github.com/vllm-project/vllm/pull/35105) [Refactor][Kernel] Add global helper to deduplicate vectoriz (@LopezCastroRoberto)
- Opened: [#35533](https://github.com/vllm-project/vllm/pull/35533) [ROCm]: fix aiter rope functionalization (@Rohan138)
- Opened: [#35334](https://github.com/vllm-project/vllm/pull/35334) [ROCm] Enabling encoder and encoder-decoder on ROCm and AITE (@gshtras)
- Opened: [#35180](https://github.com/vllm-project/vllm/pull/35180) [ROCm]: Enable customop and rope+kvcache fusion for AITER Ro (@Rohan138)
- Opened: [#35404](https://github.com/vllm-project/vllm/pull/35404) [Bugfix][Model] Fix gpt-oss batch invariance (@jzakrzew)
- Opened: [#35538](https://github.com/vllm-project/vllm/pull/35538) docs: Add kernel/operator fusions reference page (@Copilot)
- Opened: [#35253](https://github.com/vllm-project/vllm/pull/35253) Enabling some B200-specific tests on MI355 (@Alexei-V-Ivanov-AMD)
- Opened: [#35182](https://github.com/vllm-project/vllm/pull/35182) [DO NOT MERGE] Reorganize inputs (@DarkLight1337)
- Opened: [#35246](https://github.com/vllm-project/vllm/pull/35246) [ROCm] Refactor ROCm attention backend selection logic (@SageMoore)
- Opened: [#35485](https://github.com/vllm-project/vllm/pull/35485) [Bugfix][ROCm] Disable full CUDA graph capture on RDNA3/RDNA (@haosdent)
- Opened: [#35427](https://github.com/vllm-project/vllm/pull/35427) [Refactor] Fix maxsim cuda platform and add cli to control i (@yewentao256)
- Opened: [#35093](https://github.com/vllm-project/vllm/pull/35093) [ROCm] add tuned moe_wna16_triton kernel configs for CDNA4 (@amd-asalykov)
- Opened: [#35491](https://github.com/vllm-project/vllm/pull/35491) [ROCm][Quantization] support amd-quark quantized Qwen3.5 mod (@xuebwang-amd)
- Opened: [#35483](https://github.com/vllm-project/vllm/pull/35483) Add AMD AITER MLA fusion optimization for DeepSeek models (@khairulkabir1661)
- Opened: [#35196](https://github.com/vllm-project/vllm/pull/35196) [ROCm] [CI] Gate the changes to `Dockerfile.rocm_base`. (@tjtanaa)
- Opened: [#35316](https://github.com/vllm-project/vllm/pull/35316) [ROCm][Quantization] add quark w4a8 mxfp4_fp8 for LinearLaye (@divakar-amd)
- Opened: [#35095](https://github.com/vllm-project/vllm/pull/35095) [MLA] Add fused Triton concat+quantize kernel for fp8 decode (@elvircrn)
- Opened: [#35302](https://github.com/vllm-project/vllm/pull/35302) [Bugfix][Hardware][AMD] Support all MoE activations in WNA16 (@c0de128)
- Opened: [#35304](https://github.com/vllm-project/vllm/pull/35304) [Bugfix][Hardware][AMD] Fix startup hang on ROCm gfx1151 in  (@c0de128)
- Opened: [#35250](https://github.com/vllm-project/vllm/pull/35250) [Bugfix][Hardware][AMD] Gate FP4 ops on gfx950 to prevent MI (@c0de128)
- Opened: [#35239](https://github.com/vllm-project/vllm/pull/35239) [ROCm][CI] Added MI325 mirrors (stage C) (@AndreasKaratzas)
- Opened: [#35042](https://github.com/vllm-project/vllm/pull/35042) [Platform] Add current_platform.num_compute_units interface (@jikunshang)
- Opened: [#35322](https://github.com/vllm-project/vllm/pull/35322) [ROCm][CI] Amending deletion of AMD mirror (@AndreasKaratzas)
- Opened: [#35265](https://github.com/vllm-project/vllm/pull/35265) [ROCm][CI] Extending attention backend coverage for Eagle sp (@AndreasKaratzas)
- Opened: [#35156](https://github.com/vllm-project/vllm/pull/35156) [BUGFIX][Qwen3.5] Hardcode `mlp.gate` as not quantizable  (@vadiklyutiy)
- Opened: [#35049](https://github.com/vllm-project/vllm/pull/35049) [ROCm][CI] Disable skinny GEMMs in multimodal tests to fix n (@AndreasKaratzas)
- Opened: [#35011](https://github.com/vllm-project/vllm/pull/35011) remove cuda check in `top_k_top_p_triton` kernel (@jikunshang)
- Opened: [#35232](https://github.com/vllm-project/vllm/pull/35232) [Build] Fix LTO build for ROCm when default compiler is GCC  (@davispuh)
- Opened: [#35043](https://github.com/vllm-project/vllm/pull/35043) [ROCm][CI] Fix spec decode profile assertion and logprob tes (@AndreasKaratzas)
- Opened: [#35052](https://github.com/vllm-project/vllm/pull/35052) [ROCm][CI] Fix realtime test timeouts caused by aiter JIT co (@AndreasKaratzas)
- Opened: [#35008](https://github.com/vllm-project/vllm/pull/35008) [CI] Stabilizing ROCm amd-ci signal and minor name fix in up (@AndreasKaratzas)
- Merged: [#34861](https://github.com/vllm-project/vllm/pull/34861) [1/N] Elastic EP Milestone 2 (@itayalroy)
- Merged: [#34301](https://github.com/vllm-project/vllm/pull/34301) [ROCm][Quantization] Add Composable Kernel (CK) backend supp (@dllehr-amd)
- Merged: [#34390](https://github.com/vllm-project/vllm/pull/34390) [Kernel] [Helion] [7/N] Use HOP to represent Helion Kernel c (@gmagogsfm)
- Merged: [#34320](https://github.com/vllm-project/vllm/pull/34320) [Bugfix] Fix Dynamo unexpected keyword argument  (@samutamm)
- Merged: [#30357](https://github.com/vllm-project/vllm/pull/30357) [ROCm][Quantization] GPT OSS Upstream MoE wmxfp4_afp8 with s (@maleksan85)
- Merged: [#26847](https://github.com/vllm-project/vllm/pull/26847) [Frontend][torch.compile] CompilationConfig Overhaul (#20283 (@morrison-turnansky)
- Merged: [#34387](https://github.com/vllm-project/vllm/pull/34387) [ROCm] Update the torch version in rocm_build.txt to use the (@SageMoore)
- Merged: [#34157](https://github.com/vllm-project/vllm/pull/34157) [ROCm] Add dynamic mxfp4 quantization for DeepSeek V2 projec (@dllehr-amd)
- Merged: [#34336](https://github.com/vllm-project/vllm/pull/34336) [Bugfix] fix device_name for routing replay (@Li-Yongwen)
- Merged: [#33992](https://github.com/vllm-project/vllm/pull/33992) [Bugfix] Fix CUDA compatibility path setting for both datace (@ehfd)
- Merged: [#34848](https://github.com/vllm-project/vllm/pull/34848) [ROCm] Add extra step in config initialization to populate c (@gshtras)
- Merged: [#33807](https://github.com/vllm-project/vllm/pull/33807) [UX] Add `--moe-backend` arg for explicit kernel selection (@mgoin)
- Merged: [#34985](https://github.com/vllm-project/vllm/pull/34985) [CI][AMD][BugFix] Add  torch.cuda.set_device to test_punica_ (@rasmith)
- Merged: [#34773](https://github.com/vllm-project/vllm/pull/34773) [Misc][LoRA] Increase max vocab size limit to 258048 in logi (@bhoomit)
- Merged: [#34677](https://github.com/vllm-project/vllm/pull/34677) [Bugfix][CPU] Fix basic unit tests failing in CPU platforms (@jasonyanwenl)
- Merged: [#34100](https://github.com/vllm-project/vllm/pull/34100) Convert wvSplitKQ to 16x16 MFMA in prep for mi4xx. (@amd-hhashemi)
- Merged: [#34302](https://github.com/vllm-project/vllm/pull/34302) [ModelBash][DSV3] Add TRTLLM DSV3 Router GEMM kernel (6% B1  (@robertgshaw2-redhat)
- Merged: [#30976](https://github.com/vllm-project/vllm/pull/30976) Use aiter triton fused_add_rmsnorm_pad for gpt-oss (@Rohan138)
- Merged: [#34923](https://github.com/vllm-project/vllm/pull/34923) [ROCm][CI] Added MI325 mirrors (@AndreasKaratzas)
- Merged: [#34878](https://github.com/vllm-project/vllm/pull/34878) [ROCm][Test] Fix beam search determinism failures from batch (@AndreasKaratzas)
- Merged: [#34879](https://github.com/vllm-project/vllm/pull/34879) [ROCm][CI] Removing all blocking labels from MI355 until sta (@AndreasKaratzas)
- Merged: [#33277](https://github.com/vllm-project/vllm/pull/33277) [ROCm][CI] Force max_num_seqs=1 on ROCm In test_sharded_stat (@micah-wil)
- Merged: [#34466](https://github.com/vllm-project/vllm/pull/34466) [CI/Build] Add opentelemetry libs in default vllm build (req (@vladmihailescu)
- Merged: [#34688](https://github.com/vllm-project/vllm/pull/34688) [ROCm] Enable bitsandbytes quantization support on ROCm (@Abdennacer-Badaoui)
- Merged: [#34570](https://github.com/vllm-project/vllm/pull/34570) [ROCm][AITER] Fix aiter paged_attention_v1 decode for slidin (@AndreasKaratzas)
- Merged: [#34567](https://github.com/vllm-project/vllm/pull/34567) [CI] Fix ColBERT HF comparison tests on AMD CI + refactor (@AndreasKaratzas)
- Merged: [#34574](https://github.com/vllm-project/vllm/pull/34574) [Frontend] Support multimodal inputs for late-interaction sc (@craftsangjae)

### sglang
- Opened: [#19549](https://github.com/sgl-project/sglang/pull/19549) [diffusion] macOS support (@yeahdongcn)
- Opened: [#19554](https://github.com/sgl-project/sglang/pull/19554) [AMD] [DO NOT MERGE] Pin tvm-ffi Commit to Prevent Tilelang  (@bingxche)
- Opened: [#19509](https://github.com/sgl-project/sglang/pull/19509) [MUSA][16/N] Add MUSA backend support for layers (@popsiclexu)
- Opened: [#19550](https://github.com/sgl-project/sglang/pull/19550) [AMD] Add AWQ AMD CI coverage and quantization platform comp (@brucechanglongxu)
- Opened: [#19103](https://github.com/sgl-project/sglang/pull/19103) [jit_kernel] Migrate cast (downcast_fp8) from sgl-kernel AOT (@Johnsonms)
- Opened: [#19498](https://github.com/sgl-project/sglang/pull/19498) [AMD] Fix MoRI EP warmup hang by restoring deepep_mode=norma (@bingxche)
- Opened: [#19515](https://github.com/sgl-project/sglang/pull/19515) [AMD] Add triggering path for multimodal test in pr-test-amd (@yctseng0211)
- Opened: [#19152](https://github.com/sgl-project/sglang/pull/19152) [Feature] Support offload and wake up of SGLang Diffusion (@klhhhhh)
- Opened: [#19543](https://github.com/sgl-project/sglang/pull/19543) [AMD] [DO NOT MERGE] test AMD new job runner (@bingxche)
- Opened: [#19479](https://github.com/sgl-project/sglang/pull/19479) [AMD] [Qwen 3.5 Day 0] Add Qwen 3.5 nightly accuracy tests (@michaelzhang-ai)
- Opened: [#19260](https://github.com/sgl-project/sglang/pull/19260) [AMD] Add suffix decoding support for ROCm and fix non-MLA s (@amd-pedghazi)
- Opened: [#19122](https://github.com/sgl-project/sglang/pull/19122) [3/n] deepseek_v2.py Refactor: Migrate MLA forward method in (@Fridge003)
- Opened: [#19113](https://github.com/sgl-project/sglang/pull/19113) [AMD] Fix AMD CI test of TestToolChoiceLfm2Moe (@michaelzhang-ai)
- Opened: [#19443](https://github.com/sgl-project/sglang/pull/19443) [AMD] [MiniMax-M2.5 Day 0] Add MiniMax-M2.5 nightly accuracy (@michaelzhang-ai)
- Opened: [#19249](https://github.com/sgl-project/sglang/pull/19249) [WIP][diffusion] kernel fusion: scale residual norm scale sh (@linfann)
- Opened: [#19499](https://github.com/sgl-project/sglang/pull/19499) [Quantization] Fix per-tensor FP8 MoE parameter identity for (@Socratesa)
- Opened: [#19324](https://github.com/sgl-project/sglang/pull/19324) [AMD] Add kimi_linear_models test on AMD GPU (@sogalin)
- Opened: [#19416](https://github.com/sgl-project/sglang/pull/19416) [AMD] remove redundancy H2D op in aiter attention backend (@AMD-yanfeiwang)
- Opened: [#19203](https://github.com/sgl-project/sglang/pull/19203) [AMD] Merge Dockerfiles for ROCm (@akao-amd)
- Opened: [#19439](https://github.com/sgl-project/sglang/pull/19439) [AMD] Fix DeepSeekV32 Nightly on MI30x (@michaelzhang-ai)
- Opened: [#19467](https://github.com/sgl-project/sglang/pull/19467) [AMD] WIP - AMD AITER Scout (@yctseng0211)
- Opened: [#19425](https://github.com/sgl-project/sglang/pull/19425) [AMD] Fix weight load shape mismatch for amd dsr1 0528 mxfp4 (@billishyahao)
- Opened: [#19422](https://github.com/sgl-project/sglang/pull/19422) [AMD] Use fused GEMM with FP8 cast for FP8 prefill (@1am9trash)
- Opened: [#19448](https://github.com/sgl-project/sglang/pull/19448) [CI] Add Claude skill for writing SGLang tests (@sglang-bot)
- Opened: [#19228](https://github.com/sgl-project/sglang/pull/19228) [AMD] optimize Kimi K2.5 fused_moe_triton performance by tun (@ZiguanWang)
- Opened: [#19440](https://github.com/sgl-project/sglang/pull/19440) [AMD] Fix the hipDeviceGetName issue in ROCm700 based docker (@hubertlu-tw)
- Opened: [#19359](https://github.com/sgl-project/sglang/pull/19359) [AMD] Fix ROCm Docker builds, update apache-tvm-ffi (@michaelzhang-ai)
- Opened: [#19362](https://github.com/sgl-project/sglang/pull/19362) [AMD] Fix EAGLE3 speculative decoding with aiter attention b (@hubertlu-tw)
- Opened: [#19417](https://github.com/sgl-project/sglang/pull/19417) [AMD] [CI] Add Tests for Aiter Allreduce Fusion (@bingxche)
- Opened: [#19358](https://github.com/sgl-project/sglang/pull/19358) Allow SGLang to still work if Triton is not installed. (@Jonahcb)
- Opened: [#19140](https://github.com/sgl-project/sglang/pull/19140) [Re-land][jit kernel] Support per_token_group_quant_8bit jit (@yuan-luo)
- Opened: [#19351](https://github.com/sgl-project/sglang/pull/19351) Update (@HaiShaw)
- Opened: [#19216](https://github.com/sgl-project/sglang/pull/19216) [AMD][with CI Fix] support two batch overlapping for mori ep (@billishyahao)
- Opened: [#19192](https://github.com/sgl-project/sglang/pull/19192) Fix nightly grok failure on rotary embedding import (@michaelzhang-ai)
- Opened: [#19247](https://github.com/sgl-project/sglang/pull/19247) [AMD] Fix accuracy while using --enable-dp-attention (@yichiche)
- Opened: [#19262](https://github.com/sgl-project/sglang/pull/19262) Some refactors on nsa_indexer.py (@Fridge003)
- Opened: [#19210](https://github.com/sgl-project/sglang/pull/19210) fix(docker): migrate ROCm Dockerfiles from setuptools-rust t (@slin1237)
- Opened: [#19161](https://github.com/sgl-project/sglang/pull/19161) Revert "[AMD] support two batch overlapping for mori ep #179 (@Fridge003)
- Opened: [#19162](https://github.com/sgl-project/sglang/pull/19162) [ROCm] Use unreg path for custom all-reduce during CUDA grap (@zyzshishui)
- Opened: [#19176](https://github.com/sgl-project/sglang/pull/19176) [AMD] ENV Flags tuning and cleanup (@HaiShaw)
- Opened: [#19165](https://github.com/sgl-project/sglang/pull/19165) Enable Building on gfx1100 (@Calandracas606)
- Opened: [#19143](https://github.com/sgl-project/sglang/pull/19143) feat: Support MXFP4 quantized dense models on AMD CDNA2/CDNA (@fengli1702)
- Merged: [#18526](https://github.com/sgl-project/sglang/pull/18526) [AMD] Enable cudagraph for aiter nsa backend and add aiter i (@wufann)
- Merged: [#18113](https://github.com/sgl-project/sglang/pull/18113) [HiCache] refactor page_first_direct io kernel (@huangtingwei9988)
- Merged: [#18319](https://github.com/sgl-project/sglang/pull/18319) [AMD] Use `tilelang` as default NSA attention backend dispat (@fxmarty-amd)
- Merged: [#17374](https://github.com/sgl-project/sglang/pull/17374)  [4/N] (Elastic EP) Back up Expert Weights in DRAM (@ympcMark)
- Merged: [#15731](https://github.com/sgl-project/sglang/pull/15731) [Perf] Eliminate the slice op for Flashinfer `trtllm_fp4_blo (@elvischenv)
- Merged: [#17968](https://github.com/sgl-project/sglang/pull/17968) [NPU][feature adapt]remote load weight feature adp npu (@littleyellowbicycle)
- Merged: [#18537](https://github.com/sgl-project/sglang/pull/18537) [MUSA][11/N] ci: add MUSA 4.3 kernel build and release pipel (@johnnycxm)
- Merged: [#18355](https://github.com/sgl-project/sglang/pull/18355) [AMD] Support Qwen3-Coder-Next on AMD platform (@yichiche)
- Merged: [#13747](https://github.com/sgl-project/sglang/pull/13747) [AMD] Support --enable-aiter-allreduce-fusion on AMD GPUs (@hubertlu-tw)
- Merged: [#18710](https://github.com/sgl-project/sglang/pull/18710) [Diffusion] non-blocking in fuse_scale_shift_kernel (@wenqf11)
- Merged: [#18805](https://github.com/sgl-project/sglang/pull/18805) [AMD] add testcases for Qwen3 235b Instruct 2507 models (@mqhc2020)
- Merged: [#18992](https://github.com/sgl-project/sglang/pull/18992) [AMD] Enable ROCm kvcache JIT path and add AMD CI coverage. (@hubertlu-tw)
- Merged: [#18911](https://github.com/sgl-project/sglang/pull/18911) [AMD] [GLM-5 Day 0] Add GLM-5 nightly test (@michaelzhang-ai)
- Merged: [#18242](https://github.com/sgl-project/sglang/pull/18242) [ROCm] Optimize Deepseek R1 on MI300X (@zhentaocc)
- Merged: [#18624](https://github.com/sgl-project/sglang/pull/18624) [AMD] DSR1/V3 use fp8 bmm in MLA for MI300X (@zhentaocc)
- Merged: [#18978](https://github.com/sgl-project/sglang/pull/18978) [AMD]  Fix mi35x dsv32 mtp nightly (@bingxche)
- Merged: [#19091](https://github.com/sgl-project/sglang/pull/19091) Update rocm7.2 Dockerfile to install amdsmi for QuickReduce  (@clintg6)
- Merged: [#17953](https://github.com/sgl-project/sglang/pull/17953) [AMD] support two batch overlapping for mori ep (@billishyahao)

### triton
- Opened: [#9607](https://github.com/triton-lang/triton/pull/9607) [AMD][BACKEND] Add LLVM diagnostic handler for early LDS res (@claude-lgtm)
- Opened: [#9561](https://github.com/triton-lang/triton/pull/9561) [AMD]Support ScaleFactor=16 and E4M3 Scale and Add scale_fac (@knwng)
- Opened: [#9605](https://github.com/triton-lang/triton/pull/9605) [AMD] Fix thread predicate in AtomicCASOpConversion for tens (@brucechanglongxu)
- Opened: [#9604](https://github.com/triton-lang/triton/pull/9604) [AMD] Add num_threads, num_warps, and smid to tl.extra.hip (@brucechanglongxu)
- Opened: [#9593](https://github.com/triton-lang/triton/pull/9593) [AMD] ConvertWarpPipeline: recognize AsyncWaitOp and fix bar (@Hardcode84)
- Opened: [#9586](https://github.com/triton-lang/triton/pull/9586) [AMD] Drop unused logic and fix prediction for L2 prefetch (@ravil-mobile)
- Opened: [#9595](https://github.com/triton-lang/triton/pull/9595) [AMD] Add support for partitioned tensors in TDM (@plognjen)
- Opened: [#9585](https://github.com/triton-lang/triton/pull/9585) [AMD] Fixed `make_desc` lowering - i.e., findEncodingFromUse (@ravil-mobile)
- Opened: [#9562](https://github.com/triton-lang/triton/pull/9562) Add maxnreg support for ROCm/AMD backend (@fsx950223)
- Opened: [#9581](https://github.com/triton-lang/triton/pull/9581) [AMD][BACKEND] Fix mixed FP8 types promotion for WMMA (@pabloantoniom)
- Opened: [#9567](https://github.com/triton-lang/triton/pull/9567) [AMD][BACKEND] Fix mixed types MFMA fp8 instruction selectio (@pabloantoniom)
- Opened: [#9589](https://github.com/triton-lang/triton/pull/9589) [CI][AMD] Do apt-get update before apt-get install (@antiagainst)
- Opened: [#9555](https://github.com/triton-lang/triton/pull/9555) [AMD][BACKEND] Reintroduce old histogram lowering as AMD pat (@AlexAUT)
- Opened: [#9545](https://github.com/triton-lang/triton/pull/9545) [AMD] Fix scale layouts for batched WMMA scaled  (@borontion)
- Opened: [#9541](https://github.com/triton-lang/triton/pull/9541) [AMD] CanonicalizePointers: Handle different base pointers a (@kelesvol)
- Opened: [#9544](https://github.com/triton-lang/triton/pull/9544) [AMD][BACKEND] Extend padded layout selection for AsyncCopy  (@AlexAUT)

### migraphx
- Opened: [#4633](https://github.com/ROCm/AMDMIGraphX/pull/4633) Dont fuse op on attention that are not contained (@pfultz2)
- Opened: [#4630](https://github.com/ROCm/AMDMIGraphX/pull/4630) [AIMIGRAPHX-568] jit implementation for logsoftmax (@bdevorem)
- Opened: [#4642](https://github.com/ROCm/AMDMIGraphX/pull/4642) fix jit pooling (@aarushjain29)
- Opened: [#4641](https://github.com/ROCm/AMDMIGraphX/pull/4641) Dont use double or int64 when using fast_math (@pfultz2)
- Opened: [#4635](https://github.com/ROCm/AMDMIGraphX/pull/4635) Dont fuse concat when its used more than once (@pfultz2)
- Opened: [#4637](https://github.com/ROCm/AMDMIGraphX/pull/4637) Adding parse for MatMulBnb4 operator and updating MultiHeadA (@urpetkov-amd)
- Opened: [#4640](https://github.com/ROCm/AMDMIGraphX/pull/4640) Onnxruntime Weekly Sync 2026-02-27 (@github-actions[bot])
- Opened: [#4638](https://github.com/ROCm/AMDMIGraphX/pull/4638) Auto padding convTrans (@ivarusic-amd)
- Opened: [#4639](https://github.com/ROCm/AMDMIGraphX/pull/4639) yolo26 example (@alexsu52)
- Opened: [#4634](https://github.com/ROCm/AMDMIGraphX/pull/4634) Move rotary embedding to op builder (@pfultz2)
- Opened: [#4631](https://github.com/ROCm/AMDMIGraphX/pull/4631) Use Eigen 3rd party library for ref GEMMs (@kahmed10)
- Opened: [#4628](https://github.com/ROCm/AMDMIGraphX/pull/4628) Add option to skip benchmarking in compile_ops (@kahmed10)
- Opened: [#4627](https://github.com/ROCm/AMDMIGraphX/pull/4627) rocMLIR Weekly Sync 2026-02-22 (@github-actions[bot])
- Merged: [#4611](https://github.com/ROCm/AMDMIGraphX/pull/4611) Improve the logic for reaches() in find_splits (@aarushjain29)
- Merged: [#4595](https://github.com/ROCm/AMDMIGraphX/pull/4595) Handle split groups when the reduction is across different d (@pfultz2)
- Merged: [#4624](https://github.com/ROCm/AMDMIGraphX/pull/4624) Onnxruntime Weekly Sync 2026-02-20 (@github-actions[bot])
- Merged: [#4623](https://github.com/ROCm/AMDMIGraphX/pull/4623) Add support for ORT image in Jenkins pipeline (@causten)
- Merged: [#4589](https://github.com/ROCm/AMDMIGraphX/pull/4589) Fix eliminate_contiugous iterator bug (@CharlieL7)

### aiter
- Opened: [#2086](https://github.com/ROCm/aiter/pull/2086) Respect AITER_LOG_LEVEL for C++ stdout prints (@Copilot)
- Opened: [#2135](https://github.com/ROCm/aiter/pull/2135) fix(ci): Fix pre-sharded lists issues in Aiter tests (@gyohuangxin)
- Opened: [#2104](https://github.com/ROCm/aiter/pull/2104) fix smoothquant hip kernel exceed int32's maximum. (@rujiacai)
- Opened: [#2128](https://github.com/ROCm/aiter/pull/2128) fix mla_a8w8_qh64_qseqlen4_gqaratio16_ps kv_len < 4 nan erro (@minmengdie)
- Opened: [#2134](https://github.com/ROCm/aiter/pull/2134) update_unit_test (@amd-ruitang3)
- Opened: [#2113](https://github.com/ROCm/aiter/pull/2113) [FlyDSL] import flydsl implement of a4w4 moe (@lalala-sh)
- Opened: [#2133](https://github.com/ROCm/aiter/pull/2133) add asm_mla csv (@ljl1302924199)
- Opened: [#2084](https://github.com/ROCm/aiter/pull/2084) [HIP] causal conv1d hip decode (@huizzhan)
- Opened: [#2132](https://github.com/ROCm/aiter/pull/2132) Fuse qk norm group quant (@yzhou103)
- Opened: [#2088](https://github.com/ROCm/aiter/pull/2088) opt_csrc_torch_header (@amd-ruitang3)
- Opened: [#2105](https://github.com/ROCm/aiter/pull/2105) mha fwd v3 hdim128 support per tensor fp8 for MI300/MI308 (@shay-li77)
- Opened: [#2120](https://github.com/ROCm/aiter/pull/2120) Gluon gfx12 a8w8blockscale basic kernel (@amirumoAMD)
- Opened: [#2111](https://github.com/ROCm/aiter/pull/2111) [TRITON] Create script to benchmark attention kernels with s (@brunomazzottiamd)
- Opened: [#2085](https://github.com/ROCm/aiter/pull/2085) pa_decode_gluon_aot C++ api (@amd-yilizhao)
- Opened: [#2106](https://github.com/ROCm/aiter/pull/2106) fix mla ps fp8 the kv_seq tail len < 4 nan error (@minmengdie)
- Opened: [#2115](https://github.com/ROCm/aiter/pull/2115) CI: Initial Aiter benchmark tests (@gyohuangxin)
- Opened: [#2121](https://github.com/ROCm/aiter/pull/2121) [CI] Flash Attention RDNA CI (@micmelesse)
- Opened: [#2119](https://github.com/ROCm/aiter/pull/2119) [mi450] [gluon] UA3D updates (@k50112113)
- Opened: [#2096](https://github.com/ROCm/aiter/pull/2096) Hipgraph support for fav3 kvcache (@sahirema)
- Opened: [#2116](https://github.com/ROCm/aiter/pull/2116) [CK_TILE] Update fmha_bwd_traits (@DDEle)
- Opened: [#2091](https://github.com/ROCm/aiter/pull/2091) docs: add missing dependency install steps to contributor gu (@tuukkjs)
- Opened: [#2075](https://github.com/ROCm/aiter/pull/2075) Use unreg path for custom all-reduce during CUDA graph captu (@zyzshishui)
- Opened: [#2090](https://github.com/ROCm/aiter/pull/2090) mla n128 4 b64 c10k fix return lse error (@minmengdie)
- Opened: [#2074](https://github.com/ROCm/aiter/pull/2074) Add ENABLE_CK build option for CK-free builds (@sunway513)
- Opened: [#2071](https://github.com/ROCm/aiter/pull/2071) Add CLAUDE.md and skill for tune_ck_gemm_a8w8_blockscale. (@sabreshao)
- Opened: [#2122](https://github.com/ROCm/aiter/pull/2122) [Triton] Fix triton tests fail due to the API changes from T (@jwu10003)
- Opened: [#2124](https://github.com/ROCm/aiter/pull/2124) support strided gating_score for topk_softmax (@ganyi1996ppo)
- Opened: [#2131](https://github.com/ROCm/aiter/pull/2131) Revert "[MI308] add fp8 blockscale asm kernel" (@ZhangLirong-amd)
- Opened: [#2107](https://github.com/ROCm/aiter/pull/2107) [fix]: replace ck_tile by opus in ar (@TennyWang1223)
- Opened: [#2130](https://github.com/ROCm/aiter/pull/2130) add asm topsoftmax support 384x8 (@junhaha666)
- Opened: [#2127](https://github.com/ROCm/aiter/pull/2127) [OPUS] opus device test speed up (@carlushuang)
- Opened: [#2125](https://github.com/ROCm/aiter/pull/2125) [MI325][TUNE] igemm asm (@junxiaguo)
- Opened: [#2126](https://github.com/ROCm/aiter/pull/2126) Add runner-config.yml for runner->GPU mapping used by framew (@gyohuangxin)
- Opened: [#2123](https://github.com/ROCm/aiter/pull/2123) CI: Fix OOM issues in ATOM tests (@gyohuangxin)
- Opened: [#2118](https://github.com/ROCm/aiter/pull/2118) [MI308] add fp8 blockscale asm kernel (@junxiaguo)
- Opened: [#2117](https://github.com/ROCm/aiter/pull/2117) [TRITON] Sage v2 stride fix (@juuso-oskari)
- Opened: [#2108](https://github.com/ROCm/aiter/pull/2108) Fix triton3.5.1 vllm error in pa_mqa (@ZhangLirong-amd)
- Opened: [#2114](https://github.com/ROCm/aiter/pull/2114) update_unit_test (@amd-ruitang3)
- Opened: [#2110](https://github.com/ROCm/aiter/pull/2110) [OPUS] enhance cast(), add numeric_limits, add missing test  (@carlushuang)
- Opened: [#2097](https://github.com/ROCm/aiter/pull/2097) CI: Add steps to monitor the system health before ATOM tests (@gyohuangxin)
- Opened: [#2112](https://github.com/ROCm/aiter/pull/2112) [TRITON] [GLUON] Adding 2d unified attention Gluon kernel (@cagrikymk)
- Opened: [#2078](https://github.com/ROCm/aiter/pull/2078) [TRITON] MXFP4 GEMM fixes (@cagrikymk)
- Opened: [#2109](https://github.com/ROCm/aiter/pull/2109) CI: Increase timeout to 60 minutes in ATOM tests (@gyohuangxin)
- Opened: [#2100](https://github.com/ROCm/aiter/pull/2100) Fix CI prebuild: use build_ext so kernels are actually compi (@okakarpa)
- Opened: [#2092](https://github.com/ROCm/aiter/pull/2092) tune: add 493 new FP4 GEMM shapes for LLM inference (@sunway513)
- Opened: [#2077](https://github.com/ROCm/aiter/pull/2077) [OPUS] Enhance opus.hpp, add moe_sorting_opus, workgroup_bar (@carlushuang)
- Opened: [#2103](https://github.com/ROCm/aiter/pull/2103) CI: Skip CI for draft PR or docs changes (@gyohuangxin)
- Opened: [#2095](https://github.com/ROCm/aiter/pull/2095) [Triton] fix config selection bug for FP4 preshuffled GEMM (@k50112113)
- Opened: [#2082](https://github.com/ROCm/aiter/pull/2082) use regex to extract arch from rocminfo string (@ahmed-bsod)
- Opened: [#2083](https://github.com/ROCm/aiter/pull/2083) CI: fix triton commit to c147f098 (@gyohuangxin)
- Merged: [#2023](https://github.com/ROCm/aiter/pull/2023) Fix wrong path to the tune script (@amd-yashagar)
- Merged: [#2049](https://github.com/ROCm/aiter/pull/2049) [TRITON] Add smoothquant int8 MoE kernel (@nsusanto)
- Merged: [#1968](https://github.com/ROCm/aiter/pull/1968) add igemm tile size 16mx128 (@junxiaguo)
- Merged: [#1970](https://github.com/ROCm/aiter/pull/1970) CI: Split Aiter tests and triton into multiple shards (@gyohuangxin)
- Merged: [#1833](https://github.com/ROCm/aiter/pull/1833) [Triton] GEMM tunning script (@k50112113)
- Merged: [#1974](https://github.com/ROCm/aiter/pull/1974) [CI] Flash Attention Integration CI (@micmelesse)
- Merged: [#2025](https://github.com/ROCm/aiter/pull/2025) update ut (@amd-ruitang3)
- Merged: [#1973](https://github.com/ROCm/aiter/pull/1973) Defer expensive build operations to build_ext.run() (@paradigm)
- Merged: [#2042](https://github.com/ROCm/aiter/pull/2042) upload mla_a8w8_qh64_qseqlen4_gqaratio16 co in MI300 (@minmengdie)
- Merged: [#2029](https://github.com/ROCm/aiter/pull/2029) [FIX] fix a16 causal mha bwd case for python api (@JaxChen29)
- Merged: [#2066](https://github.com/ROCm/aiter/pull/2066) [TRITON] Sage attention v2: Q*K in mxfp4 (@juuso-oskari)

### atom
- Opened: [#235](https://github.com/ROCm/ATOM/pull/235) [Qwen-Next](ci): add qwen next to ci (@PerryZhang01)
- Opened: [#245](https://github.com/ROCm/ATOM/pull/245) Fix Kimi-K2-Instruct Quark MXFP4 (@thpereir)
- Opened: [#244](https://github.com/ROCm/ATOM/pull/244) [Perf][Qwen3-Next] merge qkvz and ba into qkvzba and add tri (@ganyi1996ppo)
- Opened: [#242](https://github.com/ROCm/ATOM/pull/242) [Perf][Qwen3-Next] Fused shared experts for qwen3_next (@ganyi1996ppo)
- Opened: [#237](https://github.com/ROCm/ATOM/pull/237) [QUARK-403] Add MiniMax-2.1 support (@thpereir)
- Opened: [#240](https://github.com/ROCm/ATOM/pull/240) CI: Make ATOM benchmark can be called from other repos (@gyohuangxin)
- Opened: [#236](https://github.com/ROCm/ATOM/pull/236) Not yet ready for review- Per-layer quantization config with (@thpereir)
- Opened: [#234](https://github.com/ROCm/ATOM/pull/234) fix: resolve prefix caching crashes with MTP speculative dec (@valarLip)
- Opened: [#247](https://github.com/ROCm/ATOM/pull/247) CI: Run ATOM performance benchmark nightly (@gyohuangxin)
- Opened: [#246](https://github.com/ROCm/ATOM/pull/246) CI: Add Qwen3 FP4 model (@gyohuangxin)
- Opened: [#231](https://github.com/ROCm/ATOM/pull/231) [Qwen3-Next](md): add user guide for qwen3-next (@PerryZhang01)
- Opened: [#243](https://github.com/ROCm/ATOM/pull/243) CI: add nightly schedule and run_on_pr matrix option (@gyohuangxin)
- Opened: [#241](https://github.com/ROCm/ATOM/pull/241) CI: Debug the OOM issues in ATOM tests (@gyohuangxin)
- Opened: [#239](https://github.com/ROCm/ATOM/pull/239) Qwen3-Next-MTP Impl (@ganyi1996ppo)
- Opened: [#232](https://github.com/ROCm/ATOM/pull/232) CI: Add path filters to ATOM test workflow (@gyohuangxin)
- Opened: [#230](https://github.com/ROCm/ATOM/pull/230) CI: Optimize the cleanup (@gyohuangxin)
- Opened: [#228](https://github.com/ROCm/ATOM/pull/228) enable mtp=3 (@valarLip)
- Merged: [#198](https://github.com/ROCm/ATOM/pull/198) Add the doc of ATOM enviroment_variables (@wuhuikx)
- Merged: [#210](https://github.com/ROCm/ATOM/pull/210) CI: Add thresholds for models accuracy tests (@gyohuangxin)

### mori
- Opened: [#173](https://github.com/ROCm/mori/pull/173) Feature: make MORI framework agnostic (@TianDi101)
- Opened: [#175](https://github.com/ROCm/mori/pull/175) Add elastic EP for dispatch/combine flows (@maning00)
- Opened: [#174](https://github.com/ROCm/mori/pull/174) Doc: update perf & hardware support matrix & news (@TianDi101)
- Merged: [#172](https://github.com/ROCm/mori/pull/172) Feat: Enable async kernel BF16 cast to FP8 combine (@isytwu)

### flydsl
- Opened: [#159](https://github.com/ROCm/FlyDSL/pull/159) rm ck from comments (@lalala-sh)
- Opened: [#158](https://github.com/ROCm/FlyDSL/pull/158) use abs import for correct importing kernels from wheel (@lalala-sh)
- Opened: [#143](https://github.com/ROCm/FlyDSL/pull/143) W4a16 bf16 fp8 bf16 moe (@ClementLinCF)
- Opened: [#155](https://github.com/ROCm/FlyDSL/pull/155) add AOT pre-compilation example for MOE kernels (@coderfeli)
- Opened: [#148](https://github.com/ROCm/FlyDSL/pull/148) update: publish artifacts nightlies (@kiran-thumma)
- Opened: [#152](https://github.com/ROCm/FlyDSL/pull/152) [Tool][fly-opt] Add fly-opt tool and lit-based test suite (@jli-melchior)
- Opened: [#150](https://github.com/ROCm/FlyDSL/pull/150) Add TiledCopy and Mma (@sjfeng1999)
- Opened: [#151](https://github.com/ROCm/FlyDSL/pull/151) Add decode-phase Flash Attention kernel (@ChuanLi1101)
- Opened: [#144](https://github.com/ROCm/FlyDSL/pull/144) fix all copyright & license headers & add lint (@JackWolfard)
- Opened: [#141](https://github.com/ROCm/FlyDSL/pull/141) mxfp4 preshuffled gemm optimize (@yadaish)
- Opened: [#156](https://github.com/ROCm/FlyDSL/pull/156) Add CI on pre_v0.1 branch (@gyohuangxin)
- Opened: [#154](https://github.com/ROCm/FlyDSL/pull/154) add compile only and dumpir (@coderfeli)
- Opened: [#153](https://github.com/ROCm/FlyDSL/pull/153) Pre v0.1 gemm fix (@coderfeli)
- Opened: [#149](https://github.com/ROCm/FlyDSL/pull/149) Update README.md MLIR_PATH path (@jundaf2)
- Opened: [#146](https://github.com/ROCm/FlyDSL/pull/146) moe gemm stage2 atomics fp32 (@yadaish)
- Opened: [#147](https://github.com/ROCm/FlyDSL/pull/147) [Pytest] Fix assertion in `test_gpu_with_rocir_coords.py`  (@sammysun0711)
- Opened: [#145](https://github.com/ROCm/FlyDSL/pull/145) Pre v0.1 gemm (@coderfeli)
- Opened: [#140](https://github.com/ROCm/FlyDSL/pull/140) fix(build): dereference MLIR symlinks in build output (@sunway513)
- Merged: [#128](https://github.com/ROCm/FlyDSL/pull/128) [FLYDSL]: add recast_layout op (@xudoyuan)

### transformer_engine
- Opened: [#461](https://github.com/ROCm/TransformerEngine/pull/461) [NO MERGE] Integrate CK varlen cross attention for small-seq (@VeeraRajasekhar)
- Opened: [#466](https://github.com/ROCm/TransformerEngine/pull/466) Add ROCm mismatch bypass env flag (@jiagaoxiang)
- Opened: [#463](https://github.com/ROCm/TransformerEngine/pull/463) Release 2.6 fix: cuda graph dropout accuracy  (@Micky774)
- Opened: [#459](https://github.com/ROCm/TransformerEngine/pull/459) Always use V2 hipify. Make all hipify results consistent (@ipanfilo)
- Opened: [#460](https://github.com/ROCm/TransformerEngine/pull/460) IFU v2.10 (@alextmagro)
- Opened: [#462](https://github.com/ROCm/TransformerEngine/pull/462) export HF_TOKEN for CI (@matthiasdiener)
- Merged: [#454](https://github.com/ROCm/TransformerEngine/pull/454) Updated test to include CK/AITER V2/V3 test in single backen (@Micky774)
- Merged: [#453](https://github.com/ROCm/TransformerEngine/pull/453) Update ck_fused_attn logging to direct to thread-specific fi (@Micky774)
- Merged: [#458](https://github.com/ROCm/TransformerEngine/pull/458) Triton current scaling: avoid casting amax input (@matthiasdiener)
- Merged: [#195](https://github.com/ROCm/TransformerEngine/pull/195) Added Dockerfile for CI images & Upgrate CI to ROCm 7.2 (@VeeraRajasekhar)
- Merged: [#452](https://github.com/ROCm/TransformerEngine/pull/452) Updated Triton norms dispatch consolidation (@Micky774)
- Merged: [#456](https://github.com/ROCm/TransformerEngine/pull/456) Swapped out JIT incompatible function (@Micky774)
- Merged: [#455](https://github.com/ROCm/TransformerEngine/pull/455) Avoid hipifying code outside TE tree (@ipanfilo)

## New Issues This Week

### pytorch
- [#175952](https://github.com/pytorch/pytorch/issues/175952) [torch.compile] Massive numerical discrepancy (~0.05 Max Abs (@zifan6699)
- [#175637](https://github.com/pytorch/pytorch/issues/175637) Conv2d output inconsistency across different execution modes (@zifan6699)
- [#175854](https://github.com/pytorch/pytorch/issues/175854) [Numerical Consistency] Significant drift between Eager and  (@zifan6699)
- [#175982](https://github.com/pytorch/pytorch/issues/175982) .reset_parameters() result depends on memory format (@thibaultdvx)
- [#176025](https://github.com/pytorch/pytorch/issues/176025) amp + flex_attention_backward results in NoValidChoicesError (@mauriceweiler)
- [#175994](https://github.com/pytorch/pytorch/issues/175994) CI: Dead CentOS code in docker setup scripts (@jsmolic)
- [#175875](https://github.com/pytorch/pytorch/issues/175875) [release 2.11] [HF] [cuda 12.6] Torch Compile issues est_mod (@atalman)
- [#175966](https://github.com/pytorch/pytorch/issues/175966) Silently incorrect results for torch.arange + Inductor (@avolchek)
- [#175928](https://github.com/pytorch/pytorch/issues/175928) [release 2.11][vllm] Language Models Tests (Extra Standard)  (@atalman)
- [#175989](https://github.com/pytorch/pytorch/issues/175989) test/inductor/test_fp8.py::TestCvtE8M0Rceil: error: couldn't (@jsmolic)
- [#175981](https://github.com/pytorch/pytorch/issues/175981) Segmentation fault in inductor/test_flex_attention.py (@jsmolic)
- [#175953](https://github.com/pytorch/pytorch/issues/175953) `torch.set_default_device` are ignored by `torch.compile` (@Shawn0v0)
- [#175866](https://github.com/pytorch/pytorch/issues/175866) Padding for Nested Long Tensor loses precision (@sumitbinnani)
- [#175950](https://github.com/pytorch/pytorch/issues/175950) DISABLED test_fake_device (__main__.FakeTensorTest) (@etaf)
- [#175757](https://github.com/pytorch/pytorch/issues/175757) Diagonal crosshatch artifacts in FLUX/MMDiT diffusion models (@infjb)
- [#175724](https://github.com/pytorch/pytorch/issues/175724) [Distributed][NCCL] Request for Backport: SIGSEGV in Heartbe (@tomjen12)
- [#175761](https://github.com/pytorch/pytorch/issues/175761) torch.arange() crashes when "step" attribute is non-integer  (@radeksm)

### jax
- [#35472](https://github.com/jax-ml/jax/issues/35472) ROCm tests skipped due to hipSparse Numerical Issue (@AratiGanesh)
- [#35455](https://github.com/jax-ml/jax/issues/35455) [ROCm] Batched Cholesky (potrf) OOM crash due to hipSolver a (@FlemingH)

### vllm
- [#35521](https://github.com/vllm-project/vllm/issues/35521) [Bug]: Suffix decoding crashes with assert total_num_schedul (@rosstang)
- [#35599](https://github.com/vllm-project/vllm/issues/35599) [Bug]: cpu version compile failed in 0.16.0 (@wanghualex1-wq)
- [#35344](https://github.com/vllm-project/vllm/issues/35344) [Bug]: Value error, Model architectures ['Qwen3_5MoeForCondi (@Dong09)
- [#35191](https://github.com/vllm-project/vllm/issues/35191) [Bug]: Qwen3.5 397B FP8 fills 1TB RAM and OOM killed with hi (@FWao)
- [#35569](https://github.com/vllm-project/vllm/issues/35569) [Bug]: [ROCm] ROCM_ATTN backend shows ~8.5% systematic score (@AndreasKaratzas)
- [#35507](https://github.com/vllm-project/vllm/issues/35507) [Bug]: Assertionerror when using OffloadingConnector: assert (@liunianxuxie)
- [#35028](https://github.com/vllm-project/vllm/issues/35028) [Bug]: RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE (@shahizat)
- [#35391](https://github.com/vllm-project/vllm/issues/35391) [Bug]: Value error, Model architectures ['Qwen3_5ForConditio (@Iam-a-Savage-coder)
- [#35541](https://github.com/vllm-project/vllm/issues/35541) [Bug]: vLLM hangs indefinitely with low `num_gpu_blocks_over (@kvcache670)
- [#35528](https://github.com/vllm-project/vllm/issues/35528) [Feature]: Support serving ModelOpt W4A8 MXFP4+FP8 checkpoin (@zeryx)
- [#35504](https://github.com/vllm-project/vllm/issues/35504) [Bug]: qwen3-coder-next inference randomly hangs, accuracy d (@vitush93)
- [#35465](https://github.com/vllm-project/vllm/issues/35465) [Bug]: No available shared memory broadcast block found in 6 (@DongZhaoXiong)
- [#35522](https://github.com/vllm-project/vllm/issues/35522) [Bug]: Music Flamingo `ValueError: Following weights were no (@denadai2)
- [#35519](https://github.com/vllm-project/vllm/issues/35519) [Bug]: Qwen3.5 NVFP4 models crash on ARM64 GB10 DGX Spark (C (@EmilHaase)
- [#35286](https://github.com/vllm-project/vllm/issues/35286) [Bug]: Qwen3.5-MoE failed with enable_lora (@hjh0119)
- [#35501](https://github.com/vllm-project/vllm/issues/35501) [Bug]: fp8_blockscale_gemm JIT compilation fails on vLLM Doc (@onurguner)
- [#35319](https://github.com/vllm-project/vllm/issues/35319) [Bug]: Multi-Node inference with PP > 1 crashes after proces (@JeanPaulShapo)
- [#35464](https://github.com/vllm-project/vllm/issues/35464) [Bug]: Fix GLM-OCR text_config model_type handling (@iron-shaper)
- [#35163](https://github.com/vllm-project/vllm/issues/35163) [Bug]: AMD docker image still using torch 2.9 despite 2.10.0 (@mikaylagawarecki)
- [#35089](https://github.com/vllm-project/vllm/issues/35089) [RFC]: In-Tree AMD Zen CPU Backend via zentorch (@amd-lalithnc)
- [#35133](https://github.com/vllm-project/vllm/issues/35133) [CI Failure]:  mi355_4: LoRA TP Test (Distributed) (@AndreasKaratzas)
- [#35169](https://github.com/vllm-project/vllm/issues/35169) [Bug]: Memory Access Fault during Step-3.5-Flash inference ( (@ColinZ22)
- [#35132](https://github.com/vllm-project/vllm/issues/35132) [CI Failure][ROCm]:  CrossLayer KV layout Distributed NixlCo (@AndreasKaratzas)
- [#35403](https://github.com/vllm-project/vllm/issues/35403) [Bug]: Ray Compiled DAG timeout/deadlock during VLM forward  (@emricksini-h)
- [#35266](https://github.com/vllm-project/vllm/issues/35266) [Bug]: Missing opening brace for Qwen3.5 streaming tool call (@AsterisMono)

### sglang
- [#19139](https://github.com/sgl-project/sglang/issues/19139) [Rfc] Refactor server_args.py (@vincentzed)

### aiter
- [#2129](https://github.com/ROCm/aiter/issues/2129) Large launch overhead for aiter.flash_attn_func (@SampoAMD)
- [#2102](https://github.com/ROCm/aiter/issues/2102) [Feature]: Add benchmark tests to detect performance regress (@gyohuangxin)
- [#2098](https://github.com/ROCm/aiter/issues/2098) [Issue]: GPU memory access faults on GLM-4.6 FP8 (@richhx)
- [#2093](https://github.com/ROCm/aiter/issues/2093) [Feature]: replace CK moe sorting with opus (@carlushuang)
- [#2073](https://github.com/ROCm/aiter/issues/2073) [RFC] CK-Free Build: Enable AITER builds without Composable  (@sunway513)
- [#2080](https://github.com/ROCm/aiter/issues/2080) Remove unconditional prints to stdout (@gshtras)
- [#2081](https://github.com/ROCm/aiter/issues/2081) [Feature]: Can the MLA kernel be open source? (@aidando73)
- [#2076](https://github.com/ROCm/aiter/issues/2076) [RFC][Agentic/Low-Latency] improve compile time of python bi (@carlushuang)

### atom
- [#238](https://github.com/ROCm/ATOM/issues/238) [Issue]: ValueError: could not broadcast input array from sh (@peizhang56)
- [#229](https://github.com/ROCm/ATOM/issues/229) [RFC] CK-Free AITER Compatibility: Attention, MOE, and Docke (@sunway513)

### flydsl
- [#142](https://github.com/ROCm/FlyDSL/issues/142) finish reconstruction of layout algebra (@coderfeli)
