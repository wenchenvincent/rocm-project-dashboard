# Weekly Digest

Week of 2026-02-16 to 2026-02-23

## New Releases

_No new releases this week._

## PRs This Week

### pytorch
- Opened: [#175514](https://github.com/pytorch/pytorch/pull/175514) [wip][dynamo] Refactor UserDefinedObjectVariable var_getattr (@anijain2305)
- Opened: [#175274](https://github.com/pytorch/pytorch/pull/175274) Bump transformers version to 5.2.0 (@huydhn)
- Opened: [#175497](https://github.com/pytorch/pytorch/pull/175497) symbolic shapes guarding_hint_or_throw (@laithsakka)
- Opened: [#175473](https://github.com/pytorch/pytorch/pull/175473) [Claude] Add CI ARC migration skill for Claude Code (@huydhn)
- Opened: [#175073](https://github.com/pytorch/pytorch/pull/175073) [inductor] Decompose mm to pointwise mul when K==1 (@romanmeta)
- Opened: [#175443](https://github.com/pytorch/pytorch/pull/175443) [CI][ROCm] Skip sccache PATH wrappers for theRock nightly bu (@ethanwee1)
- Opened: [#175356](https://github.com/pytorch/pytorch/pull/175356) Fix scatter_reduce in-place op handling in Inductor memref c (@sidt-meta)
- Opened: [#175285](https://github.com/pytorch/pytorch/pull/175285) [Inductor] Allow subgraphs to be benchmarked with async pipe (@PaulZhang12)
- Opened: [#175097](https://github.com/pytorch/pytorch/pull/175097) [DO NOT REBASE][ROCm][Inductor] New Inductor benchmarker bas (@naromero77amd)
- Opened: [#175468](https://github.com/pytorch/pytorch/pull/175468) [DO NOT MERGE] Test AMD Capacity. (@saienduri)
- Opened: [#175286](https://github.com/pytorch/pytorch/pull/175286) [ROCm] No-fence in normalization kernel (@anatoliylitv)
- Opened: [#175451](https://github.com/pytorch/pytorch/pull/175451) [SymmMem] Do not compile nccl_device.h for ROCm (@pragupta)
- Opened: [#175303](https://github.com/pytorch/pytorch/pull/175303) [ROCM] Refactor BFloat16 implementation for native usage of  (@anatoliylitv)
- Opened: [#175424](https://github.com/pytorch/pytorch/pull/175424) [ROCm][CI] Update test_pad_mm (@jsmolic)
- Opened: [#175427](https://github.com/pytorch/pytorch/pull/175427) Adjust bfloat16 tolerance for rocm in test_rms_norm_sharing_ (@jsmolic)
- Opened: [#175180](https://github.com/pytorch/pytorch/pull/175180) [rocm] Fix build_amd.py failure when MSLK submodule is missi (@radeksm)
- Opened: [#175428](https://github.com/pytorch/pytorch/pull/175428) [MXFP4] Fix E8M0 blockwise scale size validation for packed  (@pranavsharma)
- Opened: [#175159](https://github.com/pytorch/pytorch/pull/175159) [ROCm] forward fix #174087, take 4 (@pytorchbot)
- Opened: [#175371](https://github.com/pytorch/pytorch/pull/175371) [DO NOT MERGE] test linalg for rocm (@ethanwee1)
- Opened: [#175299](https://github.com/pytorch/pytorch/pull/175299) [benchmark] Skip pytorch_CycleGAN_and_pix2pix from inductor  (@pytorchbot)
- Opened: [#175095](https://github.com/pytorch/pytorch/pull/175095) Revert "[CI] Enable TIMM pretrained model caching on shared  (@jeffdaily)
- Opened: [#175094](https://github.com/pytorch/pytorch/pull/175094) Revert "[fix] DISABLED test_index (__main__.DistTensorOpsTes (@jeffdaily)
- Opened: [#175096](https://github.com/pytorch/pytorch/pull/175096) Update inductor expected accuracy files (@pytorchbot)

### jax
- Opened: [#35315](https://github.com/jax-ml/jax/pull/35315) Fix jax utests with rocm (@alekstheod)
- Opened: [#35251](https://github.com/jax-ml/jax/pull/35251) [WIP ROCm] Migrate rocm wheel build (@alekstheod)
- Opened: [#35111](https://github.com/jax-ml/jax/pull/35111) [ROCm] Add ROCm support for eigh export backwards compatibil (@AratiGanesh)
- Opened: [#35288](https://github.com/jax-ml/jax/pull/35288) [ROCm] Implement Mosaic GPU detection and Auto-Skips (@gulsumgudukbay)
- Opened: [#35283](https://github.com/jax-ml/jax/pull/35283) [ROCm] Improve ROCm pytest results handling (@psanal35)
- Opened: [#35102](https://github.com/jax-ml/jax/pull/35102) [ROCm] Set release rpaths to rocm so targets (@alekstheod)
- Opened: [#35115](https://github.com/jax-ml/jax/pull/35115) [ROCm] Add hip_threefry2x32_ffi to stable custom call target (@AratiGanesh)
- Merged: [#31768](https://github.com/jax-ml/jax/pull/31768) [ROCm] Support lowering through PJRT_Triton_Extension (@amd-jianli12)
- Merged: [#34829](https://github.com/jax-ml/jax/pull/34829) [ROCm] Add ROCm LU solver to backward compatibility tests (@AratiGanesh)

### vllm
- Opened: [#35071](https://github.com/vllm-project/vllm/pull/35071) [ROCm][CI] Expose tests to AMD production CI and fix amdsmi  (@AndreasKaratzas)
- Opened: [#35042](https://github.com/vllm-project/vllm/pull/35042) [Platform]Add current_platform.get_num_compute_units interfa (@jikunshang)
- Opened: [#35069](https://github.com/vllm-project/vllm/pull/35069) [ROCm] Derive device capability from GCN arch string without (@AndreasKaratzas)
- Opened: [#34652](https://github.com/vllm-project/vllm/pull/34652) [AMD][CI] Skip test new_weight_syncing/rlhf.py (@rjrock)
- Opened: [#35038](https://github.com/vllm-project/vllm/pull/35038) [BugFix] Refactor add max_num_tokens_per_forward_pass to acc (@LucasWilkinson)
- Opened: [#34839](https://github.com/vllm-project/vllm/pull/34839) [ROCm][CI] Cleaning and restructuring amd-ci legacy pipeline (@AndreasKaratzas)
- Opened: [#35049](https://github.com/vllm-project/vllm/pull/35049) [ROCm][CI] Disable skinny GEMMs in multimodal tests to fix n (@AndreasKaratzas)
- Opened: [#35043](https://github.com/vllm-project/vllm/pull/35043) [ROCm][CI] Fix spec decode profile assertion and logprob tes (@AndreasKaratzas)
- Opened: [#35064](https://github.com/vllm-project/vllm/pull/35064) [ROCm] Support MLA with nhead=32 and FP8 KV cache (e.g. Kimi (@ChuanLi1101)
- Opened: [#35052](https://github.com/vllm-project/vllm/pull/35052) [ROCm][CI] Fix realtime test timeouts caused by aiter JIT co (@AndreasKaratzas)
- Opened: [#34644](https://github.com/vllm-project/vllm/pull/34644) [release 2.11] Update to torch 2.11-rc1 (@atalman)
- Opened: [#35051](https://github.com/vllm-project/vllm/pull/35051) feat: Support MXFP4 quantized dense models on AMD CDNA2/CDNA (@fengli1702)
- Opened: [#34798](https://github.com/vllm-project/vllm/pull/34798) [Mamba1] - Kernel Level Chunk Alignment for Prefix Caching (@Josephasafg)
- Opened: [#35011](https://github.com/vllm-project/vllm/pull/35011) remove cuda check in `top_k_top_p_triton` kernel (@jikunshang)
- Opened: [#34632](https://github.com/vllm-project/vllm/pull/34632) [ROCm] Add MXFP4 inline dequant Triton kernel for RDNA4/gfx1 (@laudney)
- Opened: [#35050](https://github.com/vllm-project/vllm/pull/35050) [ROCm][CI] Fix flaky embedding chat test by using tolerance- (@AndreasKaratzas)
- Opened: [#34741](https://github.com/vllm-project/vllm/pull/34741) [ROCm] Enable FP8 KV-cache and relax constraints for RDNA4 c (@laudney)
- Opened: [#34961](https://github.com/vllm-project/vllm/pull/34961) [CI] Bump mteb version to `mteb[bm25s]>=2, <3` for pooling m (@yewentao256)
- Opened: [#34923](https://github.com/vllm-project/vllm/pull/34923) [ROCm][CI] Added MI325 mirrors (@AndreasKaratzas)
- Opened: [#35008](https://github.com/vllm-project/vllm/pull/35008) [CI] Stabilizing ROCm amd-ci signal and minor name fix in up (@AndreasKaratzas)
- Opened: [#35013](https://github.com/vllm-project/vllm/pull/35013) [CI/Build] Fix gRPC version mismatch (@DarkLight1337)
- Opened: [#34927](https://github.com/vllm-project/vllm/pull/34927) [Bugfix][Kernel] Fix activation_kernels.cu build failure on  (@FloatingVertex)
- Opened: [#34709](https://github.com/vllm-project/vllm/pull/34709) [ROCm] Enable wvSplitK skinny GEMM kernel for RDNA4/gfx1x de (@laudney)
- Opened: [#34688](https://github.com/vllm-project/vllm/pull/34688) [ROCm] Enable bitsandbytes quantization support on ROCm (@Abdennacer-Badaoui)
- Opened: [#34636](https://github.com/vllm-project/vllm/pull/34636) [ROCm][Bugfix]: Only save unpadded sizes for shared_experts  (@Rohan138)
- Opened: [#34599](https://github.com/vllm-project/vllm/pull/34599) [ROCm][CI] Fix spec decode logprobs flakiness and parametriz (@AndreasKaratzas)
- Opened: [#34998](https://github.com/vllm-project/vllm/pull/34998) [ROCm] Check that AITER MHA is not selected with sinks (@gshtras)
- Opened: [#34735](https://github.com/vllm-project/vllm/pull/34735) [AMD][CI] Fix test_custom_allreduce for A100 testgroup (@rjrock)
- Opened: [#34985](https://github.com/vllm-project/vllm/pull/34985) [CI][AMD][BugFix] Add  torch.cuda.set_device to test_punica_ (@rasmith)
- Opened: [#34922](https://github.com/vllm-project/vllm/pull/34922) [ROCm][CI] Loosen RemoteOpenAIServer Startup Timeout (@micah-wil)
- Opened: [#34931](https://github.com/vllm-project/vllm/pull/34931) [AMD][CI] Support Triton attention with ExampleConnector (@rjrock)
- Opened: [#34918](https://github.com/vllm-project/vllm/pull/34918) Change targets for AMD build in the "CI" pipeline (@Alexei-V-Ivanov-AMD)
- Opened: [#34631](https://github.com/vllm-project/vllm/pull/34631) [ROCm] Make Whisper causal attention backend-agnostic (@laudney)
- Opened: [#34885](https://github.com/vllm-project/vllm/pull/34885) [CI/Build] Try to make beam search test less flaky (@DarkLight1337)
- Opened: [#34677](https://github.com/vllm-project/vllm/pull/34677) [Bugfix][CPU] Fix basic unit tests failing in CPU platforms (@jasonyanwenl)
- Opened: [#34878](https://github.com/vllm-project/vllm/pull/34878) [ROCm][Test] Fix beam search determinism failures from batch (@AndreasKaratzas)
- Opened: [#34879](https://github.com/vllm-project/vllm/pull/34879) [ROCm][CI] Removing all blocking labels from MI355 until sta (@AndreasKaratzas)
- Opened: [#34655](https://github.com/vllm-project/vllm/pull/34655) [CI][AMD][BugFix] Skip tests in test_unquantized_backend_sel (@rasmith)
- Opened: [#34825](https://github.com/vllm-project/vllm/pull/34825) [CI] temporarily disable multi-node tests (@robertgshaw2-redhat)
- Opened: [#34653](https://github.com/vllm-project/vllm/pull/34653) [BugFix] [Build] fix string literals comparison in indexer_k (@hongxiayang)
- Opened: [#34726](https://github.com/vllm-project/vllm/pull/34726) [ROCm] Enable DBO (Dynamic Batch Optimization) on ROCm (@raviguptaamd)
- Opened: [#34629](https://github.com/vllm-project/vllm/pull/34629) Targeting the MI355 agent pool with all existing tests (@Alexei-V-Ivanov-AMD)
- Merged: [#34558](https://github.com/vllm-project/vllm/pull/34558) [New Model] Add ColModernVBERT (@athrael-soju)
- Merged: [#34466](https://github.com/vllm-project/vllm/pull/34466) [CI/Build] Add opentelemetry libs in default vllm build (req (@vladmihailescu)
- Merged: [#34541](https://github.com/vllm-project/vllm/pull/34541) [ROCM] Optimize ROCM_AITER_FA spec decode eagle performance (@jennyyyyzhen)
- Merged: [#34570](https://github.com/vllm-project/vllm/pull/34570) [ROCm][AITER] Fix aiter paged_attention_v1 decode for slidin (@AndreasKaratzas)
- Merged: [#34567](https://github.com/vllm-project/vllm/pull/34567) [CI] Fix ColBERT HF comparison tests on AMD CI + refactor (@AndreasKaratzas)
- Merged: [#33949](https://github.com/vllm-project/vllm/pull/33949) [CI][MCP][Harmony] Heavy refactoring Harmony & MCP response  (@AndreasKaratzas)
- Merged: [#34574](https://github.com/vllm-project/vllm/pull/34574) [Frontend] Support multimodal inputs for late-interaction sc (@craftsangjae)
- Merged: [#33501](https://github.com/vllm-project/vllm/pull/33501) Fix DeepSeek RoPE initialization error (@catswe)
- Merged: [#32877](https://github.com/vllm-project/vllm/pull/32877) [Bugfix][Hardware][AMD] Fix ROCM_AITER_FA speculative decodi (@c0de128)
- Merged: [#33739](https://github.com/vllm-project/vllm/pull/33739) [CI][AMD][BugFix][P/D] Add default_vllm_config to test_morii (@rasmith)
- Merged: [#32993](https://github.com/vllm-project/vllm/pull/32993) [Feature] Support CPU Offloading without Pytorch Pinned Memo (@wzhao18)
- Merged: [#26535](https://github.com/vllm-project/vllm/pull/26535) [Bugfix] Convert untraceable GroupShape to list for AMD impl (@Lucaskabela)
- Merged: [#23207](https://github.com/vllm-project/vllm/pull/23207) [Misc][qwen2_5_vl][torch.compile] Enable `supports_torch_com (@Lucaskabela)
- Merged: [#30709](https://github.com/vllm-project/vllm/pull/30709) [Misc][LLaMa4] Compile LLaMa Vision Encoder (@Lucaskabela)
- Merged: [#31748](https://github.com/vllm-project/vllm/pull/31748) [Misc][BE] Type coverage for vllm/compilation [3/3] (@Lucaskabela)
- Merged: [#34181](https://github.com/vllm-project/vllm/pull/34181) [CI][AMD][BugFix] Use torch.testing.assert_close instead of  (@rasmith)
- Merged: [#34455](https://github.com/vllm-project/vllm/pull/34455) [Bugfix] Remove assert causing hipErrorStreamCaptureUnsuppor (@JadenMathias)
- Merged: [#34507](https://github.com/vllm-project/vllm/pull/34507) [Bugfix] Fix fused MoE int32 overflow in stride*offset witho (@haosdent)
- Merged: [#34324](https://github.com/vllm-project/vllm/pull/34324) Fixed whisper CPU test that does not spawn properly. (@almayne)
- Merged: [#34566](https://github.com/vllm-project/vllm/pull/34566) [CI][Metrics] Stabilize tests with polling and subprocess gu (@AndreasKaratzas)
- Merged: [#34320](https://github.com/vllm-project/vllm/pull/34320) [Bugfix] Fix Dynamo unexpected keyword argument  (@samutamm)
- Merged: [#34279](https://github.com/vllm-project/vllm/pull/34279) [Bugfix] Fix fused MoE IMA (sans chunking) by using int64 fo (@tlrmchlsmth)
- Merged: [#34590](https://github.com/vllm-project/vllm/pull/34590) [CI][Frontend] Return 422 instead of 500 for invalid Anthrop (@AndreasKaratzas)
- Merged: [#33493](https://github.com/vllm-project/vllm/pull/33493) Perf tuning and expansion of cases covered for wvSplitKrc (@amd-hhashemi)

### sglang
- Opened: [#19162](https://github.com/sgl-project/sglang/pull/19162) [ROCm] Use unreg path for custom all-reduce during CUDA grap (@zyzshishui)
- Opened: [#19113](https://github.com/sgl-project/sglang/pull/19113) [AMD] [DO NOT MERGE] Fix 3 pre-existing AMD CI test failures (@michaelzhang-ai)
- Opened: [#19176](https://github.com/sgl-project/sglang/pull/19176) [AMD] ENV Flags tuning and cleanup (@HaiShaw)
- Opened: [#19140](https://github.com/sgl-project/sglang/pull/19140) [Re-land][jit kernel] Support per_token_group_quant_8bit jit (@yuan-luo)
- Opened: [#19091](https://github.com/sgl-project/sglang/pull/19091) Update rocm7.2 Dockerfile to install amdsmi for QuickReduce  (@clintg6)
- Opened: [#19103](https://github.com/sgl-project/sglang/pull/19103) [jit_kernel] Migrate cast (downcast_fp8) from sgl-kernel AOT (@Johnsonms)
- Opened: [#19165](https://github.com/sgl-project/sglang/pull/19165) Enable Building on gfx1100 (@Calandracas606)
- Opened: [#19161](https://github.com/sgl-project/sglang/pull/19161) Revert "[AMD] support two batch overlapping for mori ep #179 (@Fridge003)
- Opened: [#19143](https://github.com/sgl-project/sglang/pull/19143) feat: Support MXFP4 quantized dense models on AMD CDNA2/CDNA (@fengli1702)
- Opened: [#18992](https://github.com/sgl-project/sglang/pull/18992) [AMD] Enable ROCm kvcache JIT path and add AMD CI coverage. (@hubertlu-tw)
- Opened: [#19134](https://github.com/sgl-project/sglang/pull/19134) Fix spec v2+dp attention in nsa backend (@Qiaolin-Yu)
- Opened: [#19110](https://github.com/sgl-project/sglang/pull/19110) [AMD][CI] Remove hardcoded /data2 paths from MI35x tests to  (@michaelzhang-ai)
- Opened: [#19122](https://github.com/sgl-project/sglang/pull/19122) [3/n] deepseek_v2.py Refactor: Migrate MLA forward method in (@Fridge003)
- Opened: [#18911](https://github.com/sgl-project/sglang/pull/18911) [AMD] [GLM-5 Day 0] Add GLM-5 nightly test (@michaelzhang-ai)
- Opened: [#19078](https://github.com/sgl-project/sglang/pull/19078) Document & auto-enable FP8 block-wise CUTLASS GEMM for SM120 (@Arush04)
- Opened: [#19066](https://github.com/sgl-project/sglang/pull/19066) Optimization of Qwen Image, Qwen 2.5 ViT and LLM (@zhyajie)
- Opened: [#19007](https://github.com/sgl-project/sglang/pull/19007) [AMD] Replace msgpack with msgspec in MORI-IO (@Duyi-Wang)
- Opened: [#18930](https://github.com/sgl-project/sglang/pull/18930) [AMD] Unit tests for mtp in GLM-4.7  (@almaslof)
- Opened: [#18978](https://github.com/sgl-project/sglang/pull/18978) [AMD]  Fix mi35x dsv32 mtp nightly (@bingxche)
- Opened: [#18982](https://github.com/sgl-project/sglang/pull/18982) [Doc] Add `flashinfer_deepgemm` to `--fp8-gemm-backend` (@mmangkad)
- Opened: [#18972](https://github.com/sgl-project/sglang/pull/18972) [AMD] ROCm7.2: Add /sgl-workspace/aiter to PYTHONPATH (@HaiShaw)
- Opened: [#18919](https://github.com/sgl-project/sglang/pull/18919) [bugfix?] update outdated unittest document (@SoluMilken)
- Opened: [#18916](https://github.com/sgl-project/sglang/pull/18916) [TorchAO] Enable TorchAO LinearMethod and TorchAOConfig (@ZhiweiYan-96)
- Opened: [#18920](https://github.com/sgl-project/sglang/pull/18920) ROCm use rotary_embedding from sgl-kernel (@HaiShaw)
- Opened: [#18922](https://github.com/sgl-project/sglang/pull/18922) Revert "[AMD] Fix RotaryEmbedding crash on AMD/ROCm (regress (@HaiShaw)
- Opened: [#18903](https://github.com/sgl-project/sglang/pull/18903) [AMD] Fix RotaryEmbedding crash on AMD/ROCm (regression from (@michaelzhang-ai)
- Merged: [#17953](https://github.com/sgl-project/sglang/pull/17953) [AMD] support two batch overlapping for mori ep (@billishyahao)
- Merged: [#18252](https://github.com/sgl-project/sglang/pull/18252) [4/N] Quantization Refactor: Quark MoE schemes (@TamirBaydasov)
- Merged: [#17993](https://github.com/sgl-project/sglang/pull/17993) [3/N] Quantization Refactor: ModelSlim MoE schemes (@TamirBaydasov)
- Merged: [#18761](https://github.com/sgl-project/sglang/pull/18761) [AMD] Fix nightly 1-GPU test failures and bench_serving regr (@michaelzhang-ai)
- Merged: [#18395](https://github.com/sgl-project/sglang/pull/18395) [Doc] Convert the speculative decoding notebook to markdow (@alphabetc1)
- Merged: [#17503](https://github.com/sgl-project/sglang/pull/17503) [2/N] Quantization Refactor: Compressed tensors MoE schemes (@TamirBaydasov)
- Merged: [#18437](https://github.com/sgl-project/sglang/pull/18437) [AMD] MORI-EP inter kernel type switch (@Duyi-Wang)

### xla
- Opened: [#38088](https://github.com/openxla/xla/pull/38088) [ROCm] Support hipblaslt group-gemm (@mfrancepillois)
- Opened: [#38019](https://github.com/openxla/xla/pull/38019) PR #38007: Gracefully handle missing handler on legacy custo (@copybara-service[bot])
- Opened: [#38023](https://github.com/openxla/xla/pull/38023) PR #36882: [ROCm] suppress cuda error messages on ROCm when  (@copybara-service[bot])
- Opened: [#38075](https://github.com/openxla/xla/pull/38075) PR #37573: [ROCm] Enable FP8 Triton GEMM support for ROCm GP (@copybara-service[bot])
- Opened: [#38076](https://github.com/openxla/xla/pull/38076) PR #36564: [ROCm][TSAN] Read device_count from collective pa (@copybara-service[bot])
- Opened: [#38028](https://github.com/openxla/xla/pull/38028) PR #37732: [ROCm] Fix tools/xla_*_compile_lib_tests (@copybara-service[bot])
- Opened: [#37992](https://github.com/openxla/xla/pull/37992) [ROCm] Fix invalid split_k/block_k configs in autotune searc (@nurmukhametov)
- Opened: [#37981](https://github.com/openxla/xla/pull/37981) PR #37629: [ROCm] Propagate register spill info for autotune (@copybara-service[bot])
- Opened: [#37883](https://github.com/openxla/xla/pull/37883) PR #37854: [ROCm] Fix TritonAMDGPUSinkLayoutConversions pass (@copybara-service[bot])
- Merged: [#37641](https://github.com/openxla/xla/pull/37641) PR #37513: [ROCm] Add option to disable automatic solib and  (@copybara-service[bot])
- Merged: [#37643](https://github.com/openxla/xla/pull/37643) PR #37287: [ROCm] Skip swish fusion if pre-activation output (@copybara-service[bot])

### triton
- Opened: [#9506](https://github.com/triton-lang/triton/pull/9506) [AMD] Fix TensorDescType shared memory size for WS captures (@PMylon)
- Opened: [#9533](https://github.com/triton-lang/triton/pull/9533) [AMD] Update default to `block_m=16` in `make_default_opt_fl (@micah-wil)
- Opened: [#9512](https://github.com/triton-lang/triton/pull/9512) [AMD][NFC] Emit error for buffer_load_to_local on gfx1250 (@AlexAUT)
- Opened: [#9519](https://github.com/triton-lang/triton/pull/9519) [AMD][NFC] Fix error message for wmma scale (@borontion)
- Opened: [#9522](https://github.com/triton-lang/triton/pull/9522) [AMD] Update gfx1250 MXFP FA example kernel (@borontion)
- Opened: [#9509](https://github.com/triton-lang/triton/pull/9509) [AMD] Enable supportBitwidth{16|32}Elementwise in TargetInfo (@antiagainst)
- Opened: [#9513](https://github.com/triton-lang/triton/pull/9513) [AMD][GLUON] Allow DistributedLayouts in AsyncCopy and Buffe (@AlexAUT)
- Opened: [#9502](https://github.com/triton-lang/triton/pull/9502) [AMD][BACKEND] Cherry pick pr 9487 to rel 3.7 (@AmdSampsa)
- Opened: [#9496](https://github.com/triton-lang/triton/pull/9496) [AMD][gfx1250] Fix tensordesc index after kernel launch chan (@antiagainst)
- Opened: [#9494](https://github.com/triton-lang/triton/pull/9494) Revert "[AMD] Don't use s_waitcnt to lower global barrier fo (@antiagainst)
- Opened: [#9490](https://github.com/triton-lang/triton/pull/9490) [AMD]Fix a bug about CGA-layout in AccelerateAMDMatmul.  (@yangshuxin)
- Opened: [#9487](https://github.com/triton-lang/triton/pull/9487) [AMD][BACKEND] Properly handle PointerTypes in v_perm Conver (@AlexAUT)
- Opened: [#9476](https://github.com/triton-lang/triton/pull/9476) [AMD] Disable True16 for assembler on gfx11 (#9447) (@jataylo)
- Merged: [#8464](https://github.com/triton-lang/triton/pull/8464) [AMD] Optimize address increments for buffer loads in loops (@alefimov-amd)
- Merged: [#9374](https://github.com/triton-lang/triton/pull/9374) Reapply "[AMD] Introduce PartitionedSharedEncodingAttr" (#93 (@plognjen)
- Merged: [#9442](https://github.com/triton-lang/triton/pull/9442) [AMD][BACKEND] Fix OOM bug in pipelining with padded layout  (@pabloantoniom)
- Merged: [#9455](https://github.com/triton-lang/triton/pull/9455) [AMD] Enable floating-point sanitizer (FpSan) support (@kelesvol)
- Merged: [#9302](https://github.com/triton-lang/triton/pull/9302) [AMD][gfx1250] Support TDM in software pipelining  (@yangshuxin)
- Merged: [#9449](https://github.com/triton-lang/triton/pull/9449) [AMD] Added hw FP upcast conversions for gfx1250 (@ravil-mobile)

### migraphx
- Opened: [#4623](https://github.com/ROCm/AMDMIGraphX/pull/4623) Add support for ORT image in Jenkins pipeline (@causten)
- Opened: [#4627](https://github.com/ROCm/AMDMIGraphX/pull/4627) rocMLIR Weekly Sync 2026-02-22 (@github-actions[bot])
- Opened: [#4620](https://github.com/ROCm/AMDMIGraphX/pull/4620) [AIMIGRAPHX-542] implement argmin and argmax as reduce ops (@bdevorem)
- Opened: [#4626](https://github.com/ROCm/AMDMIGraphX/pull/4626) Add debug symbols for parsed and compiler pass replaced inst (@CharlieL7)
- Opened: [#4616](https://github.com/ROCm/AMDMIGraphX/pull/4616) [AIMIGRAPHX-544] Parallel compilation for dynamic graphs (@shivadbhavsar)
- Opened: [#4625](https://github.com/ROCm/AMDMIGraphX/pull/4625) [AIMIGRAPHX-544] Parallel compilation for dynamic graphs (@shivadbhavsar)
- Opened: [#4624](https://github.com/ROCm/AMDMIGraphX/pull/4624) Onnxruntime Weekly Sync 2026-02-20 (@github-actions[bot])
- Opened: [#4617](https://github.com/ROCm/AMDMIGraphX/pull/4617) Fuse GQA local window as kv-cache attention (@turneram)
- Opened: [#4619](https://github.com/ROCm/AMDMIGraphX/pull/4619) AIMIGRAPHX-578 Reintroduce blaze for better ref gemm perform (@kahmed10)
- Opened: [#4622](https://github.com/ROCm/AMDMIGraphX/pull/4622) Bump rocm-docs-core from 1.31.3 to 1.32.0 in /docs/sphinx (@dependabot[bot])
- Opened: [#4621](https://github.com/ROCm/AMDMIGraphX/pull/4621) [AIMIGRAPHX-571] Rewrite convolutions to GEMMs for constant  (@eddieliao)
- Merged: [#4432](https://github.com/ROCm/AMDMIGraphX/pull/4432) Improve layout propagation in poinwise fusion when using bro (@pfultz2)
- Merged: [#4592](https://github.com/ROCm/AMDMIGraphX/pull/4592) Bump protobuf from 4.25.8 to 6.33.5 in /tools (@dependabot[bot])
- Merged: [#4593](https://github.com/ROCm/AMDMIGraphX/pull/4593) Bump protobuf from 4.25.8 to 5.29.6 in /test/py (@dependabot[bot])
- Merged: [#4602](https://github.com/ROCm/AMDMIGraphX/pull/4602) Bump cryptography from 44.0.1 to 46.0.5 in /docs/sphinx (@dependabot[bot])
- Merged: [#4609](https://github.com/ROCm/AMDMIGraphX/pull/4609) Propagate constant optimization (@pnikolic-amd)
- Merged: [#4410](https://github.com/ROCm/AMDMIGraphX/pull/4410) clamping the scale (@aarushjain29)
- Merged: [#4510](https://github.com/ROCm/AMDMIGraphX/pull/4510) [BugFix] - Fix tile byte size overflow for LDS memory when p (@ivarusic-amd)
- Merged: [#4615](https://github.com/ROCm/AMDMIGraphX/pull/4615) rocMLIR Weekly Sync 2026-02-15 (@github-actions[bot])
- Merged: [#4362](https://github.com/ROCm/AMDMIGraphX/pull/4362) disable matching for dynamic shapes (@shivadbhavsar)
- Merged: [#4443](https://github.com/ROCm/AMDMIGraphX/pull/4443) [AIMIGRAPHX-326] Fix "reduce_sum: axes: value out of range"  (@pfultz2)
- Merged: [#4445](https://github.com/ROCm/AMDMIGraphX/pull/4445) Show attributes in onnx trace (@pfultz2)
- Merged: [#4393](https://github.com/ROCm/AMDMIGraphX/pull/4393) Flash decoding round 1; AIMIGRAPHX-242 (@bdevorem)
- Merged: [#4396](https://github.com/ROCm/AMDMIGraphX/pull/4396) Refactor GroupQueryAttention (@turneram)

### aiter
- Opened: [#2066](https://github.com/ROCm/aiter/pull/2066) [TRITON] Sage attention v2: Q*K in mxfp4 (@juuso-oskari)
- Opened: [#2075](https://github.com/ROCm/aiter/pull/2075) Use unreg path for custom all-reduce during CUDA graph captu (@zyzshishui)
- Opened: [#2074](https://github.com/ROCm/aiter/pull/2074) Add ENABLE_CK build option for CK-free builds (@sunway513)
- Opened: [#2064](https://github.com/ROCm/aiter/pull/2064) Adding double buffer option to cross_device_reduce_1stage (@RichardChamberlain1)
- Opened: [#2071](https://github.com/ROCm/aiter/pull/2071) Add CLAUDE.md and skill for tune_ck_gemm_a8w8_blockscale. (@sabreshao)
- Opened: [#2067](https://github.com/ROCm/aiter/pull/2067) Amd/satya/gluon/gemm mxfp4 (@Boss2002n)
- Opened: [#2069](https://github.com/ROCm/aiter/pull/2069) Add compiler configurations for bpreshuffle_cktile modules (@kensclin)
- Opened: [#2062](https://github.com/ROCm/aiter/pull/2062) Add ENABLE_CK=0 build option for Triton-only builds (@sunway513)
- Opened: [#2068](https://github.com/ROCm/aiter/pull/2068) Optimize top-k top-p sampler kernel by prefetching data (@aryaman-gupta)
- Opened: [#2056](https://github.com/ROCm/aiter/pull/2056) Enabling FPMX4 GEMM on non-FPMX4 devices (Navi31 in particul (@ekuznetsov139)
- Opened: [#2060](https://github.com/ROCm/aiter/pull/2060) GFX1250 Kernels - GEMMa8w8 blockscale (@amirumoAMD)
- Opened: [#2070](https://github.com/ROCm/aiter/pull/2070) [OPUS] Add f32 MFMA support (@carlushuang)
- Opened: [#2055](https://github.com/ROCm/aiter/pull/2055) Silence certain warnings stemming from CK (@Micky774)
- Opened: [#2065](https://github.com/ROCm/aiter/pull/2065) revert triton gemm kernel config due to core dump. (@Duyi-Wang)
- Opened: [#2057](https://github.com/ROCm/aiter/pull/2057) hotfix a8w8 gemm config (@valarLip)
- Merged: [#2037](https://github.com/ROCm/aiter/pull/2037) Add MI355X tuned GEMM configs for FP4 and FP8 (@sunway513)
- Merged: [#2034](https://github.com/ROCm/aiter/pull/2034) Top-K Top-P Sampling Kernel Optimization (@aryaman-gupta)
- Merged: [#1954](https://github.com/ROCm/aiter/pull/1954) feat(ck_tile): add a8w8 blockscale gemm with preshuffleB sup (@kensclin)
- Merged: [#2048](https://github.com/ROCm/aiter/pull/2048) [Gluon] Unified Attention 3D development for gfx12 (@k50112113)
- Merged: [#2016](https://github.com/ROCm/aiter/pull/2016) tune triton gemm kernel for MI355 DSV3 DP+EP configuration (@inkcherry)

### atom
- Opened: [#228](https://github.com/ROCm/ATOM/pull/228) enable mtp=3 (@valarLip)
- Opened: [#220](https://github.com/ROCm/ATOM/pull/220) Enable Triton MXFP4 MoE on gfx950 for GPT-OSS (@ChuanLi1101)
- Opened: [#227](https://github.com/ROCm/ATOM/pull/227) Refactor ATOM for top-k top-p sampling support (@aryaman-gupta)
- Opened: [#225](https://github.com/ROCm/ATOM/pull/225) Add FlyDSL MOE backend and Triton fallback for FP8 MoE (@sunway513)
- Opened: [#226](https://github.com/ROCm/ATOM/pull/226) Enable Triton MOE for MXFP4 on gfx950 (MI355X) (@sunway513)
- Opened: [#223](https://github.com/ROCm/ATOM/pull/223) Add Quark GLM4.7-MXFP4 support (@thpereir)
- Opened: [#222](https://github.com/ROCm/ATOM/pull/222) Fix prefix caching crash: recalculate num_new_tokens after b (@ChuanLi1101)
- Opened: [#219](https://github.com/ROCm/ATOM/pull/219) mtp refine (@valarLip)
- Opened: [#224](https://github.com/ROCm/ATOM/pull/224) Add Dockerfile.clean + fix linear.py shard_offset bug (@sunway513)

### mori
- Opened: [#172](https://github.com/ROCm/mori/pull/172) Feat: Enable async kernel BF16 cast to FP8 combine (@isytwu)
- Opened: [#171](https://github.com/ROCm/mori/pull/171) Fix: support runtime hidden_dim for dispatch/combine (@isytwu)
- Opened: [#170](https://github.com/ROCm/mori/pull/170) Optimize: EP4 intranode kernel for FP4 dispatch + FP8 combin (@jhchouuu)

### flydsl
- Opened: [#143](https://github.com/ROCm/FlyDSL/pull/143) W4a16 bf16 fp8 bf16 moe (@ClementLinCF)
- Opened: [#141](https://github.com/ROCm/FlyDSL/pull/141) mxfp4 preshuffled gemm optimize (@yadaish)
- Opened: [#139](https://github.com/ROCm/FlyDSL/pull/139) optimize buffer_load lds pipeline. Now it can interleave wit (@yadaish)
- Opened: [#140](https://github.com/ROCm/FlyDSL/pull/140) fix(build): dereference MLIR symlinks in build output (@sunway513)
- Opened: [#138](https://github.com/ROCm/FlyDSL/pull/138) refactor the arch check related to bf16 global atomics for e (@hongxiayang)
- Opened: [#137](https://github.com/ROCm/FlyDSL/pull/137) add declaimer (@hongxiayang)
- Opened: [#136](https://github.com/ROCm/FlyDSL/pull/136) fix N/A SKU and replace it with gfx for gpu information in r (@hongxiayang)
- Merged: [#129](https://github.com/ROCm/FlyDSL/pull/129) [MoE] simplify moe reduce kernel & add zero buffer flag (@aoli26)
- Merged: [#98](https://github.com/ROCm/FlyDSL/pull/98) fix a4w4 gemm precision (@zhiding512)

## New Issues This Week

### pytorch
- [#175520](https://github.com/pytorch/pytorch/issues/175520) Question about torch._dynamo.export and nn.GRU support (@raindrops-0199)
- [#175431](https://github.com/pytorch/pytorch/issues/175431) out‑of‑memory error, although most of GPU memory is still av (@amd-xiaoyu12)
- [#175489](https://github.com/pytorch/pytorch/issues/175489) CPU `F.interpolate(..., antialias=True)` raises NotImplement (@gautamvarmadatla)
- [#175368](https://github.com/pytorch/pytorch/issues/175368) `F.embedding_bag` segfaults when intermediate offsets exceed (@SilentTester73)
- [#175370](https://github.com/pytorch/pytorch/issues/175370) `F.embedding_bag` segfaults with float64 weight and empty of (@SilentTester73)
- [#175484](https://github.com/pytorch/pytorch/issues/175484) Unable to disable warning about tf32 precision (@ad8e)
- [#175482](https://github.com/pytorch/pytorch/issues/175482) DISABLED test_index_put_error_cuda (__main__.TestNestedTenso (@jithunnair-amd)
- [#175477](https://github.com/pytorch/pytorch/issues/175477) make_fx symbolic tracing produces wrong 2nd-order gradients  (@Pythonix)
- [#175475](https://github.com/pytorch/pytorch/issues/175475) DISABLED test_custom_op_with_layout_arg_xpu (__main__.AOTInd (@etaf)
- [#175436](https://github.com/pytorch/pytorch/issues/175436) Intermittent SIGILL in `torch.sin` on CPU (`torch==2.10.0+cp (@jasondavies)
- [#175325](https://github.com/pytorch/pytorch/issues/175325) Incorrect storage offsets propagation in inductor with as_st (@Rakul-Chauhan)
- [#175211](https://github.com/pytorch/pytorch/issues/175211) CUDA/ROCm/Accelerator testing should replace get_device_capa (@jeffdaily)

### vllm
- [#35084](https://github.com/vllm-project/vllm/issues/35084) [Bug]: VLLM tries to load "inductor" instead of custom compi (@mergian)
- [#35057](https://github.com/vllm-project/vllm/issues/35057) [Bug]: Qwen3.5 `scheduler_metadata must have shape (metadata (@ehfd)
- [#35028](https://github.com/vllm-project/vllm/issues/35028) [Bug]: RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE (@shahizat)
- [#35056](https://github.com/vllm-project/vllm/issues/35056) [Bug]: Qwen3.5 `AttributeError: 'MRotaryEmbedding' object ha (@ehfd)
- [#34994](https://github.com/vllm-project/vllm/issues/34994) [Feature]: Infrastructure Improvements for ROCm CI (@AndreasKaratzas)
- [#35031](https://github.com/vllm-project/vllm/issues/35031) [Bug]: MTP Speculative Decoding with NVFP4: Weight Shape Mis (@eleqtrizit)
- [#35061](https://github.com/vllm-project/vllm/issues/35061) [Bug]: [torch.compile] occasional failure to save AOT compil (@cjackal)
- [#34954](https://github.com/vllm-project/vllm/issues/34954) [Bug]: Triton Error [CUDA]: out of memory when received quer (@kwonmha)
- [#34859](https://github.com/vllm-project/vllm/issues/34859) [Bug]: missing shards from quantized checkpoint fails silent (@andrea-fasoli)
- [#34812](https://github.com/vllm-project/vllm/issues/34812) [Bug]: GraniteMoeHybridModel not applying embedding_multipli (@gabe-l-hart)
- [#34705](https://github.com/vllm-project/vllm/issues/34705) [Bug]: Old torch compile files cause poor CPU utilisation (@almayne)
- [#34650](https://github.com/vllm-project/vllm/issues/34650) Bug: Speculative Decoding (MTP) Causes </think> Detection Fa (@cicirori)
- [#34939](https://github.com/vllm-project/vllm/issues/34939) [CI Failure]: V1 e2e + engine : Cannot re-initialize CUDA in (@varun-sundar-rabindranath)
- [#34851](https://github.com/vllm-project/vllm/issues/34851) [Feature]: Refactor Quark MoE and mxfp4 MoE to align with Mo (@BowenBao)
- [#34755](https://github.com/vllm-project/vllm/issues/34755) Qwen3-Coder-Next-FP8 with tool calling causes system hard-fr (@zaidorx)

### sglang
- [#19139](https://github.com/sgl-project/sglang/issues/19139) [Rfc] Refactor server_args.py (@vincentzed)
- [#19031](https://github.com/sgl-project/sglang/issues/19031) [Feature] ROCm nightly in upstream lmsysorg docker org (@functionstackx)
- [#19028](https://github.com/sgl-project/sglang/issues/19028) [Bug] GLM5 nightly Mi355 broken due to transformer dependenc (@functionstackx)

### migraphx
- [#4618](https://github.com/ROCm/AMDMIGraphX/issues/4618) [Issue]: MIGraphX Dynamic Shape Issue ONNXRuntime (@DiarmuidKelly)

### aiter
- [#2061](https://github.com/ROCm/aiter/issues/2061) [Bug] Custom all-reduce IPC buffers use fixed VA, conflict w (@jhinpan)
- [#2073](https://github.com/ROCm/aiter/issues/2073) [RFC] CK-Free Build: Enable AITER builds without Composable  (@sunway513)
- [#2059](https://github.com/ROCm/aiter/issues/2059) [Issue]: GLM-5 aiter fused_moe with SGLang + MI355 (@ozziemoreno)
- [#2054](https://github.com/ROCm/aiter/issues/2054) [Feature]: Migrate Python bindings from pybind11 to apache-t (@carlushuang)

### atom
- [#229](https://github.com/ROCm/ATOM/issues/229) [RFC] CK-Free AITER Compatibility: Attention, MOE, and Docke (@sunway513)
- [#221](https://github.com/ROCm/ATOM/issues/221) [Issue]: ATOM fails on Qwen3 model when the flag "--enable_p (@vecheruk-amd)

### flydsl
- [#142](https://github.com/ROCm/FlyDSL/issues/142) finish reconstruction of layout algebra (@coderfeli)
