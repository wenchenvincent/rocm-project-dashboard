# Weekly Digest

Week of 2026-02-18 to 2026-02-25

## New Releases

- **sglang**: [v0.5.9](https://github.com/sgl-project/sglang/releases/tag/v0.5.9)

## PRs This Week

### pytorch
- Opened: [#175549](https://github.com/pytorch/pytorch/pull/175549) [inductor] Fix stride mismatch in K==1 mm/addmm decompositio (@romanmeta)
- Opened: [#175565](https://github.com/pytorch/pytorch/pull/175565) [wip][Dynamo] Follow CPython LOAD_ATTR on UserDefinedClassVa (@anijain2305)
- Opened: [#175593](https://github.com/pytorch/pytorch/pull/175593) [vision hash update] update the pinned vision hash (@pytorchupdatebot)
- Opened: [#175653](https://github.com/pytorch/pytorch/pull/175653) [Inductor] Only use bias_addmm if compatible (@PaulZhang12)
- Opened: [#175281](https://github.com/pytorch/pytorch/pull/175281) [WIP] Safely handle when decompositions add guards (@eellison)
- Opened: [#175630](https://github.com/pytorch/pytorch/pull/175630) [Inductor] Benchmark with layout constrained stride (@PaulZhang12)
- Opened: [#175428](https://github.com/pytorch/pytorch/pull/175428) [MXFP4] Fix E8M0 blockwise scale size validation for packed  (@pranavsharma)
- Opened: [#175677](https://github.com/pytorch/pytorch/pull/175677) [BE][inductor] Apply PEP 604 type annotations (part 3/3) (@Lucaskabela)
- Opened: [#175471](https://github.com/pytorch/pytorch/pull/175471) Update third_party/kineto to add cuMem_ driver activity supp (@ryanzhang22)
- Opened: [#175278](https://github.com/pytorch/pytorch/pull/175278) [inductor] Apply custom op autotuning to aten.mm (@eellison)
- Opened: [#175671](https://github.com/pytorch/pytorch/pull/175671) Skip large-XBLOCK autotune configs for combo kernels with pe (@Bao2803)
- Opened: [#175706](https://github.com/pytorch/pytorch/pull/175706) Use C++20 bit operations and contains to simplify code (@cyyever)
- Opened: [#175514](https://github.com/pytorch/pytorch/pull/175514) [dynamo] Refactor UserDefinedObjectVariable var_getattr (@anijain2305)
- Opened: [#175648](https://github.com/pytorch/pytorch/pull/175648) [ROCm] Require rocm_smi package (@naromero77amd)
- Opened: [#175701](https://github.com/pytorch/pytorch/pull/175701) Make hipify input file reading more robust (@davispuh)
- Opened: [#175427](https://github.com/pytorch/pytorch/pull/175427) Adjust bfloat16 tolerance for rocm in test_rms_norm_sharing_ (@jsmolic)
- Opened: [#175559](https://github.com/pytorch/pytorch/pull/175559) [Do not Merge] Revert "[ROCm] Upgrade GCC version to version (@jagadish-amd)
- Opened: [#175303](https://github.com/pytorch/pytorch/pull/175303) [ROCM] Refactor BFloat16 implementation for native usage of  (@anatoliylitv)
- Opened: [#175625](https://github.com/pytorch/pytorch/pull/175625) Skip test_scaled_mm for ROCm (@mstankov-amd)
- Opened: [#175473](https://github.com/pytorch/pytorch/pull/175473) [Claude] Add CI ARC migration skill for Claude Code (@huydhn)
- Opened: [#175553](https://github.com/pytorch/pytorch/pull/175553) [TESTING] Triton37 ROCm testing PR (@iupaikov-amd)
- Opened: [#175468](https://github.com/pytorch/pytorch/pull/175468) [DO NOT MERGE] Test AMD Capacity. (@saienduri)
- Opened: [#175286](https://github.com/pytorch/pytorch/pull/175286) [ROCm] No-fence in normalization kernel (@anatoliylitv)
- Opened: [#175299](https://github.com/pytorch/pytorch/pull/175299) [benchmark] Skip pytorch_CycleGAN_and_pix2pix from inductor  (@pytorchbot)
- Merged: [#175159](https://github.com/pytorch/pytorch/pull/175159) [ROCm] forward fix #174087, take 4 (@pytorchbot)

### jax
- Opened: [#35315](https://github.com/jax-ml/jax/pull/35315) [ROCm] Fix jax utests with rocm (@alekstheod)
- Opened: [#35288](https://github.com/jax-ml/jax/pull/35288) [ROCm] Implement Mosaic GPU detection and Auto-Skips (@gulsumgudukbay)
- Opened: [#35373](https://github.com/jax-ml/jax/pull/35373) [ROCm] Replace abseiltest with py test in rocm bazel (@alekstheod)
- Opened: [#35283](https://github.com/jax-ml/jax/pull/35283) [ROCm] Improve ROCm pytest results handling (@psanal35)
- Merged: [#31768](https://github.com/jax-ml/jax/pull/31768) [ROCm] Support lowering through PJRT_Triton_Extension (@amd-jianli12)
- Merged: [#35115](https://github.com/jax-ml/jax/pull/35115) [ROCm] Add hip_threefry2x32_ffi to stable custom call target (@AratiGanesh)

### vllm
- Opened: [#35183](https://github.com/vllm-project/vllm/pull/35183) [ROCm] Split wvSplitKrc into deterministic/fast kernels, add (@AndreasKaratzas)
- Opened: [#34907](https://github.com/vllm-project/vllm/pull/34907) [ROCm][P/D][MORI][BugFix] Add transfer_id for moriio_connect (@rasmith)
- Opened: [#34801](https://github.com/vllm-project/vllm/pull/34801) [ROCm][Kernel] Add fused MoE exllama GEMM for compressed-ten (@mgehre-amd)
- Opened: [#35196](https://github.com/vllm-project/vllm/pull/35196) [ROCm] [CI] Gate the changes to `Dockerfile.rocm_base`. (@tjtanaa)
- Opened: [#34848](https://github.com/vllm-project/vllm/pull/34848) [ROCm] Add extra step in config initialization to populate c (@gshtras)
- Opened: [#35265](https://github.com/vllm-project/vllm/pull/35265) [ROCm][CI] Extending attention backend coverage for Eagle sp (@AndreasKaratzas)
- Opened: [#35042](https://github.com/vllm-project/vllm/pull/35042) [Platform] Add current_platform.num_compute_units interface (@jikunshang)
- Opened: [#35011](https://github.com/vllm-project/vllm/pull/35011) remove cuda check in `top_k_top_p_triton` kernel (@jikunshang)
- Opened: [#35131](https://github.com/vllm-project/vllm/pull/35131) Fixing matcher to enable the 2-node-tests-4-gpus-in-total on (@Alexei-V-Ivanov-AMD)
- Opened: [#34931](https://github.com/vllm-project/vllm/pull/34931) [AMD][CI] Support Triton attention with ExampleConnector (@rjrock)
- Opened: [#35156](https://github.com/vllm-project/vllm/pull/35156) [BUGFIX][Qwen3.5] Hardcode `mlp.gate` as not quantizable  (@vadiklyutiy)
- Opened: [#35180](https://github.com/vllm-project/vllm/pull/35180) [ROCm]: Enable customop and rope+kvcache fusion for AITER Ro (@Rohan138)
- Opened: [#35246](https://github.com/vllm-project/vllm/pull/35246) [ROCm] Refactor attention backend selection logic (@SageMoore)
- Opened: [#35253](https://github.com/vllm-project/vllm/pull/35253) Enabling some B200-specific tests on MI355 (@Alexei-V-Ivanov-AMD)
- Opened: [#34998](https://github.com/vllm-project/vllm/pull/34998) [ROCm] Check that AITER MHA is not selected with sinks (@gshtras)
- Opened: [#34985](https://github.com/vllm-project/vllm/pull/34985) [CI][AMD][BugFix] Add  torch.cuda.set_device to test_punica_ (@rasmith)
- Opened: [#35250](https://github.com/vllm-project/vllm/pull/35250) [Bugfix][Hardware][AMD] Gate FP4 ops on gfx950 to prevent MI (@c0de128)
- Opened: [#34773](https://github.com/vllm-project/vllm/pull/34773) [Misc][LoRA] Increase max vocab size limit to 258048 in logi (@bhoomit)
- Opened: [#35245](https://github.com/vllm-project/vllm/pull/35245) [ROCm][WIP]: Fused aiter rope kvcache mla (@Rohan138)
- Opened: [#35239](https://github.com/vllm-project/vllm/pull/35239) [ROCm][CI] Added MI325 mirrors (stage C) (@AndreasKaratzas)
- Opened: [#35170](https://github.com/vllm-project/vllm/pull/35170) [ROCm][CI] Adding infiniband mappings for moriio tests (@AndreasKaratzas)
- Opened: [#35232](https://github.com/vllm-project/vllm/pull/35232) [Build] Fix LTO build for ROCm when default compiler is GCC  (@davispuh)
- Opened: [#34839](https://github.com/vllm-project/vllm/pull/34839) [ROCm][CI] Cleaning and restructuring amd-ci legacy pipeline (@AndreasKaratzas)
- Opened: [#34923](https://github.com/vllm-project/vllm/pull/34923) [ROCm][CI] Added MI325 mirrors (@AndreasKaratzas)
- Opened: [#35224](https://github.com/vllm-project/vllm/pull/35224) [ROCm][CI] Accept Different But Valid Output for `test_olmoe (@micah-wil)
- Opened: [#35144](https://github.com/vllm-project/vllm/pull/35144) [ROCm] Enable GPTQMarlinConfig on ROCm to use choose_mp_line (@mgehre-amd)
- Opened: [#35198](https://github.com/vllm-project/vllm/pull/35198) [ROCm] [Release] Change the package from `aiter` to `amd-ait (@tjtanaa)
- Opened: [#35069](https://github.com/vllm-project/vllm/pull/35069) [ROCm] Derive device capability from GCN arch string without (@AndreasKaratzas)
- Opened: [#34878](https://github.com/vllm-project/vllm/pull/34878) [ROCm][Test] Fix beam search determinism failures from batch (@AndreasKaratzas)
- Opened: [#34798](https://github.com/vllm-project/vllm/pull/34798) [Mamba1] - Kernel Level Chunk Alignment for Prefix Caching (@Josephasafg)
- Opened: [#35145](https://github.com/vllm-project/vllm/pull/35145) [ROCm][CI] Reverting changes in MI355 pipeline so that some  (@AndreasKaratzas)
- Opened: [#34879](https://github.com/vllm-project/vllm/pull/34879) [ROCm][CI] Removing all blocking labels from MI355 until sta (@AndreasKaratzas)
- Opened: [#35043](https://github.com/vllm-project/vllm/pull/35043) [ROCm][CI] Fix spec decode profile assertion and logprob tes (@AndreasKaratzas)
- Opened: [#35052](https://github.com/vllm-project/vllm/pull/35052) [ROCm][CI] Fix realtime test timeouts caused by aiter JIT co (@AndreasKaratzas)
- Opened: [#35050](https://github.com/vllm-project/vllm/pull/35050) [ROCm][CI] Fix flaky embedding chat test by using tolerance- (@AndreasKaratzas)
- Opened: [#34961](https://github.com/vllm-project/vllm/pull/34961) [CI] Bump mteb version to `mteb[bm25s]>=2, <3` for pooling m (@yewentao256)
- Opened: [#35008](https://github.com/vllm-project/vllm/pull/35008) [CI] Stabilizing ROCm amd-ci signal and minor name fix in up (@AndreasKaratzas)
- Opened: [#35013](https://github.com/vllm-project/vllm/pull/35013) [CI/Build] Fix gRPC version mismatch (@DarkLight1337)
- Opened: [#34922](https://github.com/vllm-project/vllm/pull/34922) [ROCm][CI] Loosen RemoteOpenAIServer Startup Timeout (@micah-wil)
- Opened: [#34918](https://github.com/vllm-project/vllm/pull/34918) Change targets for AMD build in the "CI" pipeline (@Alexei-V-Ivanov-AMD)
- Merged: [#34100](https://github.com/vllm-project/vllm/pull/34100) Convert wvSplitKQ to 16x16 MFMA in prep for mi4xx. (@amd-hhashemi)
- Merged: [#34302](https://github.com/vllm-project/vllm/pull/34302) [ModelBash][DSV3] Add TRTLLM DSV3 Router GEMM kernel (6% B1  (@robertgshaw2-redhat)
- Merged: [#33948](https://github.com/vllm-project/vllm/pull/33948) [Bugfix]: Fix ROCm fusion attn test; use AttentionBackend ut (@Rohan138)
- Merged: [#33943](https://github.com/vllm-project/vllm/pull/33943) move checks out of `unified_kv_cache_update` custom op (@Rohan138)
- Merged: [#30976](https://github.com/vllm-project/vllm/pull/30976) Use aiter triton fused_add_rmsnorm_pad for gpt-oss (@Rohan138)
- Merged: [#33443](https://github.com/vllm-project/vllm/pull/33443) [ROCm] AITER fused RoPE+KVCache (@Rohan138)
- Merged: [#33277](https://github.com/vllm-project/vllm/pull/33277) [ROCm][CI] Force max_num_seqs=1 on ROCm In test_sharded_stat (@micah-wil)
- Merged: [#34466](https://github.com/vllm-project/vllm/pull/34466) [CI/Build] Add opentelemetry libs in default vllm build (req (@vladmihailescu)
- Merged: [#34688](https://github.com/vllm-project/vllm/pull/34688) [ROCm] Enable bitsandbytes quantization support on ROCm (@Abdennacer-Badaoui)
- Merged: [#34636](https://github.com/vllm-project/vllm/pull/34636) [ROCm][Bugfix]: Only save unpadded sizes for shared_experts  (@Rohan138)
- Merged: [#34541](https://github.com/vllm-project/vllm/pull/34541) [ROCM] Optimize ROCM_AITER_FA spec decode eagle performance (@jennyyyyzhen)
- Merged: [#34570](https://github.com/vllm-project/vllm/pull/34570) [ROCm][AITER] Fix aiter paged_attention_v1 decode for slidin (@AndreasKaratzas)
- Merged: [#34599](https://github.com/vllm-project/vllm/pull/34599) [ROCm][CI] Fix spec decode logprobs flakiness and parametriz (@AndreasKaratzas)
- Merged: [#34567](https://github.com/vllm-project/vllm/pull/34567) [CI] Fix ColBERT HF comparison tests on AMD CI + refactor (@AndreasKaratzas)
- Merged: [#33949](https://github.com/vllm-project/vllm/pull/33949) [CI][MCP][Harmony] Heavy refactoring Harmony & MCP response  (@AndreasKaratzas)
- Merged: [#34574](https://github.com/vllm-project/vllm/pull/34574) [Frontend] Support multimodal inputs for late-interaction sc (@craftsangjae)
- Merged: [#34735](https://github.com/vllm-project/vllm/pull/34735) [AMD][CI] Fix test_custom_allreduce for A100 testgroup (@rjrock)
- Merged: [#33501](https://github.com/vllm-project/vllm/pull/33501) Fix DeepSeek RoPE initialization error (@catswe)
- Merged: [#32877](https://github.com/vllm-project/vllm/pull/32877) [Bugfix][Hardware][AMD] Fix ROCM_AITER_FA speculative decodi (@c0de128)
- Merged: [#33739](https://github.com/vllm-project/vllm/pull/33739) [CI][AMD][BugFix][P/D] Add default_vllm_config to test_morii (@rasmith)
- Merged: [#32993](https://github.com/vllm-project/vllm/pull/32993) [Feature] Support CPU Offloading without Pytorch Pinned Memo (@wzhao18)
- Merged: [#26535](https://github.com/vllm-project/vllm/pull/26535) [Bugfix] Convert untraceable GroupShape to list for AMD impl (@Lucaskabela)
- Merged: [#23207](https://github.com/vllm-project/vllm/pull/23207) [Misc][qwen2_5_vl][torch.compile] Enable `supports_torch_com (@Lucaskabela)
- Merged: [#34181](https://github.com/vllm-project/vllm/pull/34181) [CI][AMD][BugFix] Use torch.testing.assert_close instead of  (@rasmith)
- Merged: [#34655](https://github.com/vllm-project/vllm/pull/34655) [CI][AMD][BugFix] Skip tests in test_unquantized_backend_sel (@rasmith)
- Merged: [#34455](https://github.com/vllm-project/vllm/pull/34455) [Bugfix] Remove assert causing hipErrorStreamCaptureUnsuppor (@JadenMathias)
- Merged: [#34653](https://github.com/vllm-project/vllm/pull/34653) [BugFix] [Build] fix string literals comparison in indexer_k (@hongxiayang)

### sglang
- Opened: [#19152](https://github.com/sgl-project/sglang/pull/19152) [Feature] Support offload and wake up of SGLang Diffusion (@klhhhhh)
- Opened: [#19228](https://github.com/sgl-project/sglang/pull/19228) [AMD] optimize Kimi K2.5 fused_moe_triton performance by tun (@ZiguanWang)
- Opened: [#19203](https://github.com/sgl-project/sglang/pull/19203) [AMD] Merge Dockerfiles for ROCm (@akao-amd)
- Opened: [#19113](https://github.com/sgl-project/sglang/pull/19113) [AMD] [DO NOT MERGE] Fix pre-existing AMD CI test failures (@michaelzhang-ai)
- Opened: [#18992](https://github.com/sgl-project/sglang/pull/18992) [AMD] Enable ROCm kvcache JIT path and add AMD CI coverage. (@hubertlu-tw)
- Opened: [#19192](https://github.com/sgl-project/sglang/pull/19192) Fix nightly grok failure on rotary embedding import (@michaelzhang-ai)
- Opened: [#19216](https://github.com/sgl-project/sglang/pull/19216) [AMD][with CI Fix] support two batch overlapping for mori ep (@billishyahao)
- Opened: [#19103](https://github.com/sgl-project/sglang/pull/19103) [jit_kernel] Migrate cast (downcast_fp8) from sgl-kernel AOT (@Johnsonms)
- Opened: [#19247](https://github.com/sgl-project/sglang/pull/19247) [AMD] Fix accuracy while using --enable-dp-attention (@yichiche)
- Opened: [#19122](https://github.com/sgl-project/sglang/pull/19122) [3/n] deepseek_v2.py Refactor: Migrate MLA forward method in (@Fridge003)
- Opened: [#19262](https://github.com/sgl-project/sglang/pull/19262) Some refactors on nsa_indexer.py (@Fridge003)
- Opened: [#19260](https://github.com/sgl-project/sglang/pull/19260) [AMD] Add suffix decoding support for ROCm and fix non-MLA s (@amd-pedghazi)
- Opened: [#19249](https://github.com/sgl-project/sglang/pull/19249) [WIP][diffusion] kernel fusion: scale residual norm scale sh (@linfann)
- Opened: [#19140](https://github.com/sgl-project/sglang/pull/19140) [Re-land][jit kernel] Support per_token_group_quant_8bit jit (@yuan-luo)
- Opened: [#19210](https://github.com/sgl-project/sglang/pull/19210) fix(docker): migrate ROCm Dockerfiles from setuptools-rust t (@slin1237)
- Opened: [#19161](https://github.com/sgl-project/sglang/pull/19161) Revert "[AMD] support two batch overlapping for mori ep #179 (@Fridge003)
- Opened: [#19193](https://github.com/sgl-project/sglang/pull/19193) [CI] fix the teardown output of disaggregation test (@hnyls2002)
- Opened: [#19168](https://github.com/sgl-project/sglang/pull/19168) [PD-Disagg] Support query dp rank from bootstrap server. (@hnyls2002)
- Opened: [#18978](https://github.com/sgl-project/sglang/pull/18978) [AMD]  Fix mi35x dsv32 mtp nightly (@bingxche)
- Opened: [#19078](https://github.com/sgl-project/sglang/pull/19078) Document & auto-enable FP8 block-wise CUTLASS GEMM for SM120 (@Arush04)
- Opened: [#19162](https://github.com/sgl-project/sglang/pull/19162) [ROCm] Use unreg path for custom all-reduce during CUDA grap (@zyzshishui)
- Opened: [#19176](https://github.com/sgl-project/sglang/pull/19176) [AMD] ENV Flags tuning and cleanup (@HaiShaw)
- Opened: [#19091](https://github.com/sgl-project/sglang/pull/19091) Update rocm7.2 Dockerfile to install amdsmi for QuickReduce  (@clintg6)
- Opened: [#19165](https://github.com/sgl-project/sglang/pull/19165) Enable Building on gfx1100 (@Calandracas606)
- Opened: [#19143](https://github.com/sgl-project/sglang/pull/19143) feat: Support MXFP4 quantized dense models on AMD CDNA2/CDNA (@fengli1702)
- Opened: [#19134](https://github.com/sgl-project/sglang/pull/19134) Fix spec v2+dp attention in nsa backend (@Qiaolin-Yu)
- Opened: [#19066](https://github.com/sgl-project/sglang/pull/19066) Optimization of Qwen Image, Qwen 2.5 ViT and LLM (@zhyajie)
- Opened: [#19007](https://github.com/sgl-project/sglang/pull/19007) [AMD] Replace msgpack with msgspec in MORI-IO (@Duyi-Wang)
- Opened: [#18982](https://github.com/sgl-project/sglang/pull/18982) [Doc] Add `flashinfer_deepgemm` to `--fp8-gemm-backend` (@mmangkad)
- Opened: [#18972](https://github.com/sgl-project/sglang/pull/18972) [AMD] ROCm7.2: Add /sgl-workspace/aiter to PYTHONPATH (@HaiShaw)
- Merged: [#13747](https://github.com/sgl-project/sglang/pull/13747) [AMD] Support --enable-aiter-allreduce-fusion on AMD GPUs (@hubertlu-tw)
- Merged: [#18911](https://github.com/sgl-project/sglang/pull/18911) [AMD] [GLM-5 Day 0] Add GLM-5 nightly test (@michaelzhang-ai)
- Merged: [#18242](https://github.com/sgl-project/sglang/pull/18242) [ROCm] Optimize Deepseek R1 on MI300X (@zhentaocc)
- Merged: [#18624](https://github.com/sgl-project/sglang/pull/18624) [AMD] DSR1/V3 use fp8 bmm in MLA for MI300X (@zhentaocc)
- Merged: [#17953](https://github.com/sgl-project/sglang/pull/17953) [AMD] support two batch overlapping for mori ep (@billishyahao)
- Merged: [#18252](https://github.com/sgl-project/sglang/pull/18252) [4/N] Quantization Refactor: Quark MoE schemes (@TamirBaydasov)

### triton
- Opened: [#9561](https://github.com/triton-lang/triton/pull/9561) [AMD]Support ScaleFactor=16 and E4M3 Scale and Add scale_fac (@knwng)
- Opened: [#9562](https://github.com/triton-lang/triton/pull/9562) Add maxnreg support for ROCm/AMD backend (@fsx950223)
- Opened: [#9555](https://github.com/triton-lang/triton/pull/9555) [AMD][BACKEND] Reintroduce old histogram lowering as AMD pat (@AlexAUT)
- Opened: [#9545](https://github.com/triton-lang/triton/pull/9545) [AMD] Fix scale layouts for batched WMMA scaled  (@borontion)
- Opened: [#9541](https://github.com/triton-lang/triton/pull/9541) [AMD] CanonicalizePointers: Handle different base pointers a (@kelesvol)
- Opened: [#9544](https://github.com/triton-lang/triton/pull/9544) [AMD][BACKEND] Extend padded layout selection for AsyncCopy  (@AlexAUT)
- Opened: [#9506](https://github.com/triton-lang/triton/pull/9506) [AMD] Fix TensorDescType shared memory size for WS captures (@PMylon)
- Opened: [#9533](https://github.com/triton-lang/triton/pull/9533) [AMD] Update default to `block_m=16` in `make_default_opt_fl (@micah-wil)
- Opened: [#9512](https://github.com/triton-lang/triton/pull/9512) [AMD][NFC] Emit error for buffer_load_to_local on gfx1250 (@AlexAUT)
- Opened: [#9519](https://github.com/triton-lang/triton/pull/9519) [AMD][NFC] Fix error message for wmma scale (@borontion)
- Opened: [#9522](https://github.com/triton-lang/triton/pull/9522) [AMD] Update gfx1250 MXFP FA example kernel (@borontion)
- Opened: [#9509](https://github.com/triton-lang/triton/pull/9509) [AMD] Enable supportBitwidth{16|32}Elementwise in TargetInfo (@antiagainst)
- Opened: [#9513](https://github.com/triton-lang/triton/pull/9513) [AMD][GLUON] Allow DistributedLayouts in AsyncCopy and Buffe (@AlexAUT)
- Opened: [#9502](https://github.com/triton-lang/triton/pull/9502) [AMD][BACKEND] Cherry pick pr 9487 to rel 3.7 (@AmdSampsa)
- Opened: [#9496](https://github.com/triton-lang/triton/pull/9496) [AMD][gfx1250] Fix tensordesc index after kernel launch chan (@antiagainst)
- Opened: [#9494](https://github.com/triton-lang/triton/pull/9494) Revert "[AMD] Don't use s_waitcnt to lower global barrier fo (@antiagainst)
- Merged: [#8464](https://github.com/triton-lang/triton/pull/8464) [AMD] Optimize address increments for buffer loads in loops (@alefimov-amd)
- Merged: [#9374](https://github.com/triton-lang/triton/pull/9374) Reapply "[AMD] Introduce PartitionedSharedEncodingAttr" (#93 (@plognjen)
- Merged: [#9442](https://github.com/triton-lang/triton/pull/9442) [AMD][BACKEND] Fix OOM bug in pipelining with padded layout  (@pabloantoniom)

### migraphx
- Opened: [#4631](https://github.com/ROCm/AMDMIGraphX/pull/4631) Use Eigen 3rd party library for ref GEMMs (@kahmed10)
- Opened: [#4630](https://github.com/ROCm/AMDMIGraphX/pull/4630) [AIMIGRAPHX-568] jit implementation for logsoftmax (@bdevorem)
- Opened: [#4626](https://github.com/ROCm/AMDMIGraphX/pull/4626) Add debug symbols for parsed and compiler pass replaced inst (@CharlieL7)
- Opened: [#4620](https://github.com/ROCm/AMDMIGraphX/pull/4620) [AIMIGRAPHX-542] implement argmin and argmax as reduce ops (@bdevorem)
- Opened: [#4628](https://github.com/ROCm/AMDMIGraphX/pull/4628) Add option to skip benchmarking in compile_ops (@kahmed10)
- Opened: [#4625](https://github.com/ROCm/AMDMIGraphX/pull/4625) [AIMIGRAPHX-544] Parallel compilation for dynamic graphs (@shivadbhavsar)
- Opened: [#4624](https://github.com/ROCm/AMDMIGraphX/pull/4624) Onnxruntime Weekly Sync 2026-02-20 (@github-actions[bot])
- Opened: [#4623](https://github.com/ROCm/AMDMIGraphX/pull/4623) Add support for ORT image in Jenkins pipeline (@causten)
- Opened: [#4627](https://github.com/ROCm/AMDMIGraphX/pull/4627) rocMLIR Weekly Sync 2026-02-22 (@github-actions[bot])
- Opened: [#4622](https://github.com/ROCm/AMDMIGraphX/pull/4622) Bump rocm-docs-core from 1.31.3 to 1.32.0 in /docs/sphinx (@dependabot[bot])
- Opened: [#4621](https://github.com/ROCm/AMDMIGraphX/pull/4621) [AIMIGRAPHX-571] Rewrite convolutions to GEMMs for constant  (@eddieliao)
- Merged: [#4595](https://github.com/ROCm/AMDMIGraphX/pull/4595) Handle split groups when the reduction is across different d (@pfultz2)
- Merged: [#4589](https://github.com/ROCm/AMDMIGraphX/pull/4589) Fix eliminate_contiugous iterator bug (@CharlieL7)
- Merged: [#4432](https://github.com/ROCm/AMDMIGraphX/pull/4432) Improve layout propagation in poinwise fusion when using bro (@pfultz2)
- Merged: [#4592](https://github.com/ROCm/AMDMIGraphX/pull/4592) Bump protobuf from 4.25.8 to 6.33.5 in /tools (@dependabot[bot])
- Merged: [#4593](https://github.com/ROCm/AMDMIGraphX/pull/4593) Bump protobuf from 4.25.8 to 5.29.6 in /test/py (@dependabot[bot])
- Merged: [#4602](https://github.com/ROCm/AMDMIGraphX/pull/4602) Bump cryptography from 44.0.1 to 46.0.5 in /docs/sphinx (@dependabot[bot])
- Merged: [#4609](https://github.com/ROCm/AMDMIGraphX/pull/4609) Propagate constant optimization (@pnikolic-amd)
- Merged: [#4410](https://github.com/ROCm/AMDMIGraphX/pull/4410) clamping the scale (@aarushjain29)
- Merged: [#4510](https://github.com/ROCm/AMDMIGraphX/pull/4510) [BugFix] - Fix tile byte size overflow for LDS memory when p (@ivarusic-amd)
- Merged: [#4615](https://github.com/ROCm/AMDMIGraphX/pull/4615) rocMLIR Weekly Sync 2026-02-15 (@github-actions[bot])
- Merged: [#4362](https://github.com/ROCm/AMDMIGraphX/pull/4362) disable matching for dynamic shapes (@shivadbhavsar)

### aiter
- Opened: [#2105](https://github.com/ROCm/aiter/pull/2105) Lxx/dev/fa fwd fp8 (@shay-li77)
- Opened: [#2100](https://github.com/ROCm/aiter/pull/2100) Fix CI prebuild: use build_ext so kernels are actually compi (@okakarpa)
- Opened: [#2104](https://github.com/ROCm/aiter/pull/2104) fix smoothquant hip kernel exceed int32's maximum. (@rujiacai)
- Opened: [#2091](https://github.com/ROCm/aiter/pull/2091) docs: add missing dependency install steps to contributor gu (@tuukkjs)
- Opened: [#2084](https://github.com/ROCm/aiter/pull/2084) [HIP] causal conv1d hip decode (@huizzhan)
- Opened: [#2097](https://github.com/ROCm/aiter/pull/2097) CI: Add steps to monitor the system health before ATOM tests (@gyohuangxin)
- Opened: [#2062](https://github.com/ROCm/aiter/pull/2062) Add ENABLE_CK=0 build option for Triton-only builds (@sunway513)
- Opened: [#2078](https://github.com/ROCm/aiter/pull/2078) [TRITON] MXFP4 GEMM fixes (@cagrikymk)
- Opened: [#2075](https://github.com/ROCm/aiter/pull/2075) Use unreg path for custom all-reduce during CUDA graph captu (@zyzshishui)
- Opened: [#2096](https://github.com/ROCm/aiter/pull/2096) Hipgraph support for fav3 kvcache (@sahirema)
- Opened: [#2087](https://github.com/ROCm/aiter/pull/2087) CI: Test the prebuild on different runner (@gyohuangxin)
- Opened: [#2092](https://github.com/ROCm/aiter/pull/2092) tune: add 493 new FP4 GEMM shapes for LLM inference (@sunway513)
- Opened: [#2064](https://github.com/ROCm/aiter/pull/2064) Adding double buffer option to cross_device_reduce_1stage (@RichardChamberlain1)
- Opened: [#2090](https://github.com/ROCm/aiter/pull/2090) mla n128 4 b64 c10k fix return lse error (@minmengdie)
- Opened: [#2089](https://github.com/ROCm/aiter/pull/2089) Fireworks mla n128 4 b64 c10k fix return lse (@minmengdie)
- Opened: [#2088](https://github.com/ROCm/aiter/pull/2088) opt_csrc_torch_header (@amd-ruitang3)
- Opened: [#2086](https://github.com/ROCm/aiter/pull/2086) Respect AITER_LOG_LEVEL for C++ stdout prints (@Copilot)
- Opened: [#2068](https://github.com/ROCm/aiter/pull/2068) Optimize top-k top-p sampler kernel by prefetching data (@aryaman-gupta)
- Opened: [#2085](https://github.com/ROCm/aiter/pull/2085) pa_decode_gluon_aot C++ api (@amd-yilizhao)
- Opened: [#2074](https://github.com/ROCm/aiter/pull/2074) Add ENABLE_CK build option for CK-free builds (@sunway513)
- Opened: [#2071](https://github.com/ROCm/aiter/pull/2071) Add CLAUDE.md and skill for tune_ck_gemm_a8w8_blockscale. (@sabreshao)
- Opened: [#2067](https://github.com/ROCm/aiter/pull/2067) Amd/satya/gluon/gemm mxfp4 (@Boss2002n)
- Opened: [#2060](https://github.com/ROCm/aiter/pull/2060) GFX1250 Kernels - GEMMa8w8 blockscale (@amirumoAMD)
- Opened: [#2077](https://github.com/ROCm/aiter/pull/2077) [OPUS] Enhance opus.hpp, add moe_sorting_opus, workgroup_bar (@carlushuang)
- Opened: [#2103](https://github.com/ROCm/aiter/pull/2103) CI: Skip CI for draft PR or docs changes (@gyohuangxin)
- Opened: [#2095](https://github.com/ROCm/aiter/pull/2095) [Triton] fix config selection bug for FP4 preshuffled GEMM (@k50112113)
- Opened: [#2066](https://github.com/ROCm/aiter/pull/2066) [TRITON] Sage attention v2: Q*K in mxfp4 (@juuso-oskari)
- Opened: [#2082](https://github.com/ROCm/aiter/pull/2082) use regex to extract arch from rocminfo string (@ahmed-bsod)
- Opened: [#2083](https://github.com/ROCm/aiter/pull/2083) CI: fix triton commit to c147f098 (@gyohuangxin)
- Opened: [#2069](https://github.com/ROCm/aiter/pull/2069) Add compiler configurations for bpreshuffle_cktile modules (@kensclin)
- Opened: [#2070](https://github.com/ROCm/aiter/pull/2070) [OPUS] Add f32 MFMA support (@carlushuang)
- Opened: [#2065](https://github.com/ROCm/aiter/pull/2065) revert triton gemm kernel config due to core dump. (@Duyi-Wang)
- Merged: [#1973](https://github.com/ROCm/aiter/pull/1973) Defer expensive build operations to build_ext.run() (@paradigm)
- Merged: [#2042](https://github.com/ROCm/aiter/pull/2042) upload mla_a8w8_qh64_qseqlen4_gqaratio16 co in MI300 (@minmengdie)
- Merged: [#2029](https://github.com/ROCm/aiter/pull/2029) [FIX] fix a16 causal mha bwd case for python api (@JaxChen29)
- Merged: [#2055](https://github.com/ROCm/aiter/pull/2055) Silence certain warnings stemming from CK (@Micky774)
- Merged: [#2037](https://github.com/ROCm/aiter/pull/2037) Add MI355X tuned GEMM configs for FP4 and FP8 (@sunway513)
- Merged: [#2034](https://github.com/ROCm/aiter/pull/2034) Top-K Top-P Sampling Kernel Optimization (@aryaman-gupta)

### atom
- Opened: [#234](https://github.com/ROCm/ATOM/pull/234) fix: resolve prefix caching crashes with MTP speculative dec (@valarLip)
- Opened: [#223](https://github.com/ROCm/ATOM/pull/223) Add Quark GLM4.7-MXFP4 support (@thpereir)
- Opened: [#231](https://github.com/ROCm/ATOM/pull/231) [Qwen3-Next](md): add user guide for qwen3-next (@PerryZhang01)
- Opened: [#227](https://github.com/ROCm/ATOM/pull/227) Refactor ATOM for top-k top-p sampling support (@aryaman-gupta)
- Opened: [#225](https://github.com/ROCm/ATOM/pull/225) Add FlyDSL MOE backend and Triton fallback for FP8 MoE (@sunway513)
- Opened: [#226](https://github.com/ROCm/ATOM/pull/226) Enable Triton MOE for MXFP4 on gfx950 (MI355X) (@sunway513)
- Opened: [#232](https://github.com/ROCm/ATOM/pull/232) CI: Add path filters to ATOM test workflow (@gyohuangxin)
- Opened: [#230](https://github.com/ROCm/ATOM/pull/230) CI: Optimize the cleanup (@gyohuangxin)
- Opened: [#228](https://github.com/ROCm/ATOM/pull/228) enable mtp=3 (@valarLip)
- Opened: [#224](https://github.com/ROCm/ATOM/pull/224) Add Dockerfile.clean + fix linear.py shard_offset bug (@sunway513)
- Merged: [#210](https://github.com/ROCm/ATOM/pull/210) CI: Add thresholds for models accuracy tests (@gyohuangxin)

### mori
- Opened: [#173](https://github.com/ROCm/mori/pull/173) Feature: make MORI framework agnostic (@TianDi101)
- Opened: [#172](https://github.com/ROCm/mori/pull/172) Feat: Enable async kernel BF16 cast to FP8 combine (@isytwu)
- Opened: [#171](https://github.com/ROCm/mori/pull/171) Fix: support runtime hidden_dim for dispatch/combine (@isytwu)

### flydsl
- Opened: [#147](https://github.com/ROCm/FlyDSL/pull/147) [Pytest] Fix assertion in `test_gpu_with_rocir_coords.py`  (@sammysun0711)
- Opened: [#145](https://github.com/ROCm/FlyDSL/pull/145) Pre v0.1 gemm (@coderfeli)
- Opened: [#146](https://github.com/ROCm/FlyDSL/pull/146) moe gemm stage2 atomics fp32 (@yadaish)
- Opened: [#149](https://github.com/ROCm/FlyDSL/pull/149) Update README.md MLIR_PATH path (@jundaf2)
- Opened: [#143](https://github.com/ROCm/FlyDSL/pull/143) W4a16 bf16 fp8 bf16 moe (@ClementLinCF)
- Opened: [#148](https://github.com/ROCm/FlyDSL/pull/148) update: publish artifacts nightlies (@kiran-thumma)
- Opened: [#144](https://github.com/ROCm/FlyDSL/pull/144) fix all copyright & license headers & add lint (@JackWolfard)
- Opened: [#141](https://github.com/ROCm/FlyDSL/pull/141) mxfp4 preshuffled gemm optimize (@yadaish)
- Opened: [#139](https://github.com/ROCm/FlyDSL/pull/139) optimize buffer_load lds pipeline. Now it can interleave wit (@yadaish)
- Opened: [#140](https://github.com/ROCm/FlyDSL/pull/140) fix(build): dereference MLIR symlinks in build output (@sunway513)
- Opened: [#138](https://github.com/ROCm/FlyDSL/pull/138) refactor the arch check related to bf16 global atomics for e (@hongxiayang)
- Opened: [#137](https://github.com/ROCm/FlyDSL/pull/137) add declaimer (@hongxiayang)
- Merged: [#129](https://github.com/ROCm/FlyDSL/pull/129) [MoE] simplify moe reduce kernel & add zero buffer flag (@aoli26)
- Merged: [#98](https://github.com/ROCm/FlyDSL/pull/98) fix a4w4 gemm precision (@zhiding512)
- Merged: [#136](https://github.com/ROCm/FlyDSL/pull/136) fix N/A SKU and replace it with gfx for gpu information in r (@hongxiayang)

### transformer_engine
- Opened: [#461](https://github.com/ROCm/TransformerEngine/pull/461) [NO MERGE] Integrate CK varlen cross attention for small-seq (@VeeraRajasekhar)
- Opened: [#458](https://github.com/ROCm/TransformerEngine/pull/458) Triton current scaling: avoid casting amax input (@matthiasdiener)
- Opened: [#453](https://github.com/ROCm/TransformerEngine/pull/453) Update ck_fused_attn logging to direct to thread-specific fi (@Micky774)
- Opened: [#460](https://github.com/ROCm/TransformerEngine/pull/460) IFU v2.10 (@alextmagro)
- Opened: [#459](https://github.com/ROCm/TransformerEngine/pull/459) Always use V2 hipify. Make all hipify results consistent (@ipanfilo)
- Opened: [#454](https://github.com/ROCm/TransformerEngine/pull/454) Updated test to include CK/AITER V2/V3 test in single backen (@Micky774)
- Opened: [#452](https://github.com/ROCm/TransformerEngine/pull/452) Updated Triton norms dispatch consolidation (@Micky774)
- Opened: [#456](https://github.com/ROCm/TransformerEngine/pull/456) Swapped out JIT incompatible function (@Micky774)
- Opened: [#455](https://github.com/ROCm/TransformerEngine/pull/455) Avoid hipifying code outside TE tree (@ipanfilo)
- Opened: [#457](https://github.com/ROCm/TransformerEngine/pull/457) Porting fix for triton compilation error (@Micky774)
- Merged: [#195](https://github.com/ROCm/TransformerEngine/pull/195) Added Dockerfile for CI images & Upgrate CI to ROCm 7.2 (@VeeraRajasekhar)
- Merged: [#406](https://github.com/ROCm/TransformerEngine/pull/406) IFU release v2.6 (@wangye805)
- Merged: [#382](https://github.com/ROCm/TransformerEngine/pull/382) Enable AOTriton BWD V3 API (@Micky774)

## New Issues This Week

### pytorch
- [#175728](https://github.com/pytorch/pytorch/issues/175728) Numerical discrepancy in nn.Embedding between eager and comp (@zifan6699)
- [#175724](https://github.com/pytorch/pytorch/issues/175724) [Distributed][NCCL] Request for Backport: SIGSEGV in Heartbe (@tomjen12)
- [#175720](https://github.com/pytorch/pytorch/issues/175720) DISABLED test_custom_op_with_memory_format_arg_xpu (__main__ (@etaf)
- [#175719](https://github.com/pytorch/pytorch/issues/175719) DISABLED test_flex_attention_with_dynamic_max_autotune_graph (@etaf)
- [#175718](https://github.com/pytorch/pytorch/issues/175718) DISABLED test_errors_nn_MultiMarginLoss_xpu_float64 (__main_ (@etaf)
- [#175717](https://github.com/pytorch/pytorch/issues/175717) DISABLED test_errors_nn_MultiMarginLoss_xpu_float32 (__main_ (@etaf)
- [#175637](https://github.com/pytorch/pytorch/issues/175637) Conv2d output inconsistency across different execution modes (@zifan6699)
- [#175600](https://github.com/pytorch/pytorch/issues/175600) [Inductor][PrivateUse1]: Stride mismatch in constant_pad_nd  (@markc-614)
- [#175690](https://github.com/pytorch/pytorch/issues/175690) Dtensor Shard->Replicate redistribution corrupts symbolic sh (@Lucaskabela)
- [#175608](https://github.com/pytorch/pytorch/issues/175608) torch.compile fails to trace try-except semantic and throw ` (@Shawn0v0)
- [#175533](https://github.com/pytorch/pytorch/issues/175533) FlexAttention's `BlockMask._adjust` does not update the sequ (@nicolo-domyn)
- [#175610](https://github.com/pytorch/pytorch/issues/175610) `torch.compile` crashes with `InternalTorchDynamoError: Attr (@sssssh1228)
- [#175615](https://github.com/pytorch/pytorch/issues/175615) torch.compile does not support operator override for user-de (@Shawn0v0)
- [#175545](https://github.com/pytorch/pytorch/issues/175545) FlexAttention does not allow captured tensor of ndim>0 (@mseeger)
- [#175368](https://github.com/pytorch/pytorch/issues/175368) `F.embedding_bag` segfaults when intermediate offsets exceed (@SilentTester73)
- [#175370](https://github.com/pytorch/pytorch/issues/175370) `F.embedding_bag` segfaults with float64 weight and empty of (@SilentTester73)
- [#175530](https://github.com/pytorch/pytorch/issues/175530) torch.compile fails during backward for ConvNeXt LayerNorm2d (@hassonofer)
- [#175431](https://github.com/pytorch/pytorch/issues/175431) out‑of‑memory error, although most of GPU memory is still av (@amd-xiaoyu12)
- [#175482](https://github.com/pytorch/pytorch/issues/175482) DISABLED test_index_put_error_cuda (__main__.TestNestedTenso (@jithunnair-amd)

### vllm
- [#35191](https://github.com/vllm-project/vllm/issues/35191) [Bug]: Qwen3.5 397B FP8 fills 1TB RAM and OOM killed with hi (@FWao)
- [#35266](https://github.com/vllm-project/vllm/issues/35266) [Bug]: Missing opening brace for Qwen3.5 streaming tool call (@AsterisMono)
- [#35163](https://github.com/vllm-project/vllm/issues/35163) [Bug]: AMD docker image still using torch 2.9 despite 2.10.0 (@mikaylagawarecki)
- [#35255](https://github.com/vllm-project/vllm/issues/35255) [Bug]: CUDA Error 803 on host with driver 590.48: `system ha (@git-jxj)
- [#35028](https://github.com/vllm-project/vllm/issues/35028) [Bug]: RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE (@shahizat)
- [#35091](https://github.com/vllm-project/vllm/issues/35091) [Bug]: Triton CompilationError in speculative decoding (draf (@rse173)
- [#34994](https://github.com/vllm-project/vllm/issues/34994) [Feature]: Infrastructure Improvements for ROCm CI (@AndreasKaratzas)
- [#35235](https://github.com/vllm-project/vllm/issues/35235) [CI Failure]:  mi355_1: Multi-Modal Models Test (Extended) 1 (@AndreasKaratzas)
- [#35233](https://github.com/vllm-project/vllm/issues/35233) [CI Failure]:  mi355_1: Language Models Test (Extended Gener (@AndreasKaratzas)
- [#35133](https://github.com/vllm-project/vllm/issues/35133) [CI Failure]:  mi355_4: LoRA TP Test (Distributed) (@AndreasKaratzas)
- [#35138](https://github.com/vllm-project/vllm/issues/35138) [Bug]: Qwen/Qwen3.5-397B-A17B-FP8 and Qwen/Qwen3.5-397B-A17B (@xinli-sw)
- [#34817](https://github.com/vllm-project/vllm/issues/34817) [Bug]: Trying to run gpt-oss-120b on rtx pro 6000 (@chadbek)
- [#35169](https://github.com/vllm-project/vllm/issues/35169) [Bug]: Memory Access Fault during Step-3.5-Flash inference ( (@ColinZ22)
- [#35128](https://github.com/vllm-project/vllm/issues/35128) [CI Failure]:  mi355_1: Language Models Tests (Standard) (@AndreasKaratzas)
- [#35089](https://github.com/vllm-project/vllm/issues/35089) [RFC]: In-Tree AMD Zen CPU Backend via zentorch (@amd-lalithnc)
- [#34851](https://github.com/vllm-project/vllm/issues/34851) [Feature]: Refactor Quark MoE and mxfp4 MoE to align with Mo (@BowenBao)
- [#35126](https://github.com/vllm-project/vllm/issues/35126) [CI Failure]:  mi355_1: Kernels MoE Test %N (@AndreasKaratzas)
- [#35129](https://github.com/vllm-project/vllm/issues/35129) [CI Failure]:  mi355_4: 2 Node Tests (4 GPUs in total) (@AndreasKaratzas)
- [#35132](https://github.com/vllm-project/vllm/issues/35132) [CI Failure][ROCm]:  CrossLayer KV layout Distributed NixlCo (@AndreasKaratzas)
- [#35084](https://github.com/vllm-project/vllm/issues/35084) [Bug]: VLLM tries to load "inductor" instead of custom compi (@mergian)
- [#35031](https://github.com/vllm-project/vllm/issues/35031) [Bug]: MTP Speculative Decoding with NVFP4: Weight Shape Mis (@eleqtrizit)
- [#35057](https://github.com/vllm-project/vllm/issues/35057) [Bug]: Qwen3.5 `scheduler_metadata must have shape (metadata (@ehfd)

### sglang
- [#19272](https://github.com/sgl-project/sglang/issues/19272) [AMD] Aiter attention backends crash on hybrid Mamba+attenti (@michaelzhang-ai)
- [#19028](https://github.com/sgl-project/sglang/issues/19028) [Bug] GLM5 nightly Mi355 broken due to transformer dependenc (@functionstackx)
- [#19139](https://github.com/sgl-project/sglang/issues/19139) [Rfc] Refactor server_args.py (@vincentzed)
- [#19031](https://github.com/sgl-project/sglang/issues/19031) [Feature] ROCm nightly in upstream lmsysorg docker org (@functionstackx)

### migraphx
- [#4618](https://github.com/ROCm/AMDMIGraphX/issues/4618) [Issue]: MIGraphX Dynamic Shape Issue ONNXRuntime (@DiarmuidKelly)

### aiter
- [#2102](https://github.com/ROCm/aiter/issues/2102) [Feature]: Add benchmark tests to detect performance regress (@gyohuangxin)
- [#2094](https://github.com/ROCm/aiter/issues/2094) [Issue]: Triton tests fail due to the API changes from Trito (@gyohuangxin)
- [#2098](https://github.com/ROCm/aiter/issues/2098) [Issue]: GPU memory access faults on GLM-4.6 FP8 (@richhx)
- [#2099](https://github.com/ROCm/aiter/issues/2099) CI prebuild step not actually compiling kernels - causes JIT (@okakarpa)
- [#2093](https://github.com/ROCm/aiter/issues/2093) [Feature]: replace CK moe sorting with opus (@carlushuang)
- [#2073](https://github.com/ROCm/aiter/issues/2073) [RFC] CK-Free Build: Enable AITER builds without Composable  (@sunway513)
- [#2080](https://github.com/ROCm/aiter/issues/2080) Remove unconditional prints to stdout (@gshtras)
- [#2081](https://github.com/ROCm/aiter/issues/2081) [Feature]: Can the MLA kernel be open source? (@aidando73)
- [#2076](https://github.com/ROCm/aiter/issues/2076) [RFC][Agentic/Low-Latency] improve compile time of python bi (@carlushuang)
- [#2061](https://github.com/ROCm/aiter/issues/2061) [Bug] Custom all-reduce IPC buffers use fixed VA, conflict w (@jhinpan)
- [#2059](https://github.com/ROCm/aiter/issues/2059) [Issue]: GLM-5 aiter fused_moe with SGLang + MI355 (@ozziemoreno)

### atom
- [#229](https://github.com/ROCm/ATOM/issues/229) [RFC] CK-Free AITER Compatibility: Attention, MOE, and Docke (@sunway513)

### flydsl
- [#142](https://github.com/ROCm/FlyDSL/issues/142) finish reconstruction of layout algebra (@coderfeli)
